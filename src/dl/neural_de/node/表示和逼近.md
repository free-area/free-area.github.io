---

headerDepth: 2
description: 关于神经网络逼近能力的一些理论
sidebar: heading
tag: 神经微分方程 
dir:
  order: 1
---

# 表示和逼近能力

## Notation

::: info 定义：p,sup范数
设 $p \in[1, \infty)$ ， $m, n \in \mathbb{N}$. 对于可测映射 $f: \mathbb{R}^m \rightarrow \mathbb{R}^n$ ，子集 $K \subset \mathbb{R}^m$, 设
	
$$
\|f\|_{p, K}:=\left(\int_K\|f(x)\|^p d x\right)^{1 / p},
$$

其中$\|\cdot\|$ 是Euclidean 范数 . 定义 $\|f\|_{\text {sup } K}:=\sup _{x \in K}\|f(x)\|$.
:::



::: info 定义：通用逼近器
设$p \in[1, \infty)$, 令 $\mathcal{F},\mathcal{M}$ 由可测函数 $f:U_f \rightarrow \mathbb{R}^n$构成, 其中 $U_f$ 是 $\mathbb{R}^m$的可测子集， 依赖$f$， 称 $\mathcal{M}$ 是$\mathcal{F}$的$sup$意义下的通用逼近器，如果  $\forall f \in \mathcal{F}$,  $\forall \varepsilon>0$,  $\forall K  \subset U_f$,其中$K$是紧集， 那么 $\exists g \in \mathcal{M}$ ,s.t. $\|f-g\|_{sup, K}<\varepsilon$.  
:::

设
$$\mathcal{F}=\left\{f_\theta(t,x):R\times R^d\rightarrow R^d \mid f_{\theta}\text{关于$x$ 是 lipschitz连续的}\right\}$$
$$\mathcal{S}(\mathcal{F},T)=\left\{\varphi(x,T)\mid \varphi(x,t)=ivp[f,x][t],f\in \mathcal{F},T>0 \right\}$$

[下一节](#同胚和流形假设)中证明，$\mathcal{S}(\mathcal{F},T)$中的都是同胚映射。因此如果作为假设空间表示能力和逼近能力都非常有限，下面给出Neural更一般形式的假设空间：
$$
\begin{equation}
	   \mathcal{GS}(T)=\left\{g \circ \varphi \mid g \in \mathcal{T}, \varphi\in \mathcal{S}(\mathcal{F},T)\right\},\mathcal{GS}=\bigcup_{T>0} \mathcal{GS}(T) 
\end{equation}
$$
在这个更加广义的集合$\mathcal{GS}$中，将终止时间$T$也作为了一个参数，同时在$\varphi(x,T)$后添加了一层终端函数（以$\varphi(x,T)$为输入）$g$，用来提高模型的表达能力。

## 同胚和流形假设

****{#lem1}

::: tip 引理1


 设 $h_1(t)$ 和 $h_2(t)$ 分别是初值问题$ivp[f,0,x_1]$和$ivp[f,0,x_2]$的两个解， 那么 $\forall t \in [0, T], h_1(t) \neq h_2(t)$​. 

:::

::: tip 定理1
$\forall \varphi(x,T)\in \mathcal{S}(\mathcal{F},T)$,  $\varphi(x,T):R^d\rightarrow R^d$是同胚映射，保留了对输入空间的拓扑结构。

:::

::: details 证明：

证明分为三个部分：

1. $\varphi(x,T)$是连续的：$\forall$ $\epsilon>0$ , 根据 Gronwall's 引理：

$$
\left\|\varphi(x+\delta,T)-\varphi(x,T)\right\|  \leq e^{C t}\left\|\varphi(x,T)-\varphi(x+\epsilon,T)\right\|=e^{C t}\|\epsilon\|
$$

​	令 $\epsilon \rightarrow 0$, 那么 $\forall$ $t \in[0, T]$， $\varphi(t,x)$ 是连续的。

2. $\varphi(x,T)$是双射：由<a href="#lem1">引理</a>得出。

3. $\varphi_{-1}(x,T)$是连续的: 

   构造一个新的初值问题：
   $$
   h(T)=x ,\quad \frac{\mathrm{d} h}{\mathrm{~d} t}(t)=f_{\theta}(t, h(t))
   $$
   然后根据$\varphi(x,T)$连续和双射的证明得出。

:::	


​	



下面给出一个最简单的一维上$\varphi(x,T)$不能表示的函数。
	
::: warning
函数$g_1(x)=-x$不能被$\varphi(x,T)$表示。
:::

::: details 证明：

​		假设存在微分方程有两个解$h_1$和$h_2$满足：
$$
\begin{equation}
​			\begin{cases}h_1(0)=-1 & h_1(T)=1 \\ h_2(0)=1 & h_2(T)=-1\end{cases}
​		\end{equation}
$$
​		定义函数$h(t)=h_1(t)-h_2(t)$，那么h(0)=2,h(T)=-2 ,连续函数介质定理，一定存在着$t_0\in \left[0,T\right]$,使得$h_1(t_0)=h_2(t_0)$,根据\ref{ode_bi},同一个微分方程的两个轨迹是不想交的，所以矛盾，因此不存在$\phi_T(t)$能表示函数$g_1$。
​
::: 



==流形假设(manifold hypothesis)== 是机器学习和模式识别领域中的一个重要概念。它提出了一个假设，即高维数据通常存在于低维流形中。
简而言之，流形是一种具有局部线性结构的几何对象。在高维空间中，数据点可能分布在一个比观察到的维度更低的流形上。这意味着，尽管数据点在高维空间中可能看起来很复杂，但它们实际上可以由较少的自由度来描述。流形假设的核心思想是，学习算法可以通过寻找数据的低维表示来更好地理解和处理高维数据。通过将高维数据映射到低维流形上，可以减少数据的维度，并且可以更好地表示数据的内在结构和特征。流形假设在降维、特征提取、聚类和分类等机器学习任务中具有重要的应用。它为我们提供了一种理解高维数据的方法，并且可以帮助我们设计更有效的学习算法。



定理\ref{tongpei}表明：NODE会连续的变形输入空间而不会撕裂一个连接的区域。所以NODE能与流行假设优雅的进行交互：NODE描述了输入的流形如何随着深度$t$流动到输出流形，这也对于部分任务例如图形生成任务来，机器学习可解释化是优势，但对某些任务来说是劣势。因此假设空间 $\mathcal{S}(\mathcal{F},T)$对模型的表达能力很低，下面个各小节介绍一些方法来提高模型的表达能力。



## 增维的NODE

$\mathcal{S}(\mathcal{F},T)$并不能表示所有的$\mathcal{X}\rightarrow \mathcal{X}$同胚，但是可以通过增加维度来做到，下面这个定理说明通过将维度增加一倍数可以表示任何同胚映射。
::: tip 定理2
对任何同胚映射$h:\mathcal{X}\rightarrow \mathcal{X}$,都存在NeuralODE，维度为$2d$,使得：$\forall x \in \mathcal{X}$，有$\varphi\left(\left[x, 0^{(d)}\right],T\right)=\left[h(x), 0^{(d)}\right]$。
:::

在增加维度的NeuralODE后添加一个线性层$R_o$，那么可以逼近任何$R_d\rightarrow R_o$函数。



::: tip 定理3
​设$d, d_o \in \mathbb{N},d_l \in \mathbb{N}, f \in C\left(\mathbb{R} \times \mathbb{R}^{d_l} ; \mathbb{R}^{d_l}\right), l_1 \in L_b\left(\mathbb{R}^d ; \mathbb{R}^{d_l}\right)$, $l_2 \in L_b\left(\mathbb{R}^{d_l} ; \mathbb{R}^{d_o}\right)$,函数 $l_2\circ \varphi \circ l_1 : x \mapsto z$ 的形式如下

$$
	y(0)=l_1(x), \quad \frac{\mathrm{d} y}{\mathrm{~d} t}(t)=f(t, y(t)) \quad \text { for } t \in[0, T], \quad z=l_2(y(T))
$$
​	$f$要满足微分方程解的唯一性 ,那么
$$
	\mathcal{GS}_{aug}(T)=\left\{l_2\circ \varphi \circ l_1 \mid d_l \in \mathbb{N}, l_1 \in L_b\left(\mathbb{R}^d ; \mathbb{R}^{d_l}\right), l_2 \in L_b\left(\mathbb{R}^{d_l} ; \mathbb{R}^{d_o}\right)\right\}
$$
​	在$sup$意义上逼近 $C\left(\mathbb{R}^d ; \mathbb{R}^{d_o}\right)$.

:::

::: details 证明：
​	考虑如下的微分方程组：
$$
\begin{equation}
​		\label{eqq}
​		\begin{array}{cl}
​			y_0(0)=x \in \mathbb{R}^d & \frac{\mathrm{d} y_0}{\mathrm{~d} t}(t)=0, \\
​			y_1(0)=0 \in \mathbb{R}^{d \times d} & \frac{\mathrm{d} y_1}{\mathrm{~d} t}(t)=y_0(t) \otimes y_0(t), \\
​			y_2(0)=0 \in \mathbb{R}^{d \times d \times d} & \frac{\mathrm{d} y_2}{\mathrm{~d} t}(t)=y_1(t) \otimes y_0(t),\\
​	

​			y_3(0)=0 \in \mathbb{R}^{d \times d \times d \times d}&  \frac{\mathrm{d} y_3}{\mathrm{~d} t}(t)=y_2(t) \otimes y_0(t), \\
​			\cdots \\
​			y_M(0)=0 \in \mathbb{R}^{d \times \cdots \times d}&  \frac{\mathrm{d} y_M}{\mathrm{~d} t}(t)=y_{M-1}(t) \otimes y_0(t) .


​			
​	
​		
​			
​		\end{array}
​	\end{equation}
$$
​	每一组都可以单独求解：


​	
$$
\begin{aligned}
​		y_0(t) & =x, \\
​		y_1(t) & =\int_0^t x \otimes x \mathrm{~d} s=t x \otimes x, \\
​		y_2(t) & =\int_0^t s x \otimes x \mathrm{~d} s=\frac{1}{2} t^2 x^{\otimes 3}, \\
​		y_3(t) & =\int_0^t \frac{1}{2} s^2 x^{\otimes 3} \otimes x \mathrm{~d} s=\frac{1}{6} t^3 x^{\otimes 4}, \\
​		\ldots & \\
​		y_M(t) & =\frac{1}{M !} t^M x^{\otimes(M+1)} .
​	\end{aligned}
$$

​	不妨设$T=1$,那么得到了直到$M+1$阶的单项式：
$$
\begin{equation}
​		\alpha(x)=\left(x, x^{\otimes x}, \frac{1}{2 !} x^{\otimes 3}, \frac{1}{3 !} x^{\otimes 4}, \ldots, \frac{1}{M !} x^{\otimes(M+1)}\right) .
​	\end{equation}
$$

​	
​	
​	
​	
​	根据 Stone-Weierstrass theorem 定理： $\forall F \in C\left(K ; \mathbb{R}^{d_o}\right) ,\varepsilon>0$, $\exists M \in \mathbb{N}$ ,$M$足够大, 可以找到仿射函数 $l_2$,使得
​	
$$
\|F-l_2(\alpha(x))\|_{\infty, K}<\infty
$$
令$d_l=\sum_{k=1}^{M+1} d^k$ ， $f_{d_l}$ 由$\ref{eqq}$堆叠而层， $l_1$ 用来对输入 $x$ 补零，$l_2$ 对单项式进行组合以逼近目标连续函数.
	
	


​	
​	
​	
\end{proof}

定理$\ref{th:aug1}$从和定理$\ref{th:aug2}$虽然从理论上证明通过增维可以让模型具有通用逼近的能力，但是在训练过程中，会带来如下问题：

\begin{enumerate}
	\item 训练成本过高：如果维度增加$2$倍，那么显存占比也将翻倍，还有训练时间指数级增加等；
	\item 训练过程不可控：在定理$\ref{th:aug2}$中虽然对$f_\theta$的要求降低了，但是维数$M$也变成了一个不可控量，造成训练的混乱，模型的解释性差等。
	\item 造成参数冗余：NeuralODE的一个优点就在于，参数共享，一般将Neural以图像或者词向量作为输入在增加维度，那么会大幅度降低参数有效性；
	\item 破坏可逆性：只要在最理想的训练情况下，才能在输出的填充维上的值全为0。
\end{enumerate}
因此定理$\ref{th:aug1}$从和定理$\ref{th:aug2}$实际上只是一种理论上表达通过提高维度可以模型的表达能力，但实际定理只是给出了充分条件，我们可以减少增加维数，改变增加维数的方式（替换直接对输入$x$额外的维度补0来增维）来提高训练的高效性。在$\ref{sec:design} $中会介绍几种增加维度的NeuralODE网络架构。 
