---
dir.order: 100
---







# 网络设计

## 增维的的NODE

再 [表示和逼近.md](表示和逼近.md) 中提到过对输入进行维度增加可以提高模型的表达能力，再 [表示和逼近.md](表示和逼近.md)  中采用的方法非常简单：将输入的维度增加一倍 $x\in  R^d \rightarrow  x_{aug} \in R^{2d}$，然后在额外的维度上补0， 这个理论上可以表示任何同胚映射，但是它也有很大的缺陷，在 [表示和逼近.md](表示和逼近.md) 中已经表述过。下面子节是其他的用于增维的方法来解决 [表示和逼近.md](表示和逼近.md) 中描述的缺陷。

### 对输入做仿射变换

对于给定的初值 $x\in R^d$ ,   设$l_\theta:R^d\rightarrow R^l$ 是一个含参的仿射变换，那么将 $x_{aug}=l_\theta(x)$作为微分方程的初值，然后的到 $\varphi(y ,T)$作为NODE的输出，其中参数是需要学习的。
$$
h(0)=l_\theta(x), \quad \frac{d h}{dt}(t)=f_\theta(t, h(t)) .
$$
采用仿射的目的是 提高模型的表达能力的同时，做到高效的训练，不需要增加一倍的维度，更具体点可以设 $x_{aug}=(x,l_\theta(x))$,这就更进一步降低参数量了,同时也保证了前 $n$ 维的结构。当然这种增强破坏了NODE的双射的性质，那么下一章节的连续归一化流就不能使用，因为它要求可逆。



Lifting into a higher-dimensional space may be regarded as a relaxation of the Markov property[^1]. For $s<t$ then the output $\ell_\theta(y(s))$ does not completely determine $\ell_\theta(y(t))$. In contrast $y(s)$ does determine $y(t)$. (Whether $y$ is the output of an unaugmented neural $O D E$ or the latent value of an augmented neural ODE.)



### 随机化额外维度的初始值

也可以使用服从某个随机分布（正太分布或区间分布）的随机变量来初始化额外维度的初始值，和是直接设为0相比，他的优势如下

在数学上阐述为什么使用随机噪声初始化增广维度比使用零初始化更有利的原因可以从以下几个角度来考虑：

1. **非线性系统的敏感性**：对于非线性系统，初始条件的微小变化可能会导致长期行为的显著不同。这是混沌理论的一个关键观点，即初始条件的微小扰动可能导致完全不同的轨迹。在数学上，这可以通过Lyapunov指数来量化，它度量了相邻轨迹随时间分离的速率。随机噪声可以使得系统从一系列不同的初始条件开始演化，从而能够探索状态空间中的更多区域。

2. **打破对称性**：如果增广的维度被初始化为零，它们的行为会是对称的，因为在没有额外信息的情况下，它们会接收到相同的梯度更新。这在数学上意味着，如果我们有一个向量场 $f(\tilde{y})$，那么对于所有的初始点 $\tilde{y}_0$，如果它们的增广部分是相同的，那么它们的轨迹也将是相同的。随机噪声打破了这种对称性，使得即使是相同的原始输入，也能够在增广空间中产生不同的轨迹。

3. **增加状态空间的覆盖**：从概率论的角度来看，随机噪声允许系统在整个状态空间中以某种概率分布进行采样。如果增广维度总是初始化为零，那么系统的状态将始终局限于一个低维的子空间，从而限制了网络能够学习的功能。随机噪声初始化允许网络在训练过程中探索更多的状态组合。

4. **局部最小和鞍点**：在优化理论中，初始化可以极大地影响优化算法的收敛性。如果所有的路径都从相同的点开始，那么它们可能会陷入相同的局部最小或鞍点。通过随机初始化，我们可以提高找到全局最小或更好局部最小的机会，因为不同的路径可能会避开某些不良的局部最小。

5. **正则化效果**：在统计学习理论中，添加随机性可以看作是一种隐式的正则化。这有助于防止过拟合，因为它限制了模型在训练数据上的完美拟合能力，从而提高了模型在未见数据上的泛化能力。

尽管从理论上有这些考虑，但在实际操作中，是否使用随机噪声以及具体的噪声水平通常是基于经验和实验调整的。一些问题可能会从零初始化中受益，而其他问题则可能需要随机噪声来提高性能。因此，实践者通常会尝试不同的初始化策略，以找到最适合他们特定问题的方法。







[^1]: Neural综述

