const t=JSON.parse('{"key":"v-2ff46f0a","path":"/dl/robust/ad_example.html","title":"对抗攻击","lang":"zh-CN","frontmatter":{"description":"对抗攻击 基本概念 深度学习中的神经网络在精心训练后，其分类准确性可以非常出色，但其的鲁棒性却可能很差，可能会轻易被对抗攻击打破。即通过对输入图片进行一个微小的扰动，就可以在几乎肉眼看不出差距的前提下，让神经网络的分类准确率大幅下降。 img 对抗鲁棒性 【金山文档】 PGD https://kdocs.cn/l/csBKPtHtET4R 对抗攻击的分类 白箱攻击（white-box attack）：在白箱攻击中攻击者知道目标模型的所有信息，包括模型的训练集、类型、结构以及参数。 黑箱攻击（black-box attack）：在黑箱攻击中，攻击者不知道目标模型的内部细节，只能够观察目标模型对输入样本的输出结果。 定向攻击（target attack）：对于一个多分类网络，把输入分类误判到一个某个特定的错误类别上 非定向攻击（non-target attack）：只需要生成对抗样本，可以引入任意一个错误类别","head":[["meta",{"property":"og:url","content":"https://mister-hope.github.io/dl/robust/ad_example.html"}],["meta",{"property":"og:site_name","content":"丁善伟"}],["meta",{"property":"og:title","content":"对抗攻击"}],["meta",{"property":"og:description","content":"对抗攻击 基本概念 深度学习中的神经网络在精心训练后，其分类准确性可以非常出色，但其的鲁棒性却可能很差，可能会轻易被对抗攻击打破。即通过对输入图片进行一个微小的扰动，就可以在几乎肉眼看不出差距的前提下，让神经网络的分类准确率大幅下降。 img 对抗鲁棒性 【金山文档】 PGD https://kdocs.cn/l/csBKPtHtET4R 对抗攻击的分类 白箱攻击（white-box attack）：在白箱攻击中攻击者知道目标模型的所有信息，包括模型的训练集、类型、结构以及参数。 黑箱攻击（black-box attack）：在黑箱攻击中，攻击者不知道目标模型的内部细节，只能够观察目标模型对输入样本的输出结果。 定向攻击（target attack）：对于一个多分类网络，把输入分类误判到一个某个特定的错误类别上 非定向攻击（non-target attack）：只需要生成对抗样本，可以引入任意一个错误类别"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-12-16T07:52:05.000Z"}],["meta",{"property":"article:author","content":"丁善伟"}],["meta",{"property":"article:modified_time","content":"2023-12-16T07:52:05.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"对抗攻击\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2023-12-16T07:52:05.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"丁善伟\\",\\"url\\":\\"https://mister-hope.com\\"}]}"]]},"headers":[{"level":2,"title":"基本概念","slug":"基本概念","link":"#基本概念","children":[]},{"level":2,"title":"对抗鲁棒性","slug":"对抗鲁棒性","link":"#对抗鲁棒性","children":[]},{"level":2,"title":"对抗攻击的分类","slug":"对抗攻击的分类","link":"#对抗攻击的分类","children":[]},{"level":2,"title":"FGSM","slug":"fgsm","link":"#fgsm","children":[{"level":3,"title":"简介","slug":"简介","link":"#简介","children":[]},{"level":3,"title":"对抗样本的线性解释","slug":"对抗样本的线性解释","link":"#对抗样本的线性解释","children":[]},{"level":3,"title":"非线性模型的线性扰动","slug":"非线性模型的线性扰动","link":"#非线性模型的线性扰动","children":[]},{"level":3,"title":"线性模型的对抗训练与权重衰减的对比研究","slug":"线性模型的对抗训练与权重衰减的对比研究","link":"#线性模型的对抗训练与权重衰减的对比研究","children":[]}]},{"level":2,"title":"PGD","slug":"pgd","link":"#pgd","children":[]}],"git":{"createdTime":1702713125000,"updatedTime":1702713125000,"contributors":[{"name":"shanwei","email":"2369313525@qq.com","commits":1}]},"readingTime":{"minutes":7.64,"words":2293},"filePathRelative":"dl/robust/ad_example.md","localizedDate":"2023年12月16日","excerpt":"<h1> 对抗攻击</h1>\\n<h2> 基本概念</h2>\\n<p>深度学习中的神经网络在精心训练后，其分类准确性可以非常出色，但其的鲁棒性却可能很差，可能会轻易被对抗攻击打破。即通过对输入图片进行一个微小的扰动，就可以在几乎肉眼看不出差距的前提下，让神经网络的分类准确率大幅下降。</p>\\n<figure><figcaption>img</figcaption></figure>\\n<h2> 对抗鲁棒性</h2>\\n<p>【金山文档】 PGD\\nhttps://kdocs.cn/l/csBKPtHtET4R</p>\\n<h2> 对抗攻击的分类</h2>\\n<ul>\\n<li>白箱攻击（white-box attack）：在白箱攻击中攻击者知道目标模型的所有信息，包括模型的训练集、类型、结构以及参数。</li>\\n<li>黑箱攻击（black-box attack）：在黑箱攻击中，攻击者不知道目标模型的内部细节，只能够观察目标模型对输入样本的输出结果。</li>\\n<li>定向攻击（target attack）：对于一个多分类网络，把输入分类误判到一个某个<strong>特定的错误类别上</strong></li>\\n<li>非定向攻击（non-target attack）：只需要生成对抗样本，可以引入<strong>任意一个</strong>错误类别</li>\\n</ul>","autoDesc":true}');export{t as data};
