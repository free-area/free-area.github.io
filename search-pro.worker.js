const nt="ENTRIES",V="KEYS",T="VALUES",F="";class D{set;_type;_path;constructor(t,s){const n=t._tree,u=Array.from(n.keys());this.set=t,this._type=s,this._path=u.length>0?[{node:n,keys:u}]:[]}next(){const t=this.dive();return this.backtrack(),t}dive(){if(this._path.length===0)return{done:!0,value:void 0};const{node:t,keys:s}=E(this._path);if(E(s)===F)return{done:!1,value:this.result()};const n=t.get(E(s));return this._path.push({node:n,keys:Array.from(n.keys())}),this.dive()}backtrack(){if(this._path.length===0)return;const t=E(this._path).keys;t.pop(),!(t.length>0)&&(this._path.pop(),this.backtrack())}key(){return this.set._prefix+this._path.map(({keys:t})=>E(t)).filter(t=>t!==F).join("")}value(){return E(this._path).node.get(F)}result(){switch(this._type){case T:return this.value();case V:return this.key();default:return[this.key(),this.value()]}}[Symbol.iterator](){return this}}const E=e=>e[e.length-1],ut=(e,t,s)=>{const n=new Map;if(t===void 0)return n;const u=t.length+1,o=u+s,i=new Uint8Array(o*u).fill(s+1);for(let r=0;r<u;++r)i[r]=r;for(let r=1;r<o;++r)i[r*u]=r;return R(e,t,s,n,i,1,u,""),n},R=(e,t,s,n,u,o,i,r)=>{const d=o*i;t:for(const l of e.keys())if(l===F){const a=u[d-1];a<=s&&n.set(r,[e.get(l),a])}else{let a=o;for(let h=0;h<l.length;++h,++a){const m=l[h],p=i*a,f=p-i;let c=u[p];const g=Math.max(0,a-s-1),_=Math.min(i-1,a+s);for(let y=g;y<_;++y){const b=m!==t[y],z=u[f+y]+ +b,A=u[f+y+1]+1,w=u[p+y]+1,L=u[p+y+1]=Math.min(z,A,w);L<c&&(c=L)}if(c>s)continue t}R(e.get(l),t,s,n,u,a,i,r+l)}};class C{_tree;_prefix;_size=void 0;constructor(t=new Map,s=""){this._tree=t,this._prefix=s}atPrefix(t){if(!t.startsWith(this._prefix))throw new Error("Mismatched prefix");const[s,n]=x(this._tree,t.slice(this._prefix.length));if(s===void 0){const[u,o]=M(n);for(const i of u.keys())if(i!==F&&i.startsWith(o)){const r=new Map;return r.set(i.slice(o.length),u.get(i)),new C(r,t)}}return new C(s,t)}clear(){this._size=void 0,this._tree.clear()}delete(t){return this._size=void 0,ot(this._tree,t)}entries(){return new D(this,nt)}forEach(t){for(const[s,n]of this)t(s,n,this)}fuzzyGet(t,s){return ut(this._tree,t,s)}get(t){const s=I(this._tree,t);return s!==void 0?s.get(F):void 0}has(t){const s=I(this._tree,t);return s!==void 0&&s.has(F)}keys(){return new D(this,V)}set(t,s){if(typeof t!="string")throw new Error("key must be a string");return this._size=void 0,O(this._tree,t).set(F,s),this}get size(){if(this._size)return this._size;this._size=0;const t=this.entries();for(;!t.next().done;)this._size+=1;return this._size}update(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=O(this._tree,t);return n.set(F,s(n.get(F))),this}fetch(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=O(this._tree,t);let u=n.get(F);return u===void 0&&n.set(F,u=s()),u}values(){return new D(this,T)}[Symbol.iterator](){return this.entries()}static from(t){const s=new C;for(const[n,u]of t)s.set(n,u);return s}static fromObject(t){return C.from(Object.entries(t))}}const x=(e,t,s=[])=>{if(t.length===0||e==null)return[e,s];for(const n of e.keys())if(n!==F&&t.startsWith(n))return s.push([e,n]),x(e.get(n),t.slice(n.length),s);return s.push([e,t]),x(void 0,"",s)},I=(e,t)=>{if(t.length===0||e==null)return e;for(const s of e.keys())if(s!==F&&t.startsWith(s))return I(e.get(s),t.slice(s.length))},O=(e,t)=>{const s=t.length;t:for(let n=0;e&&n<s;){for(const o of e.keys())if(o!==F&&t[n]===o[0]){const i=Math.min(s-n,o.length);let r=1;for(;r<i&&t[n+r]===o[r];)++r;const d=e.get(o);if(r===o.length)e=d;else{const l=new Map;l.set(o.slice(r),d),e.set(t.slice(n,n+r),l),e.delete(o),e=l}n+=r;continue t}const u=new Map;return e.set(t.slice(n),u),u}return e},ot=(e,t)=>{const[s,n]=x(e,t);if(s!==void 0){if(s.delete(F),s.size===0)W(n);else if(s.size===1){const[u,o]=s.entries().next().value;q(n,u,o)}}},W=e=>{if(e.length===0)return;const[t,s]=M(e);if(t.delete(s),t.size===0)W(e.slice(0,-1));else if(t.size===1){const[n,u]=t.entries().next().value;n!==F&&q(e.slice(0,-1),n,u)}},q=(e,t,s)=>{if(e.length===0)return;const[n,u]=M(e);n.set(u+t,s),n.delete(u)},M=e=>e[e.length-1],it=(e,t)=>{const s=e._idToShortId.get(t);if(s!=null)return e._storedFields.get(s)},rt=/[\n\r -#%-*,-/:;?@[-\]_{}\u00A0\u00A1\u00A7\u00AB\u00B6\u00B7\u00BB\u00BF\u037E\u0387\u055A-\u055F\u0589\u058A\u05BE\u05C0\u05C3\u05C6\u05F3\u05F4\u0609\u060A\u060C\u060D\u061B\u061E\u061F\u066A-\u066D\u06D4\u0700-\u070D\u07F7-\u07F9\u0830-\u083E\u085E\u0964\u0965\u0970\u09FD\u0A76\u0AF0\u0C77\u0C84\u0DF4\u0E4F\u0E5A\u0E5B\u0F04-\u0F12\u0F14\u0F3A-\u0F3D\u0F85\u0FD0-\u0FD4\u0FD9\u0FDA\u104A-\u104F\u10FB\u1360-\u1368\u1400\u166E\u1680\u169B\u169C\u16EB-\u16ED\u1735\u1736\u17D4-\u17D6\u17D8-\u17DA\u1800-\u180A\u1944\u1945\u1A1E\u1A1F\u1AA0-\u1AA6\u1AA8-\u1AAD\u1B5A-\u1B60\u1BFC-\u1BFF\u1C3B-\u1C3F\u1C7E\u1C7F\u1CC0-\u1CC7\u1CD3\u2000-\u200A\u2010-\u2029\u202F-\u2043\u2045-\u2051\u2053-\u205F\u207D\u207E\u208D\u208E\u2308-\u230B\u2329\u232A\u2768-\u2775\u27C5\u27C6\u27E6-\u27EF\u2983-\u2998\u29D8-\u29DB\u29FC\u29FD\u2CF9-\u2CFC\u2CFE\u2CFF\u2D70\u2E00-\u2E2E\u2E30-\u2E4F\u3000-\u3003\u3008-\u3011\u3014-\u301F\u3030\u303D\u30A0\u30FB\uA4FE\uA4FF\uA60D-\uA60F\uA673\uA67E\uA6F2-\uA6F7\uA874-\uA877\uA8CE\uA8CF\uA8F8-\uA8FA\uA8FC\uA92E\uA92F\uA95F\uA9C1-\uA9CD\uA9DE\uA9DF\uAA5C-\uAA5F\uAADE\uAADF\uAAF0\uAAF1\uABEB\uFD3E\uFD3F\uFE10-\uFE19\uFE30-\uFE52\uFE54-\uFE61\uFE63\uFE68\uFE6A\uFE6B\uFF01-\uFF03\uFF05-\uFF0A\uFF0C-\uFF0F\uFF1A\uFF1B\uFF1F\uFF20\uFF3B-\uFF3D\uFF3F\uFF5B\uFF5D\uFF5F-\uFF65]+/u,S="or",$="and",ct="and_not",lt=(e,t)=>{e.includes(t)||e.push(t)},P=(e,t)=>{for(const s of t)e.includes(s)||e.push(s)},N=({score:e},{score:t})=>t-e,ht=()=>new Map,k=e=>{const t=new Map;for(const s of Object.keys(e))t.set(parseInt(s,10),e[s]);return t},G=(e,t)=>Object.prototype.hasOwnProperty.call(e,t)?e[t]:void 0,dt={[S]:(e,t)=>{for(const s of t.keys()){const n=e.get(s);if(n==null)e.set(s,t.get(s));else{const{score:u,terms:o,match:i}=t.get(s);n.score=n.score+u,n.match=Object.assign(n.match,i),P(n.terms,o)}}return e},[$]:(e,t)=>{const s=new Map;for(const n of t.keys()){const u=e.get(n);if(u==null)continue;const{score:o,terms:i,match:r}=t.get(n);P(u.terms,i),s.set(n,{score:u.score+o,terms:u.terms,match:Object.assign(u.match,r)})}return s},[ct]:(e,t)=>{for(const s of t.keys())e.delete(s);return e}},at=(e,t,s,n,u,o)=>{const{k:i,b:r,d}=o;return Math.log(1+(s-t+.5)/(t+.5))*(d+e*(i+1)/(e+i*(1-r+r*n/u)))},ft=e=>(t,s,n)=>{const u=typeof e.fuzzy=="function"?e.fuzzy(t,s,n):e.fuzzy||!1,o=typeof e.prefix=="function"?e.prefix(t,s,n):e.prefix===!0;return{term:t,fuzzy:u,prefix:o}},H=(e,t,s,n)=>{for(const u of Object.keys(e._fieldIds))if(e._fieldIds[u]===s){e._options.logger("warn",`SlimSearch: document with ID ${e._documentIds.get(t)} has changed before removal: term "${n}" was not present in field "${u}". Removing a document after it has changed can corrupt the index!`,"version_conflict");return}},gt=(e,t,s,n)=>{if(!e._index.has(n)){H(e,s,t,n);return}const u=e._index.fetch(n,ht),o=u.get(t);o==null||o.get(s)==null?H(e,s,t,n):o.get(s)<=1?o.size<=1?u.delete(t):o.delete(s):o.set(s,o.get(s)-1),e._index.get(n).size===0&&e._index.delete(n)},mt={k:1.2,b:.7,d:.5},pt={idField:"id",extractField:(e,t)=>e[t],tokenize:e=>e.split(rt),processTerm:e=>e.toLowerCase(),fields:void 0,searchOptions:void 0,storeFields:[],logger:(e,t)=>{typeof console?.[e]=="function"&&console[e](t)},autoVacuum:!0},J={combineWith:S,prefix:!1,fuzzy:!1,maxFuzzy:6,boost:{},weights:{fuzzy:.45,prefix:.375},bm25:mt},Ft={combineWith:$,prefix:(e,t,s)=>t===s.length-1},_t={batchSize:1e3,batchWait:10},U={minDirtFactor:.1,minDirtCount:20},yt={..._t,...U},Y=(e,t=S)=>{if(e.length===0)return new Map;const s=t.toLowerCase();return e.reduce(dt[s])||new Map},B=(e,t,s,n,u,o,i,r,d=new Map)=>{if(u==null)return d;for(const l of Object.keys(o)){const a=o[l],h=e._fieldIds[l],m=u.get(h);if(m==null)continue;let p=m.size;const f=e._avgFieldLength[h];for(const c of m.keys()){if(!e._documentIds.has(c)){gt(e,h,c,s),p-=1;continue}const g=i?i(e._documentIds.get(c),s,e._storedFields.get(c)):1;if(!g)continue;const _=m.get(c),y=e._fieldLength.get(c)[h],b=at(_,p,e._documentCount,y,f,r),z=n*a*g*b,A=d.get(c);if(A){A.score+=z,lt(A.terms,t);const w=G(A.match,s);w?w.push(l):A.match[s]=[l]}else d.set(c,{score:z,terms:[t],match:{[s]:[l]}})}}return d},At=(e,t,s)=>{const n={...e._options.searchOptions,...s},u=(n.fields||e._options.fields).reduce((c,g)=>({...c,[g]:G(n.boost,g)||1}),{}),{boostDocument:o,weights:i,maxFuzzy:r,bm25:d}=n,{fuzzy:l,prefix:a}={...J.weights,...i},h=e._index.get(t.term),m=B(e,t.term,t.term,1,h,u,o,d);let p,f;if(t.prefix&&(p=e._index.atPrefix(t.term)),t.fuzzy){const c=t.fuzzy===!0?.2:t.fuzzy,g=c<1?Math.min(r,Math.round(t.term.length*c)):c;g&&(f=e._index.fuzzyGet(t.term,g))}if(p)for(const[c,g]of p){const _=c.length-t.term.length;if(!_)continue;f?.delete(c);const y=a*c.length/(c.length+.3*_);B(e,t.term,c,y,g,u,o,d,m)}if(f)for(const c of f.keys()){const[g,_]=f.get(c);if(!_)continue;const y=l*c.length/(c.length+_);B(e,t.term,c,y,g,u,o,d,m)}return m},X=(e,t,s={})=>{if(typeof t!="string"){const a={...s,...t,queries:void 0},h=t.queries.map(m=>X(e,m,a));return Y(h,a.combineWith)}const{tokenize:n,processTerm:u,searchOptions:o}=e._options,i={tokenize:n,processTerm:u,...o,...s},{tokenize:r,processTerm:d}=i,l=r(t).flatMap(a=>d(a)).filter(a=>!!a).map(ft(i)).map(a=>At(e,a,i));return Y(l,i.combineWith)},K=(e,t,s={})=>{const n=X(e,t,s),u=[];for(const[o,{score:i,terms:r,match:d}]of n){const l=r.length,a={id:e._documentIds.get(o),score:i*l,terms:Object.keys(d),match:d};Object.assign(a,e._storedFields.get(o)),(s.filter==null||s.filter(a))&&u.push(a)}return u.sort(N),u},Ct=(e,t,s={})=>{s={...e._options.autoSuggestOptions,...s};const n=new Map;for(const{score:o,terms:i}of K(e,t,s)){const r=i.join(" "),d=n.get(r);d!=null?(d.score+=o,d.count+=1):n.set(r,{score:o,terms:i,count:1})}const u=[];for(const[o,{score:i,terms:r,count:d}]of n)u.push({suggestion:o,terms:r,score:i/d});return u.sort(N),u};class Et{_options;_index;_documentCount;_documentIds;_idToShortId;_fieldIds;_fieldLength;_avgFieldLength;_nextId;_storedFields;_dirtCount;_currentVacuum;_enqueuedVacuum;_enqueuedVacuumConditions;constructor(t){if(t?.fields==null)throw new Error('SlimSearch: option "fields" must be provided');const s=t.autoVacuum==null||t.autoVacuum===!0?yt:t.autoVacuum;this._options={...pt,...t,autoVacuum:s,searchOptions:{...J,...t.searchOptions||{}},autoSuggestOptions:{...Ft,...t.autoSuggestOptions||{}}},this._index=new C,this._documentCount=0,this._documentIds=new Map,this._idToShortId=new Map,this._fieldIds={},this._fieldLength=new Map,this._avgFieldLength=[],this._nextId=0,this._storedFields=new Map,this._dirtCount=0,this._currentVacuum=null,this._enqueuedVacuum=null,this._enqueuedVacuumConditions=U,this.addFields(this._options.fields)}get isVacuuming(){return this._currentVacuum!=null}get dirtCount(){return this._dirtCount}get dirtFactor(){return this._dirtCount/(1+this._documentCount+this._dirtCount)}get documentCount(){return this._documentCount}get termCount(){return this._index.size}toJSON(){const t=[];for(const[s,n]of this._index){const u={};for(const[o,i]of n)u[o]=Object.fromEntries(i);t.push([s,u])}return{documentCount:this._documentCount,nextId:this._nextId,documentIds:Object.fromEntries(this._documentIds),fieldIds:this._fieldIds,fieldLength:Object.fromEntries(this._fieldLength),averageFieldLength:this._avgFieldLength,storedFields:Object.fromEntries(this._storedFields),dirtCount:this._dirtCount,index:t,serializationVersion:2}}addFields(t){for(let s=0;s<t.length;s++)this._fieldIds[t[s]]=s}}const zt=({index:e,documentCount:t,nextId:s,documentIds:n,fieldIds:u,fieldLength:o,averageFieldLength:i,storedFields:r,dirtCount:d,serializationVersion:l},a)=>{if(l!==1&&l!==2)throw new Error("SlimSearch: cannot deserialize an index created with an incompatible version");const h=new Et(a);h._documentCount=t,h._nextId=s,h._documentIds=k(n),h._idToShortId=new Map,h._fieldIds=u,h._fieldLength=k(o),h._avgFieldLength=i,h._storedFields=k(r),h._dirtCount=d||0,h._index=new C;for(const[m,p]of h._documentIds)h._idToShortId.set(p,m);for(const[m,p]of e){const f=new Map;for(const c of Object.keys(p)){let g=p[c];l===1&&(g=g.ds),f.set(parseInt(c,10),k(g))}h._index.set(m,f)}return h},Q=Object.entries,wt=Object.fromEntries,j=(e,t)=>{const s=e.toLowerCase(),n=t.toLowerCase(),u=[];let o=0,i=0;const r=(l,a=!1)=>{let h="";i===0?h=l.length>20?`… ${l.slice(-20)}`:l:a?h=l.length+i>100?`${l.slice(0,100-i)}… `:l:h=l.length>20?`${l.slice(0,20)} … ${l.slice(-20)}`:l,h&&u.push(h),i+=h.length,a||(u.push(["mark",t]),i+=t.length,i>=100&&u.push(" …"))};let d=s.indexOf(n,o);if(d===-1)return null;for(;d>=0;){const l=d+n.length;if(r(e.slice(o,d)),o=l,i>100)break;d=s.indexOf(n,o)}return i<100&&r(e.slice(o),!0),u},Z=/[\u4e00-\u9fa5]/g,tt=(e={})=>({fuzzy:.2,prefix:!0,processTerm:t=>{const s=t.match(Z)||[],n=t.replace(Z,"").toLowerCase();return n?[n,...s]:[...s]},...e}),xt=(e,t)=>t.contents.reduce((s,[,n])=>s+n,0)-e.contents.reduce((s,[,n])=>s+n,0),kt=(e,t)=>Math.max(...t.contents.map(([,s])=>s))-Math.max(...e.contents.map(([,s])=>s)),et=(e,t,s={})=>{const n={};return K(t,e,tt({boost:{h:2,t:1,c:4},...s})).forEach(u=>{const{id:o,terms:i,score:r}=u,d=o.includes("@"),l=o.includes("#"),[a,h]=o.split(/[#@]/),m=i.sort((f,c)=>f.length-c.length).filter((f,c)=>i.slice(c+1).every(g=>!g.includes(f))),{contents:p}=n[a]??={title:"",contents:[]};if(d)p.push([{type:"customField",key:a,index:h,display:m.map(f=>u.c.map(c=>j(c,f))).flat().filter(f=>f!==null)},r]);else{const f=m.map(c=>j(u.h,c)).filter(c=>c!==null);if(f.length&&p.push([{type:l?"heading":"title",key:a,...l&&{anchor:h},display:f},r]),"t"in u)for(const c of u.t){const g=m.map(_=>j(c,_)).filter(_=>_!==null);g.length&&p.push([{type:"text",key:a,...l&&{anchor:h},display:g},r])}}}),Q(n).sort(([,u],[,o])=>"max"==="total"?xt(u,o):kt(u,o)).map(([u,{title:o,contents:i}])=>{if(!o){const r=it(t,u);r&&(o=r.h)}return{title:o,contents:i.map(([r])=>r)}})},st=(e,t,s={})=>Ct(t,e,tt(s)).map(({suggestion:n})=>n),v=wt(Q(JSON.parse("{\"/\":{\"documentCount\":315,\"nextId\":315,\"documentIds\":{\"0\":\"v-8daa1a0e\",\"1\":\"v-184f4da6\",\"2\":\"v-2e3eac9e\",\"3\":\"v-2e3eac9e#幻灯片演示\",\"4\":\"v-2e3eac9e#标注幻灯片\",\"5\":\"v-2e3eac9e#标注幻灯片-1\",\"6\":\"v-2e3eac9e#markdown\",\"7\":\"v-2e3eac9e#markdown-1\",\"8\":\"v-2e3eac9e#这是一个-h3\",\"9\":\"v-2e3eac9e#markdown-2\",\"10\":\"v-2e3eac9e#markdown-3\",\"11\":\"v-2e3eac9e#markdown-4\",\"12\":\"v-2e3eac9e#markdown-5\",\"13\":\"v-2e3eac9e#布局\",\"14\":\"v-2e3eac9e#布局-1\",\"15\":\"v-2e3eac9e#布局-2\",\"16\":\"v-2e3eac9e#布局-3\",\"17\":\"v-2e3eac9e#背景\",\"18\":\"v-2e3eac9e#动画片段\",\"19\":\"v-2e3eac9e#动画片段-1\",\"20\":\"v-2e3eac9e#动画片段-2\",\"21\":\"v-2e3eac9e#动画-class\",\"22\":\"v-2e3eac9e#动画片段-3\",\"23\":\"v-2e3eac9e#动画-class-1\",\"24\":\"v-2e3eac9e#动画片段-4\",\"25\":\"v-2e3eac9e#多个动画片段\",\"26\":\"v-2e3eac9e#动画片段-5\",\"27\":\"v-2e3eac9e#顺序\",\"28\":\"v-2e3eac9e#渐变\",\"29\":\"v-2e3eac9e#渐变-1\",\"30\":\"v-2e3eac9e#渐变-2\",\"31\":\"v-2e3eac9e#过渡动画\",\"32\":\"v-2e3eac9e#功能\",\"33\":\"v-2e3eac9e#功能-1\",\"34\":\"v-2e3eac9e#代码\",\"35\":\"v-2e3eac9e#功能-2\",\"36\":\"v-2e3eac9e#预览模式\",\"37\":\"v-2e3eac9e#功能-3\",\"38\":\"v-2e3eac9e#全屏模式\",\"39\":\"v-2e3eac9e#功能-4\",\"40\":\"v-2e3eac9e#缩放\",\"41\":\"v-2e3eac9e#结束\",\"42\":\"v-1549ef80\",\"43\":\"v-1549ef80#第一天\",\"44\":\"v-1549ef80#南京博物馆\",\"45\":\"v-1549ef80#玄武湖\",\"46\":\"v-1549ef80#明城墙-夫子庙秦淮河夜游\",\"47\":\"v-a94a8cca\",\"48\":\"v-1473bf53\",\"49\":\"v-1473bf53#目录\",\"50\":\"v-1473bf53@0\",\"51\":\"v-4e65ec78\",\"52\":\"v-4e65ec78@0\",\"53\":\"v-4e65ec78@1\",\"54\":\"v-c151bf32\",\"55\":\"v-c151bf32@0\",\"56\":\"v-c151bf32@1\",\"57\":\"v-438ffe52\",\"58\":\"v-438ffe52#markdown-介绍\",\"59\":\"v-438ffe52#markdown-配置\",\"60\":\"v-438ffe52#markdown-扩展\",\"61\":\"v-438ffe52#vuepress-扩展\",\"62\":\"v-438ffe52#主题扩展\",\"63\":\"v-438ffe52#提示容器\",\"64\":\"v-438ffe52#代码块\",\"65\":\"v-438ffe52#上下角标\",\"66\":\"v-438ffe52#自定义对齐\",\"67\":\"v-438ffe52#attrs\",\"68\":\"v-438ffe52#脚注\",\"69\":\"v-438ffe52#标记\",\"70\":\"v-438ffe52#任务列表\",\"71\":\"v-438ffe52#图片增强\",\"72\":\"v-438ffe52#组件\",\"73\":\"v-438ffe52#导入文件\",\"74\":\"v-438ffe52#代码演示\",\"75\":\"v-438ffe52#样式化\",\"76\":\"v-438ffe52#交互演示\",\"77\":\"v-438ffe52#图表\",\"78\":\"v-438ffe52#echarts\",\"79\":\"v-438ffe52#流程图\",\"80\":\"v-438ffe52#mermaid\",\"81\":\"v-438ffe52#tex-语法\",\"82\":\"v-438ffe52#vue-交互演示\",\"83\":\"v-438ffe52@0\",\"84\":\"v-438ffe52@1\",\"85\":\"v-6e19edb7\",\"86\":\"v-6e19edb7#页面信息\",\"87\":\"v-6e19edb7#页面内容\",\"88\":\"v-6e19edb7#页面结构\",\"89\":\"v-6e19edb7@0\",\"90\":\"v-6e19edb7@1\",\"91\":\"v-19d854d8\",\"92\":\"v-19d854d8#提示容器\",\"93\":\"v-19d854d8#信息容器\",\"94\":\"v-19d854d8#提示容器-1\",\"95\":\"v-19d854d8#警告容器\",\"96\":\"v-19d854d8#危险容器\",\"97\":\"v-19d854d8#重要容器\",\"98\":\"v-19d854d8#详情容器\",\"99\":\"v-19d854d8#选项卡\",\"100\":\"v-19d854d8#对齐\",\"101\":\"v-19d854d8#属性支持\",\"102\":\"v-19d854d8#id\",\"103\":\"v-19d854d8#脚注\",\"104\":\"v-19d854d8#任务列表\",\"105\":\"v-19d854d8@0\",\"106\":\"v-19d854d8@1\",\"107\":\"v-62633a0e\",\"108\":\"v-2d0a830e\",\"109\":\"v-5357408c\",\"110\":\"v-5357408c#基本概念\",\"111\":\"v-5357408c#函数间的距离\",\"112\":\"v-5357408c#变分\",\"113\":\"v-5357408c#最简泛函的变分\",\"114\":\"v-14f0dace\",\"115\":\"v-030f8c47\",\"116\":\"v-2d0aa4d7\",\"117\":\"v-2bc6566a\",\"118\":\"v-2bc6566a#标题-2\",\"119\":\"v-2bc6566a#标题-3\",\"120\":\"v-2bc6566a@0\",\"121\":\"v-2bc6566a@1\",\"122\":\"v-24b7c48d\",\"123\":\"v-24b7c48d#标题-2\",\"124\":\"v-24b7c48d#标题-3\",\"125\":\"v-24b7c48d@0\",\"126\":\"v-24b7c48d@1\",\"127\":\"v-2789ee54\",\"128\":\"v-2789ee54@0\",\"129\":\"v-2789ee54@1\",\"130\":\"v-f0ec4556\",\"131\":\"v-f0ec4556#标题-2\",\"132\":\"v-f0ec4556#标题-3\",\"133\":\"v-f0ec4556@0\",\"134\":\"v-f0ec4556@1\",\"135\":\"v-df8b6e0c\",\"136\":\"v-df8b6e0c#标题-2\",\"137\":\"v-df8b6e0c#标题-3\",\"138\":\"v-df8b6e0c@0\",\"139\":\"v-df8b6e0c@1\",\"140\":\"v-3f615d37\",\"141\":\"v-3f615d37#分发饼干\",\"142\":\"v-3f615d37#排序-贪心法-双指针\",\"143\":\"v-3f615d37#代码\",\"144\":\"v-3f615d37#注意事项\",\"145\":\"v-3f615d37#其他\",\"146\":\"v-3f615d37#有效的山脉数组\",\"147\":\"v-3f615d37#寻找数组的中心下标\",\"148\":\"v-3f615d37#按奇偶排序数组ii\",\"149\":\"v-3f615d37#旋转数组\",\"150\":\"v-3f615d37#使用额外的空间\",\"151\":\"v-3f615d37#数组反转\",\"152\":\"v-3f615d37#区间问题\",\"153\":\"v-3f615d37#插入区间\",\"154\":\"v-3f615d37#模拟法\",\"155\":\"v-3f615d37#环形数组\",\"156\":\"v-3f615d37#矩阵\",\"157\":\"v-3f615d37#螺旋矩阵\",\"158\":\"v-3f615d37#螺旋矩阵-ii\",\"159\":\"v-3f615d37#旋转图像\",\"160\":\"v-3f615d37#矩阵置零\",\"161\":\"v-3f615d37#扑克牌中的顺子\",\"162\":\"v-9a72c052\",\"163\":\"v-9a72c052#目录\",\"164\":\"v-64a42731\",\"165\":\"v-9b449d4a\",\"166\":\"v-9b449d4a#密度估计\",\"167\":\"v-11ce3d6c\",\"168\":\"v-12210fd9\",\"169\":\"v-12210fd9#目录\",\"170\":\"v-6d2b83a6\",\"171\":\"v-6d2b83a6#向量和向量空间\",\"172\":\"v-6d2b83a6#向量\",\"173\":\"v-6d2b83a6#向量空间\",\"174\":\"v-2d035e0f\",\"175\":\"v-2d035e0f#容器\",\"176\":\"v-2d035e0f#信息容器\",\"177\":\"v-2d035e0f#提示容器\",\"178\":\"v-2d035e0f#警告容器\",\"179\":\"v-2d035e0f#危险容器\",\"180\":\"v-2d035e0f#详情容器\",\"181\":\"v-2d035e0f#选项卡\",\"182\":\"v-2d035e0f#对齐\",\"183\":\"v-2d035e0f#属性支持\",\"184\":\"v-2d035e0f#id\",\"185\":\"v-2d035e0f#脚注\",\"186\":\"v-2d035e0f#任务列表\",\"187\":\"v-3f2786dc\",\"188\":\"v-475db5be\",\"189\":\"v-018e6b19\",\"190\":\"v-018e6b19#不含隐变量的参数估计\",\"191\":\"v-018e6b19#有向图模型\",\"192\":\"v-018e6b19#无向图模型\",\"193\":\"v-018e6b19#含隐变量的参数估计\",\"194\":\"v-018e6b19#em算法\",\"195\":\"v-018e6b19#高斯混合模型\",\"196\":\"v-6cedb9f8\",\"197\":\"v-6cedb9f8#图模型的基本问题\",\"198\":\"v-6cedb9f8#图模型与机器学习\",\"199\":\"v-6cedb9f8#本章目录\",\"200\":\"v-6cedb9f8#资料\",\"201\":\"v-32a05752\",\"202\":\"v-32a05752#有向图模型\",\"203\":\"v-32a05752#常见的有向图模型\",\"204\":\"v-32a05752#sigmoid信念网络\",\"205\":\"v-32a05752#朴素贝叶斯分类器\",\"206\":\"v-32a05752#隐马尔可夫模型\",\"207\":\"v-32a05752#无向图模型\",\"208\":\"v-32a05752#无向图模型的概率分解\",\"209\":\"v-32a05752#常见的无向图模型\",\"210\":\"v-32a05752#对数线性模型\",\"211\":\"v-32a05752#条件随机场\",\"212\":\"v-32a05752#有向图和无向图直接的转换\",\"213\":\"v-32a05752#参考\",\"214\":\"v-2ff46f0a\",\"215\":\"v-2ff46f0a#基本概念\",\"216\":\"v-2ff46f0a#对抗鲁棒性\",\"217\":\"v-2ff46f0a#对抗攻击的分类\",\"218\":\"v-2ff46f0a#fgsm\",\"219\":\"v-2ff46f0a#简介\",\"220\":\"v-2ff46f0a#对抗样本的线性解释\",\"221\":\"v-2ff46f0a#非线性模型的线性扰动\",\"222\":\"v-2ff46f0a#线性模型的对抗训练与权重衰减的对比研究\",\"223\":\"v-2ff46f0a#pgd\",\"224\":\"v-6e796960\",\"225\":\"v-6e796960#notation\",\"226\":\"v-6e796960#激活函数的线性上上下界\",\"227\":\"v-6e796960#定理中涉及的符号\",\"228\":\"v-2d88cdd0\",\"229\":\"v-e1a99a1e\",\"230\":\"v-4ed6beed\",\"231\":\"v-4ed6beed#相平面\",\"232\":\"v-0815c951\",\"233\":\"v-39fc050c\",\"234\":\"v-39fc050c#预备知识\",\"235\":\"v-39fc050c#euler法\",\"236\":\"v-39fc050c#talyot法\",\"237\":\"v-39fc050c#runge-kutta法\",\"238\":\"v-ff265f48\",\"239\":\"v-ff265f48#参数密度估计\",\"240\":\"v-ff265f48#正态分布\",\"241\":\"v-ff265f48#多项分布\",\"242\":\"v-ac61aa4a\",\"243\":\"v-67b8c712\",\"244\":\"v-67b8c712#标题-2\",\"245\":\"v-67b8c712#标题-3\",\"246\":\"v-67b8c712@0\",\"247\":\"v-67b8c712@1\",\"248\":\"v-696d9fb1\",\"249\":\"v-696d9fb1#标题-2\",\"250\":\"v-696d9fb1#标题-3\",\"251\":\"v-696d9fb1@0\",\"252\":\"v-696d9fb1@1\",\"253\":\"v-6b227850\",\"254\":\"v-6b227850#标题-2\",\"255\":\"v-6b227850#标题-3\",\"256\":\"v-6b227850@0\",\"257\":\"v-6b227850@1\",\"258\":\"v-6cd750ef\",\"259\":\"v-6cd750ef#标题-2\",\"260\":\"v-6cd750ef#标题-3\",\"261\":\"v-6cd750ef@0\",\"262\":\"v-6cd750ef@1\",\"263\":\"v-7a07405d\",\"264\":\"v-7a07405d#标题-2\",\"265\":\"v-7a07405d#标题-3\",\"266\":\"v-7a07405d@0\",\"267\":\"v-7a07405d@1\",\"268\":\"v-7bbc18fc\",\"269\":\"v-7bbc18fc#标题-2\",\"270\":\"v-7bbc18fc#标题-3\",\"271\":\"v-7bbc18fc@0\",\"272\":\"v-7bbc18fc@1\",\"273\":\"v-7d70f19b\",\"274\":\"v-7d70f19b#标题-2\",\"275\":\"v-7d70f19b#标题-3\",\"276\":\"v-7d70f19b@0\",\"277\":\"v-7d70f19b@1\",\"278\":\"v-7f25ca3a\",\"279\":\"v-7f25ca3a#标题-2\",\"280\":\"v-7f25ca3a#标题-3\",\"281\":\"v-7f25ca3a@0\",\"282\":\"v-7f25ca3a@1\",\"283\":\"v-7f8139e0\",\"284\":\"v-ea40b9d6\",\"285\":\"v-ea40b9d6#重参数化技巧\",\"286\":\"v-48487511\",\"287\":\"v-42d90fba\",\"288\":\"v-42d90fba#自适应检查点伴随法\",\"289\":\"v-42d90fba@1\",\"290\":\"v-4c4a0803\",\"291\":\"v-3c543444\",\"292\":\"v-1d45d382\",\"293\":\"v-1d45d382#增维的的node\",\"294\":\"v-1d45d382#对输入做仿射变换\",\"295\":\"v-1d45d382#随机化额外维度的初始值\",\"296\":\"v-1d45d382#分段的node\",\"297\":\"v-a8d6da06\",\"298\":\"v-a8d6da06#notation\",\"299\":\"v-a8d6da06#同胚和流形假设\",\"300\":\"v-a8d6da06#增维的node\",\"301\":\"v-a8d6da06@1\",\"302\":\"v-10859c10\",\"303\":\"v-56238b0d\",\"304\":\"v-56238b0d#介绍\",\"305\":\"v-56238b0d#详情\",\"306\":\"v-558b68aa\",\"307\":\"v-562590ba\",\"308\":\"v-562590ba#介绍\",\"309\":\"v-562590ba#详情\",\"310\":\"v-8ff76cae\",\"311\":\"v-0011afe4\",\"312\":\"v-e1e3da16\",\"313\":\"v-08f42f4a\",\"314\":\"v-30be3cd5\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[1,11],\"1\":[1,3],\"2\":[1,2],\"3\":[1,5],\"4\":[1,2],\"5\":[1,12],\"6\":[1,4],\"7\":[1,4],\"8\":[2,11],\"9\":[1,11],\"10\":[1,12],\"11\":[1,10],\"12\":[1,8],\"13\":[1,1],\"14\":[1,7],\"15\":[1,8],\"16\":[1],\"17\":[1,5],\"18\":[1,1],\"19\":[1,6],\"20\":[1],\"21\":[2,10],\"22\":[1],\"23\":[2,9],\"24\":[1],\"25\":[1,8],\"26\":[1],\"27\":[1,9],\"28\":[1,1],\"29\":[1,15],\"30\":[1],\"31\":[1,8],\"32\":[1,1],\"33\":[1],\"34\":[1,20],\"35\":[1],\"36\":[1,6],\"37\":[1],\"38\":[1,6],\"39\":[1],\"40\":[1,9],\"41\":[1,2],\"42\":[1,2],\"43\":[1],\"44\":[1,21],\"45\":[1,19],\"46\":[1,63],\"47\":[1],\"48\":[1],\"49\":[1,5],\"50\":[null,null,1],\"51\":[1,18],\"52\":[null,null,1],\"53\":[null,null,1],\"54\":[1,7],\"55\":[null,null,1],\"56\":[null,null,1],\"57\":[2,11],\"58\":[2,8],\"59\":[2,13],\"60\":[2,11],\"61\":[2,10],\"62\":[1,10],\"63\":[1,19],\"64\":[1,1],\"65\":[1,3],\"66\":[1,3],\"67\":[1,5],\"68\":[1,3],\"69\":[1,3],\"70\":[1,5],\"71\":[1,2],\"72\":[1,23],\"73\":[1,6],\"74\":[1,1],\"75\":[1,5],\"76\":[1,1],\"77\":[1,25],\"78\":[1,26],\"79\":[1,12],\"80\":[1,19],\"81\":[2,1],\"82\":[2,7],\"83\":[null,null,1],\"84\":[null,null,1],\"85\":[1,3],\"86\":[1,20],\"87\":[1,17],\"88\":[1,16],\"89\":[null,null,1],\"90\":[null,null,2],\"91\":[2],\"92\":[1],\"93\":[1,16],\"94\":[1,4],\"95\":[1,4],\"96\":[1,4],\"97\":[1,4],\"98\":[1,4],\"99\":[1,16],\"100\":[1,5],\"101\":[1],\"102\":[1,10],\"103\":[1,3],\"104\":[1,6],\"105\":[null,null,1],\"106\":[null,null,1],\"107\":[1],\"108\":[1],\"109\":[1,57],\"110\":[1],\"111\":[1,71],\"112\":[1,88],\"113\":[1,103],\"114\":[1],\"115\":[1],\"116\":[1],\"117\":[1],\"118\":[2,2],\"119\":[2,2],\"120\":[null,null,1],\"121\":[null,null,3],\"122\":[1],\"123\":[2,2],\"124\":[2,2],\"125\":[null,null,2],\"126\":[null,null,2],\"127\":[1],\"128\":[null,null,2],\"129\":[null,null,2],\"130\":[1],\"131\":[2,2],\"132\":[2,2],\"133\":[null,null,2],\"134\":[null,null,2],\"135\":[1,1],\"136\":[2,2],\"137\":[2,2],\"138\":[null,null,1],\"139\":[null,null,2],\"140\":[1],\"141\":[1,5],\"142\":[1,22],\"143\":[1,39],\"144\":[1,46],\"145\":[1,28],\"146\":[1],\"147\":[1,7],\"148\":[1,74],\"149\":[1,5],\"150\":[1,23],\"151\":[1,19],\"152\":[1],\"153\":[1,30],\"154\":[1,40],\"155\":[1,6],\"156\":[1],\"157\":[1],\"158\":[2],\"159\":[1],\"160\":[1],\"161\":[1,7],\"162\":[1],\"163\":[1,2],\"164\":[1],\"165\":[1,2],\"166\":[1,50],\"167\":[1,49],\"168\":[1],\"169\":[1,5],\"170\":[1,5],\"171\":[1],\"172\":[1,26],\"173\":[1],\"174\":[1],\"175\":[1],\"176\":[1,16],\"177\":[1,4],\"178\":[1,4],\"179\":[1,4],\"180\":[1,4],\"181\":[1,16],\"182\":[1,5],\"183\":[1],\"184\":[1,5],\"185\":[1,3],\"186\":[1,6],\"187\":[1,5],\"188\":[1,1],\"189\":[1,11],\"190\":[1,4],\"191\":[1,34],\"192\":[1,48],\"193\":[1,5],\"194\":[1,122],\"195\":[1,99],\"196\":[1,111],\"197\":[1,14],\"198\":[1,10],\"199\":[1,1],\"200\":[1,11],\"201\":[1,35],\"202\":[1,104],\"203\":[1,5],\"204\":[1,50],\"205\":[1,40],\"206\":[1,29],\"207\":[1,40],\"208\":[1,68],\"209\":[1,7],\"210\":[1,29],\"211\":[1],\"212\":[1],\"213\":[1,37],\"214\":[1],\"215\":[1,8],\"216\":[1,8],\"217\":[1,21],\"218\":[1,21],\"219\":[1,5],\"220\":[1,40],\"221\":[1,166],\"222\":[1],\"223\":[1,13],\"224\":[1,7],\"225\":[1,29],\"226\":[1,30],\"227\":[1],\"228\":[1,2],\"229\":[1],\"230\":[1,55],\"231\":[1,10],\"232\":[1],\"233\":[1],\"234\":[1,83],\"235\":[1,46],\"236\":[1,15],\"237\":[2,5],\"238\":[1,11],\"239\":[1,20],\"240\":[1,13],\"241\":[1,44],\"242\":[1,60],\"243\":[2],\"244\":[2,2],\"245\":[2,2],\"246\":[null,null,1],\"247\":[null,null,3],\"248\":[2,2],\"249\":[2,2],\"250\":[2,2],\"251\":[null,null,1],\"252\":[null,null,3],\"253\":[2],\"254\":[2,2],\"255\":[2,2],\"256\":[null,null,2],\"257\":[null,null,3],\"258\":[2],\"259\":[2,2],\"260\":[2,2],\"261\":[null,null,2],\"262\":[null,null,3],\"263\":[2],\"264\":[2,2],\"265\":[2,2],\"266\":[null,null,2],\"267\":[null,null,3],\"268\":[2,4],\"269\":[2,2],\"270\":[2,2],\"271\":[null,null,2],\"272\":[null,null,3],\"273\":[2],\"274\":[2,2],\"275\":[2,2],\"276\":[null,null,1],\"277\":[null,null,3],\"278\":[2],\"279\":[2,2],\"280\":[2,2],\"281\":[null,null,1],\"282\":[null,null,3],\"283\":[1,13],\"284\":[1,5],\"285\":[1,14],\"286\":[1,40],\"287\":[1],\"288\":[1,5],\"289\":[null,null,1],\"290\":[1,103],\"291\":[1,1],\"292\":[1],\"293\":[1,14],\"294\":[1,55],\"295\":[1,59],\"296\":[1,58],\"297\":[1],\"298\":[1,33],\"299\":[1,78],\"300\":[1,72],\"301\":[null,null,1],\"302\":[1,36],\"303\":[2],\"304\":[1,4],\"305\":[1,2],\"306\":[1,2],\"307\":[2],\"308\":[1,4],\"309\":[1,2],\"310\":[1,2],\"311\":[1,20],\"312\":[1],\"313\":[1],\"314\":[1]},\"averageFieldLength\":[1.1649303096277626,16.414777349222522,0.40635267583034945],\"storedFields\":{\"0\":{\"h\":\"博客主页\",\"t\":[\"这是一个博客主页的案例。\",\"要使用此布局，你应该在页面前端设置 layout: BlogHome 和 home: true。\",\"相关配置文档请见 博客主页。\"]},\"1\":{\"h\":\"个人介绍\",\"t\":[\"啦啦啦啦，我是Coding,Coding是我\"]},\"2\":{\"h\":\"幻灯片页\",\"t\":[\"@slidestart\"]},\"3\":{\"h\":\"幻灯片演示\",\"t\":[\"一个简单的幻灯片演示与各种小贴士。\",\"作者 Mr.Hope. 请滚动鼠标滚轮进入下一页\"]},\"4\":{\"h\":\"标注幻灯片\",\"t\":[\"👇\",\"--\"]},\"5\":{\"h\":\"标注幻灯片\",\"t\":[\"使用 --- 标注水平幻灯片\",\"在水平幻灯片中使用 -- 分割垂直幻灯片\",\"使用 <!-- .slide: ... --> 在幻灯片上添加属性\",\"使用 <!-- .element: ... --> 在前一个 HTML 元素上添加属性\"]},\"6\":{\"h\":\"Markdown\",\"t\":[\"你可以在幻灯片中使用 Markdown 语法的各种标记.\",\"--\"]},\"7\":{\"h\":\"Markdown\",\"t\":[\"你可以在幻灯片中使用 Markdown 语法的各种标记.\"]},\"8\":{\"h\":\"这是一个 H3\",\"t\":[\"标题默认会自动转换为大写。\",\"这是一个有着 粗体, 斜体, 删除线 文字并包含 一个链接 的段落，并且它会自动换行。所以你无需担心它的长度。\",\"--\"]},\"9\":{\"h\":\"Markdown\",\"t\":[\"你可以在幻灯片中使用 Markdown 语法的各种标记.\",\"列表默认为 inline-block\",\"项目\",\"项目\",\"项目\",\"项目 1\",\"项目 2\",\"项目 3\",\"--\"]},\"10\":{\"h\":\"Markdown\",\"t\":[\"你可以在幻灯片中使用 Markdown 语法的各种标记.\",\"在你启用 highlight 插件后，代码块会自动高亮。\",\"const a = 1; \",\"--\"]},\"11\":{\"h\":\"Markdown\",\"t\":[\"你可以在幻灯片中使用 Markdown 语法的各种标记.\",\"在你启用 math 插件后，你也可以使用 TEX 格式使用数学公式。\",\"--\"]},\"12\":{\"h\":\"Markdown\",\"t\":[\"你可以在幻灯片中使用 Markdown 语法的各种标记.\",\"⚠请注意: 表格和分割线，以及所有不在 Markdown 标准语法中的内容均不受支持。\"]},\"13\":{\"h\":\"布局\",\"t\":[\"--\"]},\"14\":{\"h\":\"布局\",\"t\":[\"👆 r-fit-text class 会让文字在不超出幻灯片范围的情况下尽可能大。\",\"--\"]},\"15\":{\"h\":\"布局\",\"t\":[\"Logo\",\"👆 r-stretch class 帮助你控制注入图片或视频的大小，使它们填充满幻灯片垂直方向上的剩余空间。\",\"--\"]},\"16\":{\"h\":\"布局\"},\"17\":{\"h\":\"背景\",\"t\":[\"你可以通过向特定幻灯片添加 data-background 属性自定义幻灯片背景.\"]},\"18\":{\"h\":\"动画片段\",\"t\":[\"--\"]},\"19\":{\"h\":\"动画片段\",\"t\":[\"动画片段用于高亮或显隐幻灯片中的元素。\",\"你需要在元素上添加 fragment 和动画 class。\",\"--\"]},\"20\":{\"h\":\"动画片段\"},\"21\":{\"h\":\"动画 class\",\"t\":[\"fade-in\",\"fade-out\",\"fade-up\",\"fade-down\",\"fade-left\",\"fade-right\",\"fade-in-then-out\",\"fade-in-then-semi-out\",\"--\"]},\"22\":{\"h\":\"动画片段\"},\"23\":{\"h\":\"动画 class\",\"t\":[\"grow\",\"shrink\",\"strike\",\"highlight-red\",\"highlight-green\",\"highlight-blue\",\"highlight-current-red\",\"highlight-current-green\",\"highlight-current-blue\",\"--\"]},\"24\":{\"h\":\"动画片段\"},\"25\":{\"h\":\"多个动画片段\",\"t\":[\"你可以按照顺序包裹一个 HTML 元素使其拥有多个动画片段 渐入 > 变红 > 渐出 \",\"--\"]},\"26\":{\"h\":\"动画片段\"},\"27\":{\"h\":\"顺序\",\"t\":[\"你可以使用 data-fragment-index 属性改变元素的动画顺序。\",\"不同元素可以有相同的动画顺序。\",\"最后显示\",\"第二个显示\",\"第一个显示\",\"第二个显示\"]},\"28\":{\"h\":\"渐变\",\"t\":[\"--\"]},\"29\":{\"h\":\"渐变\",\"t\":[\"Transition 可以通过配置中的 transition 选项全局设置，也可以通过在特定幻灯片添加 data-transition 属性局部设置.\",\"可能的值:\",\"none\",\"fade\",\"slide\",\"convex\",\"concave\",\"zoom\",\"--\"]},\"30\":{\"h\":\"渐变\"},\"31\":{\"h\":\"过渡动画\",\"t\":[\"你可以在相邻的幻灯片上添加 data-auto-animate 使相同的 HTML 元素产生过渡动画效果。\"]},\"32\":{\"h\":\"功能\",\"t\":[\"--\"]},\"33\":{\"h\":\"功能\"},\"34\":{\"h\":\"代码\",\"t\":[\"通过启用 highlight 插件，你可以对代码块进行高亮。\",\"你可以使用 [a-b|c-d] 语法来分布高亮特定行。\",\"let a = 1; let b = 2; let c = (x) => 1 + 2 + x; c(3); \",\"--\"]},\"35\":{\"h\":\"功能\"},\"36\":{\"h\":\"预览模式\",\"t\":[\"按下 Esc 或 O 即可在幻灯片获得焦点时进入预览模式。\",\"--\"]},\"37\":{\"h\":\"功能\"},\"38\":{\"h\":\"全屏模式\",\"t\":[\"按下 F 或 F11 即可在幻灯片获得焦点时进入全屏模式。\",\"--\"]},\"39\":{\"h\":\"功能\"},\"40\":{\"h\":\"缩放\",\"t\":[\"按下 alt (Linux 上使用 ctrl) 的同时点击幻灯片的任何元素，即可以向此元素进行放大。\",\"再次点击即可缩小。\"]},\"41\":{\"h\":\"结束\",\"t\":[\"@slideend\"]},\"42\":{\"h\":\"旅游之南京篇\",\"t\":[\"image-20240102213952358\"]},\"43\":{\"h\":\"第一天\"},\"44\":{\"h\":\"南京博物馆\",\"t\":[\"中国三大博物馆之一，虽然挂南京的名，但相当于其他的省博物馆。一院六馆各有特色。无论看展览、拍照还是集章，都是来南京的首选。\",\"重要\",\"镇馆之宝有：错银铜牛灯、鎏金镶嵌兽形铜盒砚、广陵王玺金印等等\",\"已预约 12:00-16:00 下午场\",\"南京博物馆\",\"image-20240102213830537\",\"image-20240102213837629\"]},\"45\":{\"h\":\"玄武湖\",\"t\":[\"南京最知名的一个湖，南京站南广场正对面，湖边看看花，走一走，感受南京人的后花园～ ⏰2小时起\",\"地铁1号线玄武湖\",\"已经预约 12:00 -19:00 场次\",\"重要\",\"可以搭配鸡鸣寺、南京城墙、情侣园等一同游玩\",\"image-20240102214059042\",\"image-20240102214118865\"]},\"46\":{\"h\":\"明城墙+夫子庙秦淮河夜游\",\"t\":[\"中华门城堡➠老门东➠乌衣巷➠秦淮河➠夫子庙➠江南贡院\",\"【中华门城堡】又称聚宝门，是国内保存zui完好的城墙堡垒，瓮城高大雄伟，在日落时分登城，看一看夕阳下的南京城真的绝了，在过年期间也会有也灯会夜游！ ⏰ 开放时间：8:30-20:30 🎫 门票50💰；游园卡可用；\",\"image-20240102215526117\",\"image-20240102215553996\",\"【老门东】三条营的龙灯已经亮了，城墙边的巨型龙灯也已经就位，今年的灯会声势浩大，不如趁人少先睹为快吧；南京小吃鸭血粉丝汤、小馄饨一定别错过，记得放辣油哦。 ⏰ 亮灯时间：17:30\",\"参考📍南京｜老门东最强攻略 (xiaohongshu.com)\",\"【夫子庙秦淮河】金陵入夜看秦淮，坐画舫夜游，从夫子庙前天下文枢牌坊下的洋池登船，沿途经过桃叶渡、白鹭洲公园、水街、东水关、回到浮池码头。 ⏰ 开放时间：日场：09:00-18:00；夜18:00-22:00 夜场需要在下午16:00前预约 🎫 船票：白天60💰、晚上80💰 \",\"参考：南京|夜游秦淮河攻略（含绝美机位） (xiaohongshu.com)\",\"重要\",\"游船上岸线路：夫子庙大成殿、江南贡院、乌衣巷、文德桥，感受下夫子庙接踵摩肩的热闹.\"]},\"47\":{\"h\":\"计算机\"},\"48\":{\"h\":\"主要功能与配置演示\"},\"49\":{\"h\":\"目录\",\"t\":[\"Markdown 展示\",\"页面展示\",\"禁用展示\",\"加密展示\"]},\"50\":{\"c\":[\"使用指南\"]},\"51\":{\"h\":\"布局与功能禁用\",\"t\":[\"你可以通过设置页面的 Frontmatter，在页面禁用功能与布局。\",\"本页面就是一个示例，禁用了如下功能:\",\"导航栏\",\"侧边栏\",\"路径导航\",\"页面信息\",\"贡献者\",\"编辑此页链接\",\"更新时间\",\"上一篇/下一篇 链接\",\"评论\",\"页脚\",\"返回顶部按钮\"]},\"52\":{\"c\":[\"使用指南\"]},\"53\":{\"c\":[\"禁用\"]},\"54\":{\"h\":\"密码加密的文章\",\"t\":[\"实际的文章内容。\",\"段落 1 文字段落 1 文字段落 1 文字段落 1 文字段落 1 文字段落 1 文字段落 1 文字段落 1 文字段落 1 文字段落 1 文字段落 1 文字段落 1 文字。\",\"段落 2 文字段落 2 文字段落 2 文字段落 2 文字段落 2 文字段落 2 文字段落 2 文字段落 2 文字段落 2 文字段落 2 文字段落 2 文字段落 2 文字段落 2 文字段落 2 文字。\"]},\"55\":{\"c\":[\"使用指南\"]},\"56\":{\"c\":[\"文章加密\"]},\"57\":{\"h\":\"Markdown 展示\",\"t\":[\"VuePress 主要从 Markdown 文件生成页面。因此，你可以使用它轻松生成文档或博客站点。\",\"你应该创建和编写 Markdown 文件，以便 VuePress 可以根据文件结构将它们转换为不同的页面。\"]},\"58\":{\"h\":\"Markdown 介绍\",\"t\":[\"如果你是一个新手，还不会编写 Markdown，请先阅读 Markdown 介绍 和 Markdown 演示。\"]},\"59\":{\"h\":\"Markdown 配置\",\"t\":[\"VuePress 通过 Frontmatter 为每个 Markdown 页面引入配置。\",\"相关信息\",\"Frontmatter 是 VuePress 中很重要的一个概念，如果你不了解它，你需要阅读 Frontmatter 介绍。\"]},\"60\":{\"h\":\"Markdown 扩展\",\"t\":[\"VuePress 会使用 markdown-it 来解析 Markdown 内容，因此可以借助于 markdown-it 插件来实现 语法扩展 。\"]},\"61\":{\"h\":\"VuePress 扩展\",\"t\":[\"为了丰富文档写作，VuePress 对 Markdown 语法进行了扩展。\",\"关于这些扩展，请阅读 VuePress 中的 Markdown 扩展。\"]},\"62\":{\"h\":\"主题扩展\",\"t\":[\"通过 vuepress-plugin-md-enhance，主题扩展了更多 Markdown 语法，提供更加丰富的写作功能。\"]},\"63\":{\"h\":\"提示容器\",\"t\":[\"安全的在 Markdown 中使用 {{ variable }}。\",\"自定义标题\",\"信息容器，包含 代码 与 链接。\",\"const a = 1; \",\"自定义标题\",\"提示容器\",\"自定义标题\",\"警告容器\",\"自定义标题\",\"危险容器\",\"自定义标题\",\"详情容器\",\"查看详情\"]},\"64\":{\"h\":\"代码块\",\"t\":[\"查看详情\"]},\"65\":{\"h\":\"上下角标\",\"t\":[\"19th H2O\",\"查看详情\"]},\"66\":{\"h\":\"自定义对齐\",\"t\":[\"我是居中的\",\"我在右对齐\",\"查看详情\"]},\"67\":{\"h\":\"Attrs\",\"t\":[\"一个拥有 ID 的 单词。\",\"查看详情\"]},\"68\":{\"h\":\"脚注\",\"t\":[\"此文字有脚注[1].\",\"查看详情\"]},\"69\":{\"h\":\"标记\",\"t\":[\"你可以标记 重要的内容 。\",\"查看详情\"]},\"70\":{\"h\":\"任务列表\",\"t\":[\" 计划 1\",\" 计划 2\",\"查看详情\"]},\"71\":{\"h\":\"图片增强\",\"t\":[\"支持为图片设置颜色模式和大小\",\"查看详情\"]},\"72\":{\"h\":\"组件\",\"t\":[\"title: Mr.Hope desc: Where there is light, there is hope logo: https://mister-hope.com/logo.svg link: https://mister-hope.com color: rgba(253, 230, 138, 0.15) \",\"查看详情\"]},\"73\":{\"h\":\"导入文件\",\"t\":[\"Markdown 展示\",\"页面展示\",\"禁用展示\",\"加密展示\",\"查看详情\"]},\"74\":{\"h\":\"代码演示\",\"t\":[\"查看详情\"]},\"75\":{\"h\":\"样式化\",\"t\":[\"向 Mr.Hope 捐赠一杯咖啡。 \",\"查看详情\"]},\"76\":{\"h\":\"交互演示\",\"t\":[\"查看详情\"]},\"77\":{\"h\":\"图表\",\"t\":[\"::: chart 一个散点图案例\",\"{ \\\"type\\\": \\\"scatter\\\", \\\"data\\\": { \\\"datasets\\\": [ { \\\"label\\\": \\\"散点数据集\\\", \\\"data\\\": [ { \\\"x\\\": -10, \\\"y\\\": 0 }, { \\\"x\\\": 0, \\\"y\\\": 10 }, { \\\"x\\\": 10, \\\"y\\\": 5 }, { \\\"x\\\": 0.5, \\\"y\\\": 5.5 } ], \\\"backgroundColor\\\": \\\"rgb(255, 99, 132)\\\" } ] }, \\\"options\\\": { \\\"scales\\\": { \\\"x\\\": { \\\"type\\\": \\\"linear\\\", \\\"position\\\": \\\"bottom\\\" } } } } \",\":::\",\"查看详情\"]},\"78\":{\"h\":\"Echarts\",\"t\":[\"::: echarts 一个折线图案例\",\"{ \\\"xAxis\\\": { \\\"type\\\": \\\"category\\\", \\\"data\\\": [\\\"Mon\\\", \\\"Tue\\\", \\\"Wed\\\", \\\"Thu\\\", \\\"Fri\\\", \\\"Sat\\\", \\\"Sun\\\"] }, \\\"yAxis\\\": { \\\"type\\\": \\\"value\\\" }, \\\"series\\\": [ { \\\"data\\\": [150, 230, 224, 218, 135, 147, 260], \\\"type\\\": \\\"line\\\" } ] } \",\":::\",\"查看详情\"]},\"79\":{\"h\":\"流程图\",\"t\":[\"cond=>condition: 是否执行操作? process=>operation: 操作 e=>end: 结束 cond(yes)->process->e cond(no)->e \",\"查看详情\"]},\"80\":{\"h\":\"Mermaid\",\"t\":[\"--- title: Flowchart --- flowchart TB c1-->a2 subgraph one a1-->a2 end subgraph two b1-->b2 end subgraph three c1-->c2 end one --> two three --> two two --> c2 \",\"查看详情\"]},\"81\":{\"h\":\"Tex 语法\",\"t\":[\"查看详情\"]},\"82\":{\"h\":\"Vue 交互演示\",\"t\":[\"::: vue-playground Vue 交互演示\",\"这是脚注内容 ↩︎\"]},\"83\":{\"c\":[\"使用指南\"]},\"84\":{\"c\":[\"Markdown\"]},\"85\":{\"h\":\"页面配置\",\"t\":[\"more 注释之前的内容被视为文章摘要。\"]},\"86\":{\"h\":\"页面信息\",\"t\":[\"你可以在 Markdown 的 Frontmatter 中设置页面信息。\",\"作者设置为 Ms.Hope。\",\"写作日期为 2020 年 1 月 1 日\",\"分类为 “使用指南”\",\"标签为 “页面配置” 和 “使用指南”\"]},\"87\":{\"h\":\"页面内容\",\"t\":[\"你可以自由在这里书写你的 Markdown。\",\"提示\",\"你可以将图片和 Markdown 文件放置在一起，但是你需要使用相对链接./ 进行引用。\",\"对于 .vuepress/public 文件夹的图片，请使用绝对链接 / 进行引用。\",\"主题包含了一个自定义徽章可以使用:\",\"文字结尾应该有深蓝色的 徽章文字 徽章。 \"]},\"88\":{\"h\":\"页面结构\",\"t\":[\"此页面应当包含:\",\"路径导航\",\"标题和页面信息\",\"TOC (文章标题列表)\",\"贡献者、更新时间等页面元信息\",\"评论\",\"导航栏\",\"侧边栏\",\"页脚\",\"返回顶部按钮\",\"你可以通过主题选项和页面 Frontmatter 自定义它们。\"]},\"89\":{\"c\":[\"使用指南\"]},\"90\":{\"c\":[\"页面配置\",\"使用指南\"]},\"91\":{\"h\":\"Markdown 测试\"},\"92\":{\"h\":\"提示容器\"},\"93\":{\"h\":\"信息容器\",\"t\":[\"::: info 自定义标题 信息容器，包含 `代码` 与 [链接](#提示容器)。 ```js const a = 1; ``` ::: \",\"自定义标题\",\"信息容器，包含 代码 与 链接。\",\"const a = 1; \"]},\"94\":{\"h\":\"提示容器\",\"t\":[\"::: tip 自定义标题 提示容器 ::: \",\"自定义标题\",\"提示容器\"]},\"95\":{\"h\":\"警告容器\",\"t\":[\"::: warning 自定义标题 警告容器 ::: \",\"自定义标题\",\"警告容器\"]},\"96\":{\"h\":\"危险容器\",\"t\":[\"::: caution 自定义标题 危险容器 ::: \",\"自定义标题\",\"危险容器\"]},\"97\":{\"h\":\"重要容器\",\"t\":[\"::: important 重要信息 ::: \",\"重要\",\"重要信息\"]},\"98\":{\"h\":\"详情容器\",\"t\":[\"::: details 自定义标题 详情容器 ::: \",\"自定义标题\",\"详情容器\"]},\"99\":{\"h\":\"选项卡\",\"t\":[\"::: code-tabs @tab pnpm ```bash pnpm add -D vuepress-theme-hope ``` @tab yarn ```bash yarn add -D vuepress-theme-hope ``` @tab:active npm ```bash npm i -D vuepress-theme-hope ``` ::: \"]},\"100\":{\"h\":\"对齐\",\"t\":[\"::: center 我是居中的 ::: ::: right 我在右对齐 ::: \",\"我是居中的\",\"我在右对齐\"]},\"101\":{\"h\":\"属性支持\"},\"102\":{\"h\":\"id\",\"t\":[\"一个拥有 ID 的 **单词**{#word}。 \",\"一个拥有 ID 的 单词。\",\"\\\\\",\"\\\\\",\"\\\\\",\"\\\\\",\"<a href=\\\"#word\\\">跳转单词</a> \",\"跳转单词\"]},\"103\":{\"h\":\"脚注\",\"t\":[\"此文字有脚注[1].\",\"\\\\\",\"\\\\\",\"\\\\\"]},\"104\":{\"h\":\"任务列表\",\"t\":[\" 计划1\",\" 计划二\",\"- 查看详情\",\"这是脚注内容 ↩︎\"]},\"105\":{\"c\":[\"使用指南\"]},\"106\":{\"c\":[\"Markdown\"]},\"107\":{\"h\":\"基础概论\"},\"108\":{\"h\":\"深度学习\"},\"109\":{\"h\":\"变分法\",\"t\":[\"dsw2718314\",\"image-20231229114511324\",\"Euler-Lagrange Equation (欧拉-拉格朗日方程)推导 - 知乎 (zhihu.com)\",\"【变分计算1】欧拉-拉格朗日方程 - 知乎 (zhihu.com)\",\"许多数学物理问题都涉及到能够以积分形式表示的量的最小化或者最大化。比如说对于下图，同一平面内的两点 之间的路径 有无数条，那么哪一条路径 能够使得两点之间的距离最小呢?\",\"img\",\"对于任意一条路径，比如图中的蓝色曲线，通过路径 连接的两点之间的距离可以通过积分来表示。将两点之间的 区间分成很多个小的 ，对于一个 区间（很小）上的弧长 可以近似成线段，那么弧长便可写为 \",\"其中 很小时， ，其中 表示 对 的一阶导数。那么将所有 区间内的弧长相加，便得到 两点在路径上 的距离为\",\"因此固定的两点之间的距离实际上就是路径函数 的泛函。函数代表了数到数的映射，而泛函代表了函数到数的映射，即给定一个函数，泛函能够得到一个数（比如此例中，如果取不同的路径 ，那么积分得到的结果 即距离也会不同）。问题就是要找到一个路径函数 使得泛函 取极小值。\"]},\"110\":{\"h\":\"基本概念\"},\"111\":{\"h\":\"函数间的距离\",\"t\":[\"设已知函数 在区间 上有连续的 阶导数, 则所有与 函数 在区间 上的 阶距离小于正数 的函数 所组成的集合称为函数 在区间 上的 阶 邻域或 级 邻域, 记为 , 可表示为\",\"根据上述定义, 函数 的 阶 邻域内的任一函数 应在所讨论的区间内同时满足下列不等式\",\"函数 的零阶 邻域由所有满足 的函数 所组成。而函数 的一阶 邻域则由所有满足 和 的函数 所组成。所以 的一阶 邻域是 的零阶 邻域的一部分。函数 的零阶 邻域称为该函数的强 邻域或强邻域。函数 的一阶 邻域称为该函数的弱 邻域或弱邻域。显然函数的弱邻域是函数的强邻域的一部分。\",\"曲线 的零阶 邻域由所有位于 上下宽为 的带状区域内的曲线组成。以上概念可以推广到多元函数的情形。 若 , 则 与 称为具有 阶的 接近度。\",\"==若两曲线具有 阶的 接近度, 则它们具有任何低于 阶的 接近度。==接近度的阶数越高, 两曲线的接近程度就越近。例如图 2-2-1 是仅具有零阶接近度的两条曲线, 而下图是具有零阶和一阶接近度的两条曲线。有了接近度的概念可以精确地定义泛函的连续。\",\"image-20231228182958923\",\"设函数 是定义域为 的泛函。若对于任意给定的一个正数 ,总可以找到一个 , 只要\",\"都有\",\"成立, 则泛函 称为在函数 处具有 阶 接近度的连续泛函。\"]},\"112\":{\"h\":\"变分\",\"t\":[\"对于任意定值 , 可取函数 与另一可取函数 之差 称为函数 在 处的变分或函数的变分, 记作 称为变分记号、变分符号或变分算子, 这时有\",\"式中 为拉格朗日引进的一个小参数, 但它不是 的函数, 而 为 的任意函数。由于可取函数都通过区间的固定端点, 即它们在区间的端点的值都相等, 故在区间的端点, 任意函数 都满足\",\"也就是\",\"式 就是函数变分在固定端点应该满足的条件即固定边界条件。\",\"因为可取函数 是泛函 的变量, 故也可以这样定义变分: 泛函的变量量 与另一宗量 之差 称为宗量 在 处的变分。\",\"对于任意定值 , 若可取函数 与另一可取函数 具有零阶接近度, 则 称为函数 在 处的强变分。若 与 具有一阶(或一阶以上) 接近度,则 称为函数 在 处的弱变分。强变分与弱变分统称变分。\",\"上述变分的定义也可以推广到多元函数的情形。\",\"显然, 根据变分的定义, 函数 的变分 是 的函数。注意函数变分 与函数增量 的区别, 如图所示, 函数的变分 是两个不同函数 与 在自变量 取固定值时之差 , 函数发生了改变; 函数的增量 是由于自变量 取一个增量而使同一函数 产生的增量, 函数仍是原来的函数。由此也可以看出变分符号 的作用是用以表示相应于自变量 取某一定值时函数的微小改变。\",\"变分符号 不仅可以表示当自变量 取某一定值时函数的微小改变, 同样也可以表示当 取某一定值时函数的导数的改变。如果函数 与另一函数 都可导, 则函数的变分 有以下性质\",\"由此得到变分符号 与导数符号 之间的关系\",\"即函数导数的变分等于函数变分的导数。换句话说, 求变分与求导数这两种运算次序可以交换。在进行变分法的有关推导时要经常用到变分的这个性质。 上面的性质可推广到高阶导数的变分情形, 即\",\"哈密顿算子\",\"拉布拉斯算子\"]},\"113\":{\"h\":\"最简泛函的变分\",\"t\":[\"设 是三个独立变量 在区间 上的已知函数, 且二阶连续可微,其中 和 是 的末知函数, 则泛函\",\"称为最简单的积分型泛函,简称最简泛函, 有时也称为价值泛函。泛函 称为泛函形式或变分积分,被积函数 称为泛函的核、变分被积函数或拉格朗日函数。因对 的积分得到的 值取决于函数 的形式, 故 是 的泛函。 其实不仅仅只是 的函数, 而且还是 和 的函数, 但只要求出了 也就能求出来了,于是只是写成 的形式。\",\"在 的一阶邻域内, 任取一曲线 , 则有\",\"最简泛函 的增量为\",\"有时泛函的增量也称为泛函的全变分。\",\"由二元函数的泰勒中值定理得\",\"式中, 与 分别表示 与 在 处的值, 介于 与 之间, 介于 与 之间。因而.\",\"对于任意 , 当 充分小时, 必有\",\"因此, 有\",\"式中\",\"且 随 趋于零而趋于零。这是由于\",\"面 随 趋于零而趋于零。\",\"这样 与 相差一个比 更高阶的无穷小量, 是泛函增量的主要部分, 记为 。下面将进一步证明 是关于 的线性泛函。下面给出线性泛函的定义。\",\"·\",\"若该泛函具有二阶连续性, 且其增量可表示为 , 其中 为 的高阶无穷小量, 则这个泛函称为在 处可微, 并把 称为泛函 在 上的一阶变分或一次变分, 又称泛函的变分或变分, 记作 或 , 即\",\"定理 2.3.1 若泛函 在 上达到极值, 则在它在 上的变分 等于零。\",\"从Wasserstein距离、对偶理论到WGAN - 科学空间|Scientific Spaces (kexue.fm)\",\"梯度流：探索通往最小值之路 - 科学空间|Scientific Spaces (kexue.fm)s\",\"[论文解读 02]Stein变分梯度下降详细解读 - 知乎 (zhihu.com)\"]},\"114\":{\"h\":\"数学\"},\"115\":{\"h\":\"\"},\"116\":{\"h\":\"机器学习\"},\"117\":{\"h\":\"樱桃\"},\"118\":{\"h\":\"标题 2\",\"t\":[\"这里是内容。\"]},\"119\":{\"h\":\"标题 3\",\"t\":[\"这里是内容。\"]},\"120\":{\"c\":[\"樱桃\"]},\"121\":{\"c\":[\"红\",\"小\",\"圆\"]},\"122\":{\"h\":\"火龙果\"},\"123\":{\"h\":\"标题 2\",\"t\":[\"这里是内容。\"]},\"124\":{\"h\":\"标题 3\",\"t\":[\"这里是内容。\"]},\"125\":{\"c\":[\"火龙果\",\"水果\"]},\"126\":{\"c\":[\"红\",\"大\"]},\"127\":{\"h\":\"重参数技巧\"},\"128\":{\"c\":[\"机器学习\",\"深度学习\"]},\"129\":{\"c\":[\"红\",\"小\"]},\"130\":{\"h\":\"草莓\"},\"131\":{\"h\":\"标题 2\",\"t\":[\"这里是内容。\"]},\"132\":{\"h\":\"标题 3\",\"t\":[\"这里是内容。\"]},\"133\":{\"c\":[\"水果\",\"草莓\"]},\"134\":{\"c\":[\"红\",\"小\"]},\"135\":{\"h\":\"番茄\",\"t\":[\"本文摘要\"]},\"136\":{\"h\":\"标题 2\",\"t\":[\"这里是内容。\"]},\"137\":{\"h\":\"标题 3\",\"t\":[\"这里是内容。\"]},\"138\":{\"c\":[\"蔬菜\"]},\"139\":{\"c\":[\"红\",\"圆\"]},\"140\":{\"h\":\"数组\"},\"141\":{\"h\":\"分发饼干\",\"t\":[\"455. 分发饼干 - 力扣（LeetCode）\"]},\"142\":{\"h\":\"排序+贪心法+双指针\",\"t\":[\"代码随想录 (programmercarl.com)\",\"为了满足更多的小孩，就不要造成饼干尺寸的浪费。\",\"大尺寸的饼干既可以满足胃口大的孩子也可以满足胃口小的孩子，那么就应该优先满足胃口大的。\",\"这里的局部最优就是大饼干喂给胃口大的，充分利用饼干尺寸喂饱一个，全局最优就是喂饱尽可能多的小孩。\",\"可以尝试使用贪心策略，先将饼干数组和小孩数组排序。\",\"然后从后向前遍历小孩数组，用大饼干优先满足胃口大的，并统计满足小孩数量。\",\"如图：\",\"img\",\"这个例子可以看出饼干9只有喂给胃口为7的小孩，这样才是整体最优解，并想不出反例，那么就可以撸代码了。\"]},\"143\":{\"h\":\"代码\",\"t\":[\"// 版本一 // 时间复杂度：O(nlogn) // 空间复杂度：O(1) class Solution { public: int findContentChildren(vector<int>& g, vector<int>& s) { sort(g.begin(), g.end()); sort(s.begin(), s.end()); int index = s.size() - 1; // 饼干数组的下标 int result = 0; for (int i = g.size() - 1; i >= 0; i--) { // 遍历胃口 if (index >= 0 && s[index] >= g[i]) { // 遍历饼干 result++; index--; } } return result; } }; \",\"从代码中可以看出我用了一个index来控制饼干数组的遍历，遍历饼干并没有再起一个for循环，而是采用自减的方式，这也是常用的技巧。\",\"有的同学看到要遍历两个数组，就想到用两个for循环，那样逻辑其实就复杂了\"]},\"144\":{\"h\":\"注意事项\",\"t\":[\"注意版本一的代码中，可以看出来，是先遍历的胃口，在遍历的饼干，那么可不可以 先遍历 饼干，在遍历胃口呢？\",\"其实是不可以的。\",\"外面的for 是里的下标i 是固定移动的，而if里面的下标 index 是符合条件才移动的。\",\"如果 for 控制的是饼干， if 控制胃口，就是出现如下情况 ：\",\"img\",\"if 里的 index 指向 胃口 10， for里的i指向饼干9，因为 饼干9 满足不了 胃口10，所以 i 持续向前移动，而index 走不到s[index] >= g[i] 的逻辑，所以index不会移动，那么当i 持续向前移动，最后所有的饼干都匹配不上。\",\"所以 一定要for 控制 胃口，里面的if控制饼干。\"]},\"145\":{\"h\":\"其他\",\"t\":[\"也可以换一个思路，小饼干先喂饱小胃口\",\"class Solution { public: int findContentChildren(vector<int>& g, vector<int>& s) { sort(g.begin(),g.end()); sort(s.begin(),s.end()); int index = 0; for(int i = 0; i < s.size(); i++) { // 饼干 if(index < g.size() && g[index] <= s[i]){ // 胃口 index++; } } return index; } }; \"]},\"146\":{\"h\":\"有效的山脉数组\"},\"147\":{\"h\":\"寻找数组的中心下标\",\"t\":[\"[数组]\",\"力扣题目链接(opens new window)a\"]},\"148\":{\"h\":\"按奇偶排序数组II\",\"t\":[\"922. 按奇偶排序数组 II - 力扣（LeetCode）\",\"给定一个非负整数数组 A， A 中一半整数是奇数，一半整数是偶数。\",\"对数组进行排序，以便当 A[i] 为奇数时，i 也是奇数；当 A[i] 为偶数时， i 也是偶数。\",\"你可以返回任何满足上述条件的数组作为答案。\",\"示例：\",\"输入：[4,2,5,7]\",\"输出：[4,5,2,7]\",\"解释：[4,7,2,5]，[2,5,4,7]，[2,7,4,5] 也会被接受。\",\"优化版本\",\"class Solution { public: vector<int> sortArrayByParityII(vector<int>& A) { vector<int> result(A.size()); int evenIndex = 0; // 偶数下标 int oddIndex = 1; // 奇数下标 for (int i = 0; i < A.size(); i++) { if (A[i] % 2 == 0) { result[evenIndex] = A[i]; evenIndex += 2; } else { result[oddIndex] = A[i]; oddIndex += 2; } } return result; } }; \",\"空间复杂度o(1)\",\"class Solution { public: bool validMountainArray(vector<int>& arr) { int index=0; for(int i=1;i<arr.size();i++){ if(arr[i]==arr[i-1])return false; if(arr[i]>arr[i-1]){ index++; } else{ break; } } cout<<index<<endl; //index==0表明arr[1]<arrp[0]导致推出循环 //index=arr.size()-1;一直到最后也没有找到index if(index==0||index==arr.size()-1){ return false; } for(int i=index+1;i<arr.size();i++){ if(arr[i]>=arr[i-1])return false; } return true; } }; \"]},\"149\":{\"h\":\"旋转数组\",\"t\":[\"189. 轮转数组 - 力扣（LeetCode）\"]},\"150\":{\"h\":\"使用额外的空间\",\"t\":[\"class Solution { public: void rotate(vector<int>& nums, int k) { vector<int> temp=nums; int n=k%nums.size(); for(int i=0;i<nums.size();i++){ int index=(i+n)%(nums.size()); nums[index]=temp[i]; } return ; } }; \"]},\"151\":{\"h\":\"数组反转\",\"t\":[\"class Solution { public: void rotate(vector<int>& nums, int k) { if(nums.size()<=1)return; k=k%nums.size(); reverse(nums.begin(),nums.end()); reverse(nums.begin(),nums.begin()+k); reverse(nums.begin()+k,nums.end()); } }; \"]},\"152\":{\"h\":\"区间问题\"},\"153\":{\"h\":\"插入区间\",\"t\":[\"给你一个 无重叠的 *，*按照区间起始端点排序的区间列表。\",\"在列表中插入一个新的区间，你需要确保列表中的区间仍然有序且不重叠（如果有必要的话，可以合并区间）\",\"示例 1：\",\"输入：intervals = [[1,3],[6,9]], newInterval = [2,5] 输出：[[1,5],[6,9]] \",\"示例 2：\",\"输入：intervals = [[1,2],[3,5],[6,7],[8,10],[12,16]], newInterval = [4,8] 输出：[[1,2],[3,10],[12,16]] 解释：这是因为新的区间 [4,8] 与 [3,5],[6,7],[8,10] 重叠。 \",\"示例 3：\",\"输入：intervals = [], newInterval = [5,7] 输出：[[5,7]] \",\"示例 4：\",\"输入：intervals = [[1,5]], newInterval = [2,3] 输出：[[1,5]] \",\"示例 5：\",\"输入：intervals = [[1,5]], newInterval = [2,7] 输出：[[1,7]] \"]},\"154\":{\"h\":\"模拟法\",\"t\":[\"class Solution { public: vector<vector<int>> insert(vector<vector<int>>& intervals, vector<int>& newInterval) { int left=newInterval[0]; int right=newInterval[1]; bool placed=false; vector<vector<int>> res; for(const auto interval:intervals){ if(placed){ res.push_back(interval); continue; } if(interval[1]<left){//interval在左边，不用管，直接加入 res.push_back(interval); } else if(interval[0]>right){ res.push_back({left,right}); placed=true; res.push_back(interval); } else{//有重叠，开始合并 left=min(left,interval[0]); right=max(right,interval[1]); } } if(!placed){ res.push_back({left,right}); } return res; } }; \"]},\"155\":{\"h\":\"环形数组\",\"t\":[\"环形子数组的最大和\",\"189. 轮转数组 - 力扣（LeetCode）\"]},\"156\":{\"h\":\"矩阵\"},\"157\":{\"h\":\"螺旋矩阵\"},\"158\":{\"h\":\"螺旋矩阵 II\"},\"159\":{\"h\":\"旋转图像\"},\"160\":{\"h\":\"矩阵置零\"},\"161\":{\"h\":\"扑克牌中的顺子\",\"t\":[\"剑指 Offer 61. 扑克牌中的顺子 - 力扣（LeetCode）、\"]},\"162\":{\"h\":\"数据结构与算法\"},\"163\":{\"h\":\"目录\",\"t\":[\"数组\",\"字符串\"]},\"164\":{\"h\":\"字符串\"},\"165\":{\"h\":\"生成模型的功能\",\"t\":[\"生成模型一般具有两个基本功能：密度估计和生成样本\"]},\"166\":{\"h\":\"密度估计\",\"t\":[\"给定一组数据 , 假设它们都是独立地从相同的概率密度函数为 的未知分布中产生的. 密度估计 (Density Estimation) 是根据数据集 来估计其概率密度函数 .\",\"在机器学习中, 密度估计是一类无监督学习问题. 比如在手写体数字图像的密度估计问题中, 我们将图像表示为一个随机向量 , 其中每一维都表示一个像素值. 假设手写体数字图像都服从一个未知的分布 , 希望通过一些观测样本来估计其分布. 但是, 手写体数字图像中不同像素之间存在复杂的依赖关系 (比如相邻像素的颜色一般是相似的 ), 很难用一个明确的图模型来描述其依赖关系, 所以直接建模 比较困难.\",\"因此, 我们通常通过引入隐变量 来简化模型, 这样密度估计问题可以转换为估计变\",\"量 的两个局部条件概率 和 . 一般为了简化模型, 假设隐变量 的先验分布为标准高斯分布 .隐变量 的每一维之间都是独立的. 在这个假设下, 先验分布 中没有参数.因此,密度估计的重点是估计条件分布 .\",\"如果要建模含隐变量的分布, 就需要利用 EM 算法来进行密度估计. 而在 EM 算法中, 需要估计条件分布 以及近似后验分布 .当这两个分布比较复杂时, 我们可以利用神经网络来进行建模, 这就是变分自编码器的思想.\",\"image-20231216231326318\"]},\"167\":{\"h\":\"深度生成模型\",\"t\":[\"概率生成模型 (Probabilistic Generative Model), 简称生成模型, 是概率统计和机器学习领域的一类重要模型, 指一系列用于随机生成可观测数据的模型.\",\"假设在一个连续或离散的高维空间 中, 存在一个随机向量 服从一个未知的数据分布 . 生成模型是根据一些可观测的样本 来学习一个参数化的模型 来近似未知分布 , 并可以用这个模型来生成一些样本, 使得 “生成” 的样本和 “真实” 的样本尽可能地相似.\",\"生成模型通常包含两个基本功能: 概率密度估计和生成样本 (即采样). 下图以手写体数字图像为例给出了生成模型的两个功能示例, 其中左图表示手写体数字图像的真实分布 以及从中采样的一些 “真实” 样本, 右图表示估计出了分布 以及从中采样的 “生成”样本.\",\"生成模型的两个功能\",\"生成模型的应用十分广泛，可以用来建模不同的数据，比如图像、文本、声音等．但对于一个高维空间中的复杂分布，密度估计和生成样本通常都不容易实现，原因在于：\",\"高维随机向量一般比较难以直接建模，需要通过一些条件独立性来简化模型；\",\"给定一个已建模的复杂分布，也缺乏有效的采样方法.\",\"深度生成模型就是利用深度神经网络可以近似任意函数的能力来建模一个复杂分布  或直接生成符合分布 的样本．本章先介绍概率生成模型的基本概念，然后介绍两种深度生成模型：变分自编码器和生成对抗网络．\"]},\"168\":{\"h\":\"数学基础\"},\"169\":{\"h\":\"目录\",\"t\":[\"线性代数.md\",\"微积分.md\",\"概率论.md\",\"优化.md\"]},\"170\":{\"h\":\"线性代数\",\"t\":[\"线性代数主要包含向量、向量空间（或称线性空间）以及向量的线性变换和有限维的线性方程组.\"]},\"171\":{\"h\":\"向量和向量空间\"},\"172\":{\"h\":\"向量\",\"t\":[\"标量 (Scalar) 是一个实数, 只有大小, 没有方向. 标量一般用斜体小写英文字母 来表示. 向量 (Vector) 是由一组实数组成的有序数组, 同时具有大小和方向. 一个 维向量 是由 个有序实数组成, 表示为\",\"其中 称为向量 的第 个分量, 或第 维. 向量符号一般用黑斜体小写英文字母 , 或小写希腊字母 等来表示.\"]},\"173\":{\"h\":\"向量空间\"},\"174\":{\"h\":\"介绍\"},\"175\":{\"h\":\"容器\"},\"176\":{\"h\":\"信息容器\",\"t\":[\"::: info 自定义标题 信息容器，包含 `代码` 与 [链接](#提示容器)。 ```js const a = 1; ``` ::: \",\"自定义标题\",\"信息容器，包含 代码 与 链接。\",\"const a = 1; \"]},\"177\":{\"h\":\"提示容器\",\"t\":[\"::: tip 自定义标题 提示容器 ::: \",\"自定义标题\",\"提示容器\"]},\"178\":{\"h\":\"警告容器\",\"t\":[\"::: warning 自定义标题 警告容器 ::: \",\"自定义标题\",\"警告容器\"]},\"179\":{\"h\":\"危险容器\",\"t\":[\"::: caution 自定义标题 危险容器 ::: \",\"自定义标题\",\"危险容器\"]},\"180\":{\"h\":\"详情容器\",\"t\":[\"::: details 自定义标题 详情容器 ::: \",\"自定义标题\",\"详情容器\"]},\"181\":{\"h\":\"选项卡\",\"t\":[\"::: code-tabs @tab pnpm ```bash pnpm add -D vuepress-theme-hope ``` @tab yarn ```bash yarn add -D vuepress-theme-hope ``` @tab:active npm ```bash npm i -D vuepress-theme-hope ``` ::: \"]},\"182\":{\"h\":\"对齐\",\"t\":[\"::: center 我是居中的 ::: ::: right 我在右对齐 ::: \",\"我是居中的\",\"我在右对齐\"]},\"183\":{\"h\":\"属性支持\"},\"184\":{\"h\":\"id\",\"t\":[\"一个拥有 ID 的 单词。\",\"\\\\\",\"\\\\\",\"\\\\\",\"\\\\\",\"跳转单词\"]},\"185\":{\"h\":\"脚注\",\"t\":[\"此文字有脚注[1].\",\"\\\\\",\"\\\\\",\"\\\\\"]},\"186\":{\"h\":\"任务列表\",\"t\":[\" 计划1\",\" 计划二\",\"- 查看详情\",\"这是脚注内容 ↩︎\"]},\"187\":{\"h\":\"神经微分方程\",\"t\":[\"绪论.md\",\"神经常微分方程.md\",\"连续归一化流.md\",\"不规则序列建模.md\"]},\"188\":{\"h\":\"绪论\",\"t\":[\"介绍\"]},\"189\":{\"h\":\"参数学习\",\"t\":[\"图模型的学习可以分为两部分：一是网络结构学习，即寻找最优的网络结构；二是网络参数估计，即已知网络结构，估计每个条件概率分布的参数．\",\"网络结构学习比较困难，一般是由领域专家来构建．本节只讨论在给定网络结构条件下的参数估计问题．图模型的参数估计问题又分为不包含隐变量时的参数估计问题和包含隐变量时的参数估计问题．\"]},\"190\":{\"h\":\"不含隐变量的参数估计\",\"t\":[\"如果图模型中不包含隐变量，即所有变量都是可观测的，那么网络参数一般可以直接通过最大似然来进行估计．\"]},\"191\":{\"h\":\"有向图模型\",\"t\":[\"在有向图模型中, 所有变量 的联合概率分布可以分解为每个随机变量 的局部条件概率 的连乘形式, 其中 为第 个变量的局部条件概率的参数. 给定 个训练样本 , 其对数似然函数为\",\"其中 为模型中的所有参数. 因为所有变量都是可观测的, 最大化对数似然 , 只需要分别最大化每个变量的条件似然来估计其参数.\",\"如果变量 𝒙 是离散的，简单直接的方式是在训练集上统计每个变量的条件概率表．但是条件概率表需要的参数比较多．假设条件概率 的父节点数量为𝑀，所有变量为二值变量，其条件概率表需要 个参数．为了减少参数数量，可以使用参数化的模型，比如Sigmoid信念网络．如果变量 是连续的，可以使用高斯函数来表示条件概率分布，称为高斯信念网络．在此基础上，还可以通过让所有的条件概率分布共享使用同一组参数来进一步减少参数的数量．\"]},\"192\":{\"h\":\"无向图模型\",\"t\":[\"在无向图模型中, 所有变量 的联合概率分布可以分解为定义在最大团上的势能函数的连乘形式. 以对数线性模型为例,\",\"其中 . 给定 个训练样本 , 其对数似然函数为\",\"其中 为定义在团 上的势能函数的参数. 采用梯度上升方法进行最大似然估计, 关于参数 的偏导数为\",\"其中\",\"因此,\",\"其中 定义为经验分布 (Empirical Distribution). 由于在最优点时梯度为 0 , 因此无向图的最大似然估计的优化目标等价于: 对于每个团 上的特征 ,使得其在经验分布 下的期望等于其在模型分布 下的期望.\",\"对比公式 和公式 可以看出, 无向图模型的参数估计要比有向图更为复杂. 在有向图中, 每个局部条件概率的参数是独立的; 而在无向图中, 所有的参数都是相关的, 无法分解.\",\"对于一般的无向图模型, 公式 中的 往往很难计算,因为涉及在联合概率空间 计算期望. 当模型变量比较多时, 这个计算往往无法实现. 因此, 无向图的参数估计通常采用近似的方法:\",\"利用采样来近似计算这个期望;\",\"采用坐标上升法, 即固定其他参数, 来优化一个势能函数的参数.\"]},\"193\":{\"h\":\"含隐变量的参数估计\",\"t\":[\"如果图模型中包含隐变量，即有部分变量是不可观测的，就需要用EM 算法进行参数估计．\"]},\"194\":{\"h\":\"EM算法\",\"t\":[\"【机器学习】EM——期望最大（非常详细） - 知乎 (zhihu.com)\",\"在一个包含隐变量的图模型中, 令 定义可观测变量集合, 定义隐变量集合,一个样本 的边际似然函数 (Marginal Likelihood) 为\",\"其中 为模型参数. 边际似然也称为证据 (Evidence). 下图给出了带隐变量的贝叶斯网络的图模型结构, 其中矩形表示其中的变量重复 次. 这种表示方法称为盘子表示法 (Plate Notation), 是图模型中表示重复变量的方法.\",\"带隐变量的贝叶斯网络\",\"给定 个训练样本 , 整个训练集的对数边际似然为\",\"通过最大化整个训练集的对数边际似然 , 可以估计出最优的参数 . 然而计算边际似然函数时涉及 的推断问题, 需要在对数函数的内部进行求和 (或积分). 这样, 当计算参数 的梯度时, 这个求和操作依然存在. 除非 的形式非常简单, 否则这个求和难以直接计算.\",\"为了计算 , 我们引入一个额外的变分函数 为定义在隐变量 上的分布. 样本 的对数边际似然函数为\",\"其中 为对数边际似然函数 的下界, 称为证据下界 (Evidence Lower BOund, ELBO ).\",\"该公式使用了 Jensen 不等式, 即对于凹函数 成立. 由 Jensen 不等式的性质可知, 仅当 时, 对数边际似然函数 和其下界 相等, 即 . 这样, 最大化对数边际似然函数 的过程可以分解为两个步骤:\",\"先找到近似分布 使得 .\",\"再寻找参数 最大化 . 这就是期望最大化 (ExpectationMaximum, EM ) 算法.\",\"EM 算法是含隐变量图模型的常用参数估计方法, 通过迭代的方法来最大化边际似然. EM 算法具体分为两个步骤: 步和 步. 这两步不断重复, 直到收玫到某个局部最优解. 在第 步更新时, 步和 步分别为:\",\"E步 (Expectation Step): 固定参数 , 找到一个分布 使得证据下界 等于 .根据 Jensen 不等式的性质, 时, 最大. 因此在 E步中, 最理想的分布 是等于后验分布 . 而计算后验分布 是一个推断 (Inference) 问题. 如果 是有限的一维离散变量 (比如混合高斯模型 ), 计算起来还比较容易; 否则, 一般情况下很难计算,需要通过变分推断的方法来进行近似估计.\",\"M 步 (Maximization Step): 固定 , 找到一组参数使得证据下界最大,即\",\"这一步可以看作全观测变量图模型的参数估计问题，可以使用第11.2.1节中方法进行参数估计．\"]},\"195\":{\"h\":\"高斯混合模型\",\"t\":[\"本节介绍一个EM算法的应用例子：高斯混合模型．\",\"高斯混合模型（Gaussian Mixture Model，GMM）是由多个高斯分布组成的模型，其总体密度函数为多个高斯密度函数的加权组合．如果一个连续随机变量或连续随机向量的分布比较复杂，那么我们通常可以用高斯混合模型来估计其分布情况．\",\"不失一般性, 这里考虑一维的情况. 假设样本 是从 个高斯分布中的一个分布生成的, 但是无法观测到具体由哪个分布生成. 我们引入一个隐变量 来表示样本 来自于哪个高斯分布, 服从多项分布:\",\"其中 为多项分布的参数, 并满足 . 表示样本 由第 个高斯分布生成的概率. 给定 ,条件分布 为高斯分布:\",\"其中 和 分别为第 个高斯分布的均值和方差.\",\"从高斯混合模型中生成一个样本 的过程可以分为两步:\",\"首先根据多项分布 随机选取一个高斯分布.\",\"假设选中第 个高斯分布 ( 即 ), 再从高斯分布 中选取一个样本 .\",\"下图给出了高斯混合模型的图模型表示：\",\"高斯混合模型\",\"参数估计\",\"给定 个由高斯混合模型生成的训练样本 , 希望能学习其中的参数 . 由于我们无法观测样本 是从哪个高斯分布生成的,因此无法直接用最大似然来进行参数估计. 对每个样本 , 其对数边际分布为\",\"根据 算法, 参数估计可以分为两步进行迭代:\",\"E步 先固定参数 , 计算后验分布 , 即\",\"其中 定义了样本 属于第 个高斯分布的后验概率.\",\" 步 令 , 训练集 的证据下界为\",\"其中 为和参数无关的常数．\",\"将参数估计问题转为优化问题:\",\"利用拉格朗日乘数法来求解上面的等式约束优化问题, 分别求拉格朗日函数 关于 的偏导数, 并令其等于 0 .可得\",\"其中\",\"高斯混合模型的参数学习过程：\",\"input: 训练样本: ;\",\"随机初始化参数: ;\",\"repeat // E步 固定参数, 根据公式 (11.58) 计算 ; // M 步 固定 , 根据公式 (11.63)、公式(11.64) 和公式 (11.65), 计算 , ;\",\"​ until 对数边际分布 收敛; 输出: \",\"下图给出一个高斯混合模型训练过程的简单示例．给定一组数据，我们用两个高斯分布来估计这组数据的分布情况\",\"image-20231224235548436\"]},\"196\":{\"h\":\"概率图\",\"t\":[\"概率图模型（Probabilistic Graphical Model，PGM），简称图模型（Graphical Model，GM），是指一种用图结构来描述多元随机变量之间条件独立关系的概率模型，从而给研究高维空间中的概率模型带来了很大的便捷性．\",\"对于一个 维随机向量 , 其联合概率为高维空间中的分布, 一般难以直接建模. 假设每个变量为离散变量并有 个取值, 在不作任何独立假设条件下, 则需要 个参数才能表示其概率分布. 当 时, 参数量约为 , 远远超出了目前计算机的存储能力.\",\"一种有效减少参数量的方法是独立性假设. 一个 维随机向量 的联合概率分解为 个条件概率的乘积,\",\"其中 表示变量 的取值. 如果某些变量之间存在条件独立, 其参数量就可以大幅减少.\",\"假设有四个二值变量 , 在不知道这几个变量依赖关系的情况下, 可以用一个联合概率表来记录每一种取值的概率 , 共需要 个参数. 假设在已知 时, 和 独立, 即有\",\"在已知 和 时, 也和 独立, 即有\",\"那么其联合概率 可以分解为\",\"即 4 个局部条件概率的乘积. 如果分别用 4 个表格来记录这 4 个条件概率的话, 只需要 个独立参数，计算过程如下：\",\"计算过程：\",\"我们将分别计算每个条件概率所需的参数数量，并将它们相加以得到总数。首先，让我们确定每个条件概率所需的参数。\",\": 这是一个二值变量，所以它可以取两个值（比如0或1）。但是，由于概率总和必须为1，我们只需要知道其中一个值（例如），就可以推断出另一个值（）。因此，只需要1个参数。\",\": 这是一个条件概率，也是二值变量，而且它的概率依赖于的值。对于的每个值，都有一个概率分布，所以我们需要2个值（和）来描述这个条件概率。但是，对于每个的值，只需要知道的一个值的条件概率，因为另一个可以通过1减去已知的条件概率来得到。所以，需要2个参数。\",\": 这个条件概率与类似，因为也是二值变量，并且它的概率只依赖于的值。因此，也需要2个参数。\",\": 这是一个条件概率，的概率依赖于和的组合。由于和都是二值变量，所以有4种组合（00、01、10、11）。对于每种组合，我们需要知道的一个值的概率，因为另一个值的概率可以通过1减去已知的概率来得到。因此，需要4个参数。\",\"现在，我们将这些参数加起来得到总数：,所以，总共需要9个参数来描述这个系统的联合概率分布。\",\"当概率模型中的变量数量比较多时, 其条件依赖关系也比较复杂. 我们可以使用图结构的方式将概率模型可视化, 以一种直观、简单的方式描述随机变量之间的条件独立性, 并可以将一个复杂的联合概率模型分解为一些简单条件概率模型的组合. 图11.1给出了上述例子中 4 个变量之间的条件独立性的图形化描述. 图中每个节点表示一个变量, 每条连边表示变量之间的依赖关系.\",\"变量 之间条件独立性的图形化表示\"]},\"197\":{\"h\":\"图模型的基本问题\",\"t\":[\"图模型有三个基本问题：\",\"表示问题：对于一个概率模型，如何通过图结构来描述变量之间的依\",\"赖关系．\",\"学习问题：图模型的学习包括图结构的学习和参数的学习．在本章中，\",\"我们只关注在给定图结构时的参数学习，即参数估计问题．\",\"推断问题：在已知部分变量时，计算其他变量的条件概率分布．\"]},\"198\":{\"h\":\"图模型与机器学习\",\"t\":[\"很多机器学习模型都可以归结为概率模型，即建模输入和输出之间的条件概率分布．因此，图模型提供了一种新的角度来解释机器学习模型，并且这种角度有很多优点，比如了解不同机器学习模型之间的联系，方便设计新模型等．在机器学习中，图模型越来越多地用来设计和分析各种学习算法．\"]},\"199\":{\"h\":\"本章目录\",\"t\":[\"模型表示\"]},\"200\":{\"h\":\"资料\",\"t\":[\"机器学习-白板推导系列(九)-概率图模型基础_哔哩哔哩_bilibili\",\"机器学习-白板推导系列(九)-概率图模型基础笔记 - 知乎 (zhihu.com)\"]},\"201\":{\"h\":\"模型表示\",\"t\":[\" headerDepth: 2 sidebar: heading \",\"图由一组节点和节点之间的边组成．在概率图模型中，每个节点都表示一个随机变量（或一组随机变量），边表示这些随机变量之间的概率依赖关系．\",\"常见的概率图模型可以分为两类: 有向图模型和无向图模型.\",\"有向图模型使用有向非循环图 (Directed Acyclic Graph, DAG) 来描述变量之间的关系. 如果两个节点之间有连边, 表示对应的两个变量为因果关系,即不存在其他变量使得这两个节点对应的变 量条件独立.\",\"无向图模型使用无向图 (Undirected Graph) 来描述变量之间的关系.每条边代表两个变量之间有概率依赖关系, 但是并不一定是因果关系.\",\"图1给出了两个代表性图模型 (有向图和无向图) 的示例, 分别表示了四个变量 之间的依赖关系. 图中带阴影的节点表示可观测到的变量, 不带阴影的节点表示隐变量,连边表示两变量间的条件依赖关系.\",\"有向图和无向图示例\"]},\"202\":{\"h\":\"有向图模型\",\"t\":[\"有向图模型（Directed Graphical Model），也称为贝叶斯网络（BayesianNetwork）或信念网络（Belief Network，BN），是一类用有向图来描述随机向量概率分布的模型．\",\"下图中 表示 的父类：\",\"img\",\"定义（贝叶斯网络）\",\"对于一个 维随机向量 和一个有 个节点的有向非循环图 中的每个节点都对应一个随机变量, 每个连接 表示两个随机变量 和 之间具有非独立的因果关系. 令 表示变量 的所有父节点变量集合, 表示每个随机变量的局部条件概率分布 ( Local Conditional Probability Distribution). 如果 的联合概率分布可以分解为每个随机变量 的局部条件概率的连乘形式, 即\",\"那么 构成了一个贝叶斯网络.\",\"因此可以根据有向图写出因子分解的结果。以下，有三种情况：\",\"共因关系（tail2tail）\",\"img\",\"可以写出其因子分解的结果：\",\"同时我们可以根据链式法则写出联合概率:\",\"因此:\",\"所以可知: 与 独立 (在 已知的条件下)\",\"若 被观测，则路径被堵塞，即 与 独立 \",\"image-20231216143312896\",\"注意\",\"此规则并不是强加上去的，是我们用概率图的因子分解表示联合概率后，本身存在的现象，我们在证明此现象存在后，总结出的规律，根据此规律可以直接看图得出结论。\",\"间接因果关系（head2tail）\",\"img\",\"如图可以写出其因子分解的结果：\",\"同时我们可以根据链式法则写出联合概率:\",\"因此:\",\"所以可知： 与 独立 (在 已知的条件下) 从 的角度来看，是 的头和 的尾巴相连，所以称为head-to-tail\",\"image-20231216143808424\",\"共果关系（head2head)\",\"img\",\"如图可以写出其因子分解的结果:\",\"同时我们可以根据链式法则写出联合概率:\",\"因此:\",\"所以可知: 与 独立 (在 不知的情况下)\",\"由此，我们可以总结一个规律：\",\"从 的角度来看，是 的头和 的头相连，所以称为head-to-head 默认情况下， ，路径是阻塞的\",\"特殊情况：若 被观测，则路径是通的， 和 不独立\",\"这种情况中，在给定 时， 和 反而不独立了，与前两种情况刚好相反，我们举个例子说明一下:\",\"局部马尔可夫性质\",\"局部马尔可夫性质 对一个更一般的贝叶斯网络, 其局部马尔可夫性质为: 每个随机变量在给定父节点的情况下, 条件独立于它的非后代节点.\",\"其中 为 的非后代变量.\"]},\"203\":{\"h\":\"常见的有向图模型\",\"t\":[\"很多经典的机器学习模型可以使用有向图模型来描述，比如朴素贝叶斯分类器、隐马尔可夫模型、深度信念网络等．\"]},\"204\":{\"h\":\"Sigmoid信念网络\",\"t\":[\"为了减少模型参数，可以使用参数化模型来建模有向图模型中的条件概率分布．一种简单的参数化模型为Sigmoid信念网络[1]\",\"Sigmoid 信念网络 (Sigmoid Belief Network, SBN) 中的变量取值为 .对于变量 和它的父节点集合 , 其条件概率分布表示为\",\"其中 是 Logistic 函数, 是可学习的参数. 假设变量 的父节点数量为 ,如果使用表格来记录条件概率需要 个参数, 如果使用参数化模型只需要 1 个参数. 如果对不同的变量的条件概率都共享使用一个参数化模型, 其参数数量又可以大幅减少.\",\"值得一提的是, Sigmoid 信念网络与 Logistic 回归模型都采用 Logistic 函数来计算条件概率. 如果假设 Sigmoid 信念网络中只有一个叶子节点, 其所有的父节点之间没有连接, 且取值为实数, 那么 Sigmoid 信念网络的网络结构和 Logistic 回归模型类似, 如图11.4所示. 但是, 这两个模型的区别在于, Logistic 回归模型中的 作为一种确定性的参数, 而非变量. 因此, Logistic 回归模型只建模条件概率 , 是一种判别模型; 而 Sigmoid 信念网络建模联合概率 , 是一种生成模型.\",\"Sigmoid信念网络和Logistic回归模型的比较\"]},\"205\":{\"h\":\"朴素贝叶斯分类器\",\"t\":[\"朴素贝叶斯 ( Naive Bayes, NB) 分类器是一类简单的概率分类器, 在强 (朴素 ) 独立性假设的条件下运用贝叶斯公式来计算每个类别的条件概率. 给定一个有 维特征的样本 和类别 , 类别 的条件概率为\",\"其中 为概率分布的参数. 在朴素贝叶斯分类器中, 假设在给定 的情况下, 之间是条件独立的, 即 . 下图给出了朴素贝叶斯分类器的图模型表示.\",\"image-20231216150648772\",\"条件概率分布 可以分解为\",\"其中 是 的先验概率分布的参数, 是条件概率分布 的参数. 若 为连续值, 可以用高斯分布建模; 若 为离散值, 可以用多项分布建模.\",\"虽然朴素贝叶斯分类器的条件独立性假设太强, 但是在实际应用中, 朴素贝叶斯分类器在很多任务上也能得到很好的结果, 并且模型简单, 可以有效防止过拟合.\"]},\"206\":{\"h\":\"隐马尔可夫模型\",\"t\":[\"隐马尔可夫模型 ( Hidden Markov Model, HMM ) [Baum et al., 1966] 是用来表示一种含有隐变量的马尔可夫过程.\",\"图11.6给出隐马尔可夫模型的图模型表示, 其中 为可观测变量, 为隐变量. 所有的隐变量构成一个马尔可夫链, 每个可观测标量 依赖当前时刻的隐变量 .\",\"隐马尔可夫模型的联合概率可以分解为\",\"为了描述方便, 这里用 表示 .\",\"其中 和 分别为可观测变量和隐变量的取值, 条件概率 称为输出概率, 条件概率 称为转移概率, 和 分别表示两类条件概率的参数.\"]},\"207\":{\"h\":\"无向图模型\",\"t\":[\"无向图模型，也称为马尔可夫随机场（Markov Random Field，MRF）或马尔可夫网络（Markov Network），是一类用无向图来描述一组具有局部马尔可夫性质的随机向量 的联合概率分布的模型．\",\"定义（马尔可夫随机场）\",\"对于一个随机向量 和一个有 个节点的无向图 ( 可以存在循环), 图 中的节点 表示随机变量 . 如果 满足局部马尔可夫性质, 即一个变量 在给定它的邻居的情况下独立于所有其他变量,\",\"其中 为变量 的邻居集合, 为除 外其他变量的集合, 那么 就构成了一个马尔可夫随机场.\",\"向图中的局部马尔可夫性质可以表示为\",\"其中 表示除 和 外的其他变量. 对于图中的 个变量, 根据马尔可夫性质, 可以得到 和 \",\"image-20231216152932251\"]},\"208\":{\"h\":\"无向图模型的概率分解\",\"t\":[\"团\",\"由于无向图模型并不提供一个变量的拓扑顺序, 因此无法用链式法则对 进行逐一分解. 无向图模型的联合概率一般以全连通子图为单位进行分解.无向图中的一个全连通子图, 称为团 (Clique), 即团内的所有节点之间都连边.在下图所示的无向图中共有 7 个团, 包括 , .\",\"在所有团中, 如果一个团不能被其他的团包含, 这 个团就是一个最大团 ( Maximal Clique).\",\"image-20231216153149245\",\"无向图中的联合概率可以分解为一系列定义在最大团上的非负函数的乘积形式．\",\"Hammersley-Clifford定理\",\"如果一个分布 满足无向图 中的局部马尔可夫性质, 当且仅当 可以表示为一系列定义在最大团上的非负函数的乘积形式, 即\",\"其中 为 中的最大团集合, 是定义在团 上的势能函数 (Potential Function ), 是配分函数 (Partition Function), 用来将乘积归一化为概率形式:\",\"其中 为随机向量 的取值空间.\",\"Hammersley-Clifford 定理的证明可以参考 [2]．无向图模型与有向图模型的一个重要区别是有配分函数𝑍．配分函数的计算复杂度是指数的，因此在推断和参数学习时都需要重点考虑．\",\"吉布斯分布\",\"公式 中定义的分布形式也称为吉布斯分布（Gibbs Distribution）．根据 Hammersley-Clifford 定理，无向图模型和吉布斯分布是一致的．吉布斯分布一定满足马尔可夫随机场的条件独立性质，并且马尔可夫随机场的概率分布一定可以表示成吉布斯分布．\",\"由于势能函数必须为正,因此我们一般定义为\",\"其中 为能量函数 (Energy Function).\",\"因此,无向图上定义的概率分布可以表示为\",\"这种形式的分布又称为玻尔兹曼分布 (Boltzmann Distribution). 任何一个无向图模型都可以用公式 来表示其联合概率.\"]},\"209\":{\"h\":\"常见的无向图模型\",\"t\":[\"很多经典的机器学习模型可以使用无向图模型来描述，比如对数线性模型（也叫最大熵模型）、条件随机场、玻尔兹曼机、受限玻尔兹曼机等．\"]},\"210\":{\"h\":\"对数线性模型\",\"t\":[\"势能函数一般定义为\",\"其中函数 为定义在 上的特征向量, 为权重向量. 这样联合概率 的对数形式为\",\"其中 代表所有势能函数中的参数 . 这种形式的无向图模型也称为对数线性模型 ( Log-Linear Model) 或最大熵模型 (Maximum Entropy Model) [Berger et al., 1996; Della Pietra et al., 1997]. 图11.8a所示是一个常用的最大熵模型. 如果用对数线性模型来建模条件概率 ,\",\"其中 . 对数线性模型也称为条件最大熵模型或Softmax 回归模型.\"]},\"211\":{\"h\":\"条件随机场\"},\"212\":{\"h\":\"有向图和无向图直接的转换\"},\"213\":{\"h\":\"参考\",\"t\":[\"[PII: 0004-3702(92)90065-6 (toronto.edu)](https://www.cs.toronto.edu/~bonner/courses/2016s/csc321/readings/Connectionist learning of belief networks.pdf) ↩︎\",\"Koller D, Friedman N, 2009. Probabilistic graphical models: principles and techniques[M]. MITpress. ↩︎\"]},\"214\":{\"h\":\"对抗攻击\"},\"215\":{\"h\":\"基本概念\",\"t\":[\"深度学习中的神经网络在精心训练后，其分类准确性可以非常出色，但其的鲁棒性却可能很差，可能会轻易被对抗攻击打破。即通过对输入图片进行一个微小的扰动，就可以在几乎肉眼看不出差距的前提下，让神经网络的分类准确率大幅下降。\",\"img\"]},\"216\":{\"h\":\"对抗鲁棒性\",\"t\":[\"【金山文档】 PGD https://kdocs.cn/l/csBKPtHtET4R\"]},\"217\":{\"h\":\"对抗攻击的分类\",\"t\":[\"白箱攻击（white-box attack）：在白箱攻击中攻击者知道目标模型的所有信息，包括模型的训练集、类型、结构以及参数。\",\"黑箱攻击（black-box attack）：在黑箱攻击中，攻击者不知道目标模型的内部细节，只能够观察目标模型对输入样本的输出结果。\",\"定向攻击（target attack）：对于一个多分类网络，把输入分类误判到一个某个特定的错误类别上\",\"非定向攻击（non-target attack）：只需要生成对抗样本，可以引入任意一个错误类别\"]},\"218\":{\"h\":\"FGSM\",\"t\":[\"白盒非指向性\",\"论文地址\",\"对抗样本（三）FGSM | BaiDing's blog (baidinghub.github.io)\",\"对抗攻击篇：FGSM 与 PGD 攻击算法 | Just for Life. (muyuuuu.github.io)\"]},\"219\":{\"h\":\"简介\",\"t\":[\"早期对对抗样本产生的原因的猜测集中于神经网络的非线性性和过拟合, 但是这篇论文证明神经网络的线性性质是造成神经网络具有对抗样本的主要原因. 同时, 该篇论文提出了一个能供更简单与更快速的生成对抗样本的方法。\"]},\"220\":{\"h\":\"对抗样本的线性解释\",\"t\":[\"我们知道，输入图像通常都是8bits的，这也就丟失了输入图像的1/255之间的信息。而如果对抗扰动足够小的话，是会被忽略的，因此作者猜测，是由于模型的线性所导致的。作者通过数学公式来解释。一个网络模型的权重为 \",\"对抗扰动让网络的激励增加了 ，我们只要将 ，就可以最大化的增加模型的激励，当 具有 维，平均权重值为 ，那么激励就会增长 ，但是 却并不会因为维度的增加而增加，这样，当我们增加一个很小的扰动的时候，就会产生很大的改变。这被称为\\\"accidental steganography\\\"，这种隐藏术的意思是，一个线性模型被迫只关注与权重相接近的信号，却会忽略那些权重大但不相关的振幅(像素点)。\",\"上述的解释说明，对一个简单的线性网络来说，如果他的输入有着足够的维度，那么他就会有对抗样本。先前对对抗样本的解释引用了了神经网络的假设特性，例如它们假定的高度非线性性质。我们基于线性的假设更简单，也可以解释为什么 softmax回归容易受到对抗性例子的影响。\"]},\"221\":{\"h\":\"非线性模型的线性扰动\",\"t\":[\"从对抗样本的线性视角来看，我们得出了一个很快的生成对抗样本的方法。我们假设神经网络是十分线性的。\",\"我们已知的一些模型，LSTM、ReLU、maxout网络都是被设计用线性的方式来运作的，所以比较容易优化，而非线性的模型，比如Sigmoid网络，我们会很难优化。但是线性，会让模型更容易受到攻击。\",\"模型的参数设为 记为模型的输入， 记为模型得到的标签， 记为神经网络使用的损失函数，同门可以通过以下的公式来得到对抗扰动:\",\"我们把这个叫做FGSM (fast gradient sign method)\",\"损失函数 衡量了网络预测 时的错误程度，其中 是网络参数， 是真实标签。当我们计算这个损失函数关于输入 的梯度时， 指示了哪个方向的微小变化会导致损失最大的增加。\",\"在机器学习中，梯度的数学定义告诉我们在多维空间中函数增长最快的方向。具体来说，对于一个可微分函数 ，其在点 处的梯度 指向函数值增长最快的方向。这是梯度的基本性质。\",\"现在，我们来形式化这个直觉：\",\"假设我们有一个小的扰动 加到输入 上，损失函数 在 处的一阶泰勒展开大约为：\",\"为了最大化损失增量，我们希望 尽可能大。根据柯西-施瓦茨不等式（Cauchy-Schwarz inequality），两个向量的点积的最大值是当它们是平行时取得的，即：\",\"等号成立当且仅当 是 的正比例，因此，损失增加最快的方向就是梯度的方向。\",\"然而，我们通常希望对输入的扰动 有一个大小限制，以确保扰动是微小的。FGSM 通过选择 来实现这一点，其中 是一个小常数。这样，每个元素的扰动都受到限制，并且整个扰动的 范数不超过 。\",\"因此， 是在给定大小限制下，使得损失函数增加最快的方向。这就是为什么 FGSM 使用这种特定形式的扰动来生成对抗样本的原因。\",\"这个公式是快速梯度符号方法（Fast Gradient Sign Method, 简称 FGSM）的核心，它是一种生成对抗性样本的技术。这个方法由 Ian Goodfellow 等人在 2014 年提出，旨在通过执行单步梯度更新来生成对抗性样本，具体来说是沿着使损失最大化的方向。\",\"在这个公式中：\",\" 是一个小的扰动量，它控制了对抗性扰动的强度。\",\" 是损失函数 关于输入 的梯度，这里 表示模型参数， 表示输入样本， 表示目标输出。\",\" 函数取梯度的符号，为每个元素返回 或 （或 ，如果梯度为零）。\",\"FGSM 的设计基于以下直觉：\",\"梯度方向：损失函数的梯度指向了损失增加最快的方向。在对抗性攻击的背景下，目标是找到一个小的扰动 ，使得当它加到输入样本 上时，会导致模型的损失增加，从而降低模型的性能。\",\"符号函数：使用 函数是为了得到一个大小为 的扰动，而不是直接使用梯度值。这确保了每个元素的扰动都是等量的，并且扰动的 范数（无穷范数，即向量中的最大元素）不超过 。这样的扰动通常很小，对人类的感知影响不大，但足以欺骗神经网络。\",\"效率：FGSM 是一种快速而简单的方法，因为它只需要计算一次输入 的梯度，然后应用符号函数和扰动系数 。这使得 FGSM 成为一种计算高效的对抗性攻击方法，尤其适合于生成大量对抗性样本。\",\"FGSM 生成的对抗性样本可以用来测试模型的鲁棒性或用于对抗性训练，后者是一种通过在训练过程中引入对抗性样本来提高模型鲁棒性的方法。\",\"作者在使用中，使用了 ，在MINST测试集上，对softmax分类器攻击达到了99.9%的攻击成功率，平均置信度为79.3%。使用相同的配置，对maxout网络，能达到89.4%的攻击成功率，平均置信度为97.6%。当使用卷积maxout网络与CIFAR-10数据集时，使用，达到了87.15%的攻击成功率，以及96.6%的平均置信度。同时，作者发现，使用其他的简单的方法也可以产生对抗样本，比如使x在梯度方向上旋转一定的角度，就可以产生对抗样本。\"]},\"222\":{\"h\":\"线性模型的对抗训练与权重衰减的对比研究\"},\"223\":{\"h\":\"PGD\",\"t\":[\"[1706.06083] Towards Deep Learning Models Resistant to Adversarial Attacks (arxiv.org)\"]},\"224\":{\"h\":\"抗扰性验证\",\"t\":[\"【金山文档】 Efficient Neural Network Robustness Certification\"]},\"225\":{\"h\":\"Notation\",\"t\":[\"假设神经网络一共有 层，输入 ,每一层神经元数量为 ,这里 。\",\"第 层到 层的权值矩阵为 ，偏置 。\",\" 表示第 层的激活值，， 其中，， 。\",\"那么对于第 层的第 个神经元，设它的预激活值为 , ， 表示矩阵 的第 行，\",\"让输入 的扰动值，限制在内，当 , let 是预激活值 的上界和下界 i.e. 。\"]},\"226\":{\"h\":\"激活函数的线性上上下界\",\"t\":[\"定义两个线性函数 : ,\",\"使得 其中.\",\" 依赖 and , 也就是说对于不同的 and 可以选择不同的参数。\",\"Also, for ease of exposition, in this paper we restrict . However, Theorem 3.2 can be easily generalized to the case of negative .\"]},\"227\":{\"h\":\"定理中涉及的符号\"},\"228\":{\"h\":\"抗扰性\",\"t\":[\"对抗样本\",\"抗扰性验证\"]},\"229\":{\"h\":\"动力系统\"},\"230\":{\"h\":\"动力系统的稳定性\",\"t\":[\"定义 如果对任意给定的 , 存在 一般与 和 有关), 使当任一 满足\",\"时, 方程组 (6.8) 的由初始条件 确定的解 均有\",\"则称方程组 (6.8) 的零解 为稳定的.\",\"如果零解 稳定, 且存在这样的 使当\",\"时, 满足初始条仵 的解 均有\",\"则称零解 为渐近稳定的.\",\"如果 渐近稳定, 且存在域 , 当且仅当 时满足初始条件 的解 约有 , 则域 称为 (渐近) 稳定域或吸引域. 若稳定域为全空间, 即 , 则称零解 为全局渐近稳定的或简称全局稳定的.\",\"当零解 不是稳定时, 称它是不稳定的. 即是说: 如果对某个给定的 不管 怎样小, 总有一个 满足 , 使由初始条件 所确定的解 , 至少存在某个 。使得\",\"则称方程组 (6.8) 的零解 为不稳定的.\",\"在二维情形零解的稳定形态，在平面上的示意图如下图\",\"零解的稳定形态\"]},\"231\":{\"h\":\"相平面\",\"t\":[\"现在讨论二阶微分方程组\",\"它的解\",\"在以  为坐标的 (欧氏)空间中决定了一条曲线, 这曲线称为积分曲线。假设方程右端的函数满足解的存在唯一性和连续性定理的条件, 例如存在连续偏导数, 此时空间的每一点都有一条且只有一条积分曲线经过\"]},\"232\":{\"h\":\"常微分方程\"},\"233\":{\"h\":\"常微分方程数值解\"},\"234\":{\"h\":\"预备知识\",\"t\":[\"术语截断误差指的是使用被截的即有限的和来近似计算无穷级数的和所产生的误差。\",\"舍入误差是计算器或计算机进行实数计算时所产生的。之所以产生舍人误差是因为机器中进行的算术运算所涉及的数是有限位的，从而导致计算只能用实际数值的近似表示式来完成。在典型的计算机中，仅实数系统的一个相对小的子集用于表示所有的实数。这个子集仅包含了正/负有理数，且存储了小数部分和指数部分。\",\"定义 1.17 假设 表示初始误差, 表示在以后的 步运算之后误差的大小。如果 (这里 是一个不依赖于 的常数), 则误差的增长称为是线性的。如果 (对某个 ), 则误差的增长称为是指数的。\",\"定义 1.18 设 是一个收玫于 0 的已知序列, 收敛于数 。如果存在一个正常数 使得\",\"对大的 成立, 则称 以收玫速度 (此表达式读作 “ 的大 oh”) 收玫于 。记为 \",\"虽然定义 1.18 允许将 与任意序列 进行比较, 但是几乎在每一种情况下都使用\",\"其中 为大于零的某个数。人们通常对于使 成立的最大的 值感兴趣。\",\"初值问题\",\"称为是一个适定的问题,如果:\",\"问题存在一个唯一的解 ;\",\"对任何 , 存在一个正常数 , 使得只要当 是连续的且在 上 时, 就有问题\",\"存在唯一解 , 且\",\"对一切 成立。 由式 所定义的问题称作和原问题 相伴的摄动问题。它假定微分方程有可能有误差 以及初值条件也有可能存在误差 。\",\"因为表示式中的任何舍人误差都使原问题摄动,所以数值方法总与求解摄动问题有关。如果原问题不是适定的，则没有理由期望摄动问题的数值解会精确地近原问题的解。\",\"定理 5.6 假设 。如果 是连续的, 且在 上关于变量 满足 条件, 则初值问题\",\"是适定的。\"]},\"235\":{\"h\":\"Euler法\",\"t\":[\"虽然Euler法的精确性不足以保证它在实际中的使用，但是在分析从它的应用所产生的误差方面它是一个重要的基础。在下面各节所考虑的更精确方法的误差分析按照同样的模式，只不过更复杂一些。\",\"为推导 Euler 法的误差界, 需要两个计算引理。\",\"引理 5.7 对所有 和任何正整数 , 有 。\",\"证明 对 和 , 应用 Taylor 定理得\",\"其中, 位于 和 0 之间。从而,有\",\"又因为 , 所以\",\"定理 5.9 假设 是连续的, 且在\",\"上满足常数为 的 条件, 又假设存在常数 使得\",\"对一切 成立。设 表示初值问题\",\"的唯一解, 是由 Euler 法对某个正整数 产生的近似。则对于 ,有\"]},\"236\":{\"h\":\"Talyot法\",\"t\":[\"定义 5.11 差分法\",\"有局部截断误差\",\"对 成立。 对于 Euler 法,在第 步对问题\",\"的局部截断误差是\",\"image-20231211151256664\"]},\"237\":{\"h\":\"runge-kutta法\",\"t\":[\"image-20231211152044454\",\"image-20231211152029421\",\"image-20231211152930899\",\"​\"]},\"238\":{\"h\":\"概率密度估计\",\"t\":[\"概率密度估计（Probabilistic Density Estimation），简称密度估计（Density Estimation），是基于一些观测样本来估计一个随机变量的概率密度函数．密度估计在数据建模、机器学习中使用广泛．\",\"密度估计方法可以分为两类：参数密度估计和非参数密度估计．\"]},\"239\":{\"h\":\"参数密度估计\",\"t\":[\"参数密度估计 ( Parametric Density Estimation) 是根据先验知识假设随机变量服从某种分布, 然后通过训练样本来估计分布的参数.\",\"令 为从某个未知分布中独立抽取的 个训练样本, 假设这些样本服从一个概率分布函数 , 其对数似然函数为\",\"使用最大似然估计 ( Maximum Likelihood Estimation, MLE ) 来寻找参数 使得 最大. 这样参数估计问题就转化为最优化问题:\"]},\"240\":{\"h\":\"正态分布\",\"t\":[\"假设样本 服从正态分布\",\"其中 和 分别为正态分布的均值和方差. 数据集 的对数似然函数为\",\"分别求上式关于 的偏导数,并令其等于 0 . 可得,\"]},\"241\":{\"h\":\"多项分布\",\"t\":[\"假设样本服从 个状态的多项分布, 令 one-hot 向量 来表示第 个状态, 即 , 其余 . 样本 的概率密度函数为\",\"其中 为第 个状态的概率, 并满足 . 数据集 的对数似然函数为\",\"多项分布的参数估计为约束优化问题. 引入拉格朗日乘子 , 将原问题转换为无约束优化问题.\",\"分别求上式关于 的偏导数, 并令其等于 0 . 可得,\",\"其中 为数据集中取值为第 个状态的样本数量. 在实际应用中,参数密度估计一般存在以下问题:\",\"模型选择问题: 即如何选择数据分布的密度函数. 实际数据的分布往往是非常复杂的,而不是简单的正态分布或多项分布.\",\"不可观测变量问题: 即我们用来训练的样本只包含部分的可观测变量, 还有一些非常关键的变量是无法观测的, 这导致我们很难准确估计数据的真实分布.\",\"维度灾难问题: 即高维数据的参数估计十分困难. 随着维度的增加, 估计参数所需要的样本数量指数增加. 在样本不足时会出现过拟合.\"]},\"242\":{\"h\":\"无监督学习\",\"t\":[\"无监督学习（Unsupervised Learning，UL）是指从无标签的数据中学习出一些有用的模式．\",\"无监督学习算法一般直接从原始数据中学习，不借助于任何人工给出标签或者反馈等指导信息．如果监督学习是建立输入-输出之间的映射关系，那么无监督学习就是发现隐藏的数据中的有价值信息，包括有效的特征、类别、结构以及概率分布等．\",\"典型的无监督学习问题可以分为以下几类：\",\"无监督特征学习（Unsupervised Feature Learning）是从无标签的训练数据中挖掘有效的特征或表示．无监督特征学习一般用来进行降维、数据可视化或监督学习前期的数据预处理．（特征学习也包含很多的监督学习算法，比如线性判别分析等．）\",\"概率密度估计（Probabilistic Density Estimation）简称密度估计，是根据一组训练样本来估计样本空间的概率密度．密度估计可以分为参数密度估计和非参数密度估计．参数密度估计是假设数据服从某个已知概率密度函数形式的分布（比如高斯分布），然后根据训练样本去估计概率密度函数的参数．非参数密度估计是不假设数据服从某个已知分布，只利用训练样本对密度进行估计，可以进行任意形状密度的估计．非参数密度估计的方法有直方图、核密度估计等．\",\"聚类（Clustering）是将一组样本根据一定的准则划分到不同的组（也称为簇（Cluster））．一个比较通用的准则是组内样本的相似性要高于组间样本的相似性．常见的聚类算法包括K-Means算法、谱聚类等\",\"和监督学习一样，无监督学习方法也包含三个基本要素：模型、学习准则和优化算法．无监督学习的准则非常多，比如最大似然估计、最小重构错误等．在无监督特征学习中，经常使用的准则为最小化重构错误，同时也经常对特征进行一些约束，比如独立性、非负性或稀释性等．而在密度估计中，经常采用最大似然估计来进行学习．\"]},\"243\":{\"h\":\"苹果 1\"},\"244\":{\"h\":\"标题 2\",\"t\":[\"这里是内容。\"]},\"245\":{\"h\":\"标题 3\",\"t\":[\"这里是内容。\"]},\"246\":{\"c\":[\"苹果\"]},\"247\":{\"c\":[\"红\",\"大\",\"圆\"]},\"248\":{\"h\":\"苹果 2\",\"t\":[\"一个被星标了的苹果文章。\"]},\"249\":{\"h\":\"标题 2\",\"t\":[\"这里是内容。\"]},\"250\":{\"h\":\"标题 3\",\"t\":[\"这里是内容。\"]},\"251\":{\"c\":[\"苹果\"]},\"252\":{\"c\":[\"红\",\"大\",\"圆\"]},\"253\":{\"h\":\"苹果 3\"},\"254\":{\"h\":\"标题 2\",\"t\":[\"这里是内容。\"]},\"255\":{\"h\":\"标题 3\",\"t\":[\"这里是内容。\"]},\"256\":{\"c\":[\"苹果\",\"水果\"]},\"257\":{\"c\":[\"红\",\"大\",\"圆\"]},\"258\":{\"h\":\"苹果 4\"},\"259\":{\"h\":\"标题 2\",\"t\":[\"这里是内容。\"]},\"260\":{\"h\":\"标题 3\",\"t\":[\"这里是内容。\"]},\"261\":{\"c\":[\"苹果\",\"水果\"]},\"262\":{\"c\":[\"红\",\"大\",\"圆\"]},\"263\":{\"h\":\"香蕉 1\"},\"264\":{\"h\":\"标题 2\",\"t\":[\"这里是内容。\"]},\"265\":{\"h\":\"标题 3\",\"t\":[\"这里是内容。\"]},\"266\":{\"c\":[\"香蕉\",\"水果\"]},\"267\":{\"c\":[\"黄\",\"弯曲的\",\"长\"]},\"268\":{\"h\":\"香蕉 2\",\"t\":[\"一个被数字 10 星标了的香蕉文章。\"]},\"269\":{\"h\":\"标题 2\",\"t\":[\"这里是内容。\"]},\"270\":{\"h\":\"标题 3\",\"t\":[\"这里是内容。\"]},\"271\":{\"c\":[\"香蕉\",\"水果\"]},\"272\":{\"c\":[\"黄\",\"弯曲的\",\"长\"]},\"273\":{\"h\":\"香蕉 3\"},\"274\":{\"h\":\"标题 2\",\"t\":[\"这里是内容。\"]},\"275\":{\"h\":\"标题 3\",\"t\":[\"这里是内容。\"]},\"276\":{\"c\":[\"香蕉\"]},\"277\":{\"c\":[\"黄\",\"弯曲的\",\"长\"]},\"278\":{\"h\":\"香蕉 4\"},\"279\":{\"h\":\"标题 2\",\"t\":[\"这里是内容。\"]},\"280\":{\"h\":\"标题 3\",\"t\":[\"这里是内容。\"]},\"281\":{\"c\":[\"香蕉\"]},\"282\":{\"c\":[\"黄\",\"弯曲的\",\"长\"]},\"283\":{\"h\":\"连续归一化流\",\"t\":[\"标准化流Normalizing flows (NF) 是一系列高维数据上灵活可重参数化概率分布的方法。它通过构造一系列可逆映射（通常由可逆神经网络参数化）将将任意复杂的数据分布变换为一个基本的简单分布（如单高斯分布、均匀分布等），通过巧妙设计的模型结构，模型可以对数据样本进行精确的最大似然估计,最终分布的概率密度由变量变换公式给出。\"]},\"284\":{\"h\":\"Preliminaries\",\"t\":[\"重参数化技巧 - 知乎 (zhihu.com)\"]},\"285\":{\"h\":\"重参数化技巧\",\"t\":[\"重参数化技巧，就是从一个分布  中进行采样，而该分布是带有参数  的，如果直接进行采样 (采样动作是离散的，其不可微），是没有梯度信息的，那么在BP反向传播的时候就不会对参数梯度进行更新。重参数化技巧可以保证我们从  进行采样，同时又能保留梯度信息。\"]},\"286\":{\"h\":\"神经常微分方程\",\"t\":[\"神经常微分方程（简称：NeuralODE）是一种通过使用神经网络参数化向量场来结合常微分方程和神经网络的一种全新的深度学习模型。在2018年首次被提出和使用[1]，它将传统的有限层神经网络架构转变为参数共享的无限层神经网络架构，弥合了深度学习和动态系统的差距。NODE从网络架构，训练方法，超参数选择，表示能力等均和传统的神经网络例如CNN,MLP，RNN等有所不同。\",\"本章首先给出神经常微分方程(以后简称NODE)的定义，NODE模型的思想来源。然后介绍针对NODE的一种全新的反向传播方法-adjoint sensitivity 方法和相应的更广义的证明，对于NODE的表示能力用严格的数学理论进行详细论述，同时对训练用到的超参数的选择，网络架构的选取进行讨论。\",\"预备知识.md\",\"定义.md\",\"表示和逼近能力.md\",\"优化算法.md\",\"设计网络架构.md\",\"[1806.07366] Neural Ordinary Differential Equations (arxiv.org)↩︎\"]},\"287\":{\"h\":\"优化算法\"},\"288\":{\"h\":\"自适应检查点伴随法\",\"t\":[\"我们证明了其性能较差的一个原因是现有的梯度估计方法的不准确性：伴随方法在反模态积分中存在数值误差；\",\"由于正模和反模轨迹被视为两个独立的ivp，它们并不准确相等，导致梯度估计的误差\"]},\"289\":{\"c\":[\"神经微分方程\"]},\"290\":{\"h\":\"定义\",\"t\":[\"首先给出NODE的定义： 设有如下高阶常微分方程和初始条件：\",\"定义\",\"其中是神经网络架构（比如全连接神经网络），称该微分方程为神经常微分方程，简称NeuralODE。在上关于 是连续的， 是该网络架构的参数，根据picard存在定理， 初值问题有唯一解，当固定时,函数以作为特征输入可以看成一个神经网络，称为终止时间,表示网络的深度。\",\"这里 假设全局 关于 是连续是非常合理的，因为最常见和有效的激活函数和网络例如ReLUs，CNN，max,全连接网络等都是全局连续的。\",\"Lemma 7.1 Suppose is composed of a finite number of ReLU activations and linear transforms,\",\"假设 由有限的激活函数为ReLU的仿射层组成，其中仿射层有界 ,即：\",\"那么初值问题 有唯一解。\",\"证明： 不依赖与 ，所以关于 连续，ReLU（或者其他激活函数例如sigmoid）是一制度\",\"回顾下一个resnet网络架构中的resnet块：\",\"函数是带参数的神 经网络函数，是网络层数。 将区间分割为为单位,通过使用euler法来可以求得的每个时刻， 那么就有\",\"观察和可以发现每个resnet块类似于欧拉法求数值解微分方程时步长为1的情形（）。\",\"例如：一个resnet块具体定义为\",\"Consider now the Neural ODE\",\"在常规神经网络中，我们从输出出发得到隐状态层(假设是层神经网络)， 每层参数化，参数量随着层数的增加而增加。 所以若我们在renet层级间加入更多的层，且最终趋向于添加了无穷层时，残差神经网络架构就与欧拉法解方程统一起来：resnet前向传播可以看做时欧拉法解神经微分方程，而神经微分方程就是参数共享的resnet网络模型的连续化。 后面还会从rnn的角度和ode进行统一建模，其实目前大部分有效的流行的深度学习框架都类似于微分方程，若干个世纪以来，微分方程一直被用来做应用建模。\",\"目前可以看出研究和使用node的一个好处在于ode的理论优势，数学领域，对ode的研究不管是理论还是数值分析，建模方法，训练思路都是丰富且先进，可以帮助我们更好的进行深度学习的研究。 由于ode的求解比较难，虽然可以使用ode求解器，但是由于维度巨大（例如一个1080p像素点有个），计算的时间复杂仍然很高，一般将node作为深度学习网络框架中的一部分，node的输入是被前神经网络处理过的维度较低的数据，同时对node的输出再进行处理，例如添加一层全连接层或者非线性层，这也大幅提高NeuralODE的表达能力。 下面给出更一般形式的含有node的网络架构：\",\"注意\",\"虽然NODE中的输入和输出是固定的维度，但是网络架构是任意的，可以出现更高维度。在后续会深入讨论网络架构的选择和设计。\"]},\"291\":{\"h\":\"神经微分方程的稳定性\",\"t\":[\"神经微分方程稳定性概念\"]},\"292\":{\"h\":\"网络设计\"},\"293\":{\"h\":\"增维的的NODE\",\"t\":[\"再 表示和逼近.md 中提到过对输入进行维度增加可以提高模型的表达能力，再 表示和逼近.md 中采用的方法非常简单：将输入的维度增加一倍 ，然后在额外的维度上补0， 这个理论上可以表示任何同胚映射，但是它也有很大的缺陷，在 表示和逼近.md 中已经表述过。下面子节是其他的用于增维的方法来解决 表示和逼近.md 中描述的缺陷。\"]},\"294\":{\"h\":\"对输入做仿射变换\",\"t\":[\"对于给定的初值 , 设 是一个含参的仿射变换，那么将 作为微分方程的初值，然后的到 作为NODE的输出，其中参数是需要学习的。\",\"采用仿射的目的是 提高模型的表达能力的同时，做到高效的训练，不需要增加一倍的维度，更具体点可以设 ,这就更进一步降低参数量了,同时也保证了前 维的结构。当然这种增强破坏了NODE的双射的性质，那么下一章节的连续归一化流就不能使用，因为它要求可逆。\",\"Lifting into a higher-dimensional space may be regarded as a relaxation of the Markov property[1]. For then the output does not completely determine . In contrast does determine . (Whether is the output of an unaugmented neural or the latent value of an augmented neural ODE.)\"]},\"295\":{\"h\":\"随机化额外维度的初始值\",\"t\":[\"也可以使用服从某个随机分布（正太分布或区间分布）的随机变量来初始化额外维度的初始值，和是直接设为0相比，他的优势如下\",\"在数学上阐述为什么使用随机噪声初始化增广维度比使用零初始化更有利的原因可以从以下几个角度来考虑：\",\"非线性系统的敏感性：对于非线性系统，初始条件的微小变化可能会导致长期行为的显著不同。这是混沌理论的一个关键观点，即初始条件的微小扰动可能导致完全不同的轨迹。在数学上，这可以通过Lyapunov指数来量化，它度量了相邻轨迹随时间分离的速率。随机噪声可以使得系统从一系列不同的初始条件开始演化，从而能够探索状态空间中的更多区域。\",\"打破对称性：如果增广的维度被初始化为零，它们的行为会是对称的，因为在没有额外信息的情况下，它们会接收到相同的梯度更新。这在数学上意味着，如果我们有一个向量场 ，那么对于所有的初始点 ，如果它们的增广部分是相同的，那么它们的轨迹也将是相同的。随机噪声打破了这种对称性，使得即使是相同的原始输入，也能够在增广空间中产生不同的轨迹。\",\"增加状态空间的覆盖：从概率论的角度来看，随机噪声允许系统在整个状态空间中以某种概率分布进行采样。如果增广维度总是初始化为零，那么系统的状态将始终局限于一个低维的子空间，从而限制了网络能够学习的功能。随机噪声初始化允许网络在训练过程中探索更多的状态组合。\",\"局部最小和鞍点：在优化理论中，初始化可以极大地影响优化算法的收敛性。如果所有的路径都从相同的点开始，那么它们可能会陷入相同的局部最小或鞍点。通过随机初始化，我们可以提高找到全局最小或更好局部最小的机会，因为不同的路径可能会避开某些不良的局部最小。\",\"正则化效果：在统计学习理论中，添加随机性可以看作是一种隐式的正则化。这有助于防止过拟合，因为它限制了模型在训练数据上的完美拟合能力，从而提高了模型在未见数据上的泛化能力。\",\"尽管从理论上有这些考虑，但在实际操作中，是否使用随机噪声以及具体的噪声水平通常是基于经验和实验调整的。一些问题可能会从零初始化中受益，而其他问题则可能需要随机噪声来提高性能。因此，实践者通常会尝试不同的初始化策略，以找到最适合他们特定问题的方法。\"]},\"296\":{\"h\":\"分段的NODE\",\"t\":[\"可以让一个NODE由若干的不同范围的的NODE表示,例如：\",\"其中参数 , s都表示不同的网络，但是为了表示，研究和训练的方便，所有的 的基本架构是一样的, 唯一的区别就是依赖的向量 。 关于这种形式的微分方程，训练方式有两种：\",\"调用一次ODESolver直接在 上计算，那么如果采用的是自适应步长的ODE求解器，那么需要提前告知分段点，否则求解器会在分段点减速再缓慢返回。\",\"调用次ODESolver，分别计算在每个求解相应的ODE，这会需要额外的 来存储前向计算中的 用来做反向传播。\",\"根据灵敏度公式，他的参数梯度如下：\",\"Proof. The proof follows from the one of Theorems 1 and 1 by recalling the solution of the stacked neural ODEs:\",\"We can recover a relation similar to (12)\",\"Since\",\"we have\",\"which leads to the result by assuming to satisfy\",\"Neural综述 ↩︎\"]},\"297\":{\"h\":\"表示和逼近能力\"},\"298\":{\"h\":\"Notation\",\"t\":[\"定义：p,sup范数\",\"设 ， . 对于可测映射 ，子集 , 设\",\"其中 是Euclidean 范数 . 定义 .\",\"定义：通用逼近器\",\"设, 令 由可测函数 构成, 其中 是 的可测子集， 依赖， 称 是的意义下的通用逼近器，如果 , , ,其中是紧集， 那么 ,s.t. .\",\"设\",\"下一节中证明，中的都是同胚映射。因此如果作为假设空间表示能力和逼近能力都非常有限，下面给出Neural更一般形式的假设空间：\",\"在这个更加广义的集合中，将终止时间也作为了一个参数，同时在后添加了一层终端函数（以为输入），用来提高模型的表达能力。\"]},\"299\":{\"h\":\"同胚和流形假设\",\"t\":[\"引理1\",\"设 和 分别是初值问题和的两个解， 那么 ​.\",\"定理1\",\", 是同胚映射，保留了对输入空间的拓扑结构。\",\"证明：\",\"证明分为三个部分：\",\"是连续的： , 根据 Gronwall's 引理：\",\"​ 令 , 那么 ， 是连续的。\",\"是双射：由引理得出。\",\"是连续的:\",\"构造一个新的初值问题：\",\"然后根据连续和双射的证明得出。\",\"​\",\"下面给出一个最简单的一维上不能表示的函数。\",\"注意\",\"函数不能被表示。\",\"证明：\",\"​ 假设存在微分方程有两个解和满足：\",\"​ 定义函数，那么h(0)=2,h(T)=-2 ,连续函数介质定理，一定存在着,使得,根据\\\\ref{ode_bi},同一个微分方程的两个轨迹是不想交的，所以矛盾，因此不存在能表示函数。 ​\",\"流形假设(manifold hypothesis) 是机器学习和模式识别领域中的一个重要概念。它提出了一个假设，即高维数据通常存在于低维流形中。 简而言之，流形是一种具有局部线性结构的几何对象。在高维空间中，数据点可能分布在一个比观察到的维度更低的流形上。这意味着，尽管数据点在高维空间中可能看起来很复杂，但它们实际上可以由较少的自由度来描述。流形假设的核心思想是，学习算法可以通过寻找数据的低维表示来更好地理解和处理高维数据。通过将高维数据映射到低维流形上，可以减少数据的维度，并且可以更好地表示数据的内在结构和特征。流形假设在降维、特征提取、聚类和分类等机器学习任务中具有重要的应用。它为我们提供了一种理解高维数据的方法，并且可以帮助我们设计更有效的学习算法。\",\"定理\\\\ref{tongpei}表明：NODE会连续的变形输入空间而不会撕裂一个连接的区域。所以NODE能与流行假设优雅的进行交互：NODE描述了输入的流形如何随着深度流动到输出流形，这也对于部分任务例如图形生成任务来，机器学习可解释化是优势，但对某些任务来说是劣势。因此假设空间 对模型的表达能力很低，下面个各小节介绍一些方法来提高模型的表达能力。\"]},\"300\":{\"h\":\"增维的NODE\",\"t\":[\"并不能表示所有的同胚，但是可以通过增加维度来做到，下面这个定理说明通过将维度增加一倍数可以表示任何同胚映射。\",\"定理2\",\"对任何同胚映射,都存在NeuralODE，维度为,使得：，有。\",\"在增加维度的NeuralODE后添加一个线性层，那么可以逼近任何函数。\",\"定理3\",\"​设, ,函数 的形式如下\",\"​ 要满足微分方程解的唯一性 ,那么\",\"​ 在意义上逼近 .\",\"证明：\",\"​ 考虑如下的微分方程组：\",\"​ 每一组都可以单独求解：\",\"​\",\"​ 不妨设,那么得到了直到阶的单项式：\",\"​ ​ ​ ​ ​ 根据 Stone-Weierstrass theorem 定理： , ,足够大, 可以找到仿射函数 ,使得 ​\",\"令 ， 由堆叠而层， 用来对输入 补零， 对单项式进行组合以逼近目标连续函数.\",\"​ ​ ​ \\\\end\",\"定理从和定理虽然从理论上证明通过增维可以让模型具有通用逼近的能力，但是在训练过程中，会带来如下问题：\",\"\\\\begin{enumerate}\\n\\\\item 训练成本过高：如果维度增加倍，那么显存占比也将翻倍，还有训练时间指数级增加等；\\n\\\\item 训练过程不可控：在定理中虽然对的要求降低了，但是维数也变成了一个不可控量，造成训练的混乱，模型的解释性差等。\\n\\\\item 造成参数冗余：NeuralODE的一个优点就在于，参数共享，一般将Neural以图像或者词向量作为输入在增加维度，那么会大幅度降低参数有效性；\\n\\\\item 破坏可逆性：只要在最理想的训练情况下，才能在输出的填充维上的值全为0。\\n\\\\end{enumerate}\\n因此定理从和定理实际上只是一种理论上表达通过提高维度可以模型的表达能力，但实际定理只是给出了充分条件，我们可以减少增加维数，改变增加维数的方式（替换直接对输入额外的维度补0来增维）来提高训练的高效性。在$\\\\ref{sec:design} $中会介绍几种增加维度的NeuralODE网络架构。\"]},\"301\":{\"c\":[\"神经微分方程\"]},\"302\":{\"h\":\"预备知识\",\"t\":[\"定义\",\"设函数,其中是一个维向量，设未知函数满足方程\",\"称该微分方程为一阶标准方程。如果与时间无关，那么称微分方程是自治的，否则为非自治的。\",\"本文只讨论基于一阶标准微分方程的神经网络架构，原因在于一阶标准方程具有良好表示能力，在优化算法上有丰富的理论来源，有各种高效的数值解和现成的计算框架，而且模型简单更容易设计，根据机器学习中的奥卡姆剃刀原理： 需要假设最少的解释往往是最接近真相解释。\",\"定义\",\"方程的初值问题就是在未知函数有初值条件的条件下，求出未知函数，它在包含的某个区间上可微，且满足。\",\"任何初值问题记为,解记为,如果微分方程是自治的，那么根据积分曲线的平移不变形，取，,相应的解记作。\",\"定义\",\"设开集 是一个 映射 映射, , 且满足性质：\",\";\",\"。\",\"则对给定的  是一个  映射  映射) ，  称为  上的  动力系统 (  动力系统) 或  流 (  流)。\"]},\"303\":{\"h\":\"Bar 功能\"},\"304\":{\"h\":\"介绍\",\"t\":[\"我们支持 bar 功能，...\"]},\"305\":{\"h\":\"详情\",\"t\":[\"baz\",\"...\"]},\"306\":{\"h\":\"Baz\",\"t\":[\"功能详情...\"]},\"307\":{\"h\":\"Foo 功能\"},\"308\":{\"h\":\"介绍\",\"t\":[\"我们支持 foo 功能，...\"]},\"309\":{\"h\":\"详情\",\"t\":[\"ray\",\"...\"]},\"310\":{\"h\":\"Ray\",\"t\":[\"功能详情...\"]},\"311\":{\"h\":\"\",\"t\":[\"Alessio Quaglino, Marco Gallieri, Jonathan Masci, and Jan Koutn´ık. Accelerating neural odes with spectral elements. arXiv preprint arXiv:1906.07038, 2019\"]},\"312\":{\"h\":\"Posts\"},\"313\":{\"h\":\"Apple\"},\"314\":{\"h\":\"Banana\"}},\"dirtCount\":0,\"index\":[[\"quaglino\",{\"1\":{\"311\":1}}],[\"详情\",{\"0\":{\"305\":1,\"309\":1}}],[\"详情容器\",{\"0\":{\"98\":1,\"180\":1},\"1\":{\"63\":1,\"98\":2,\"180\":2}}],[\"映射\",{\"1\":{\"302\":4}}],[\"解记为\",{\"1\":{\"302\":1}}],[\"解释\",{\"1\":{\"148\":1,\"153\":1}}],[\"求出未知函数\",{\"1\":{\"302\":1}}],[\"求变分与求导数这两种运算次序可以交换\",{\"1\":{\"112\":1}}],[\"$中会介绍几种增加维度的neuralode网络架构\",{\"1\":{\"300\":1}}],[\"替换直接对输入额外的维度补0来增维\",{\"1\":{\"300\":1}}],[\"改变增加维数的方式\",{\"1\":{\"300\":1}}],[\"才能在输出的填充维上的值全为0\",{\"1\":{\"300\":1}}],[\"破坏可逆性\",{\"1\":{\"300\":1}}],[\"造成参数冗余\",{\"1\":{\"300\":1}}],[\"造成训练的混乱\",{\"1\":{\"300\":1}}],[\"补零\",{\"1\":{\"300\":1}}],[\"足够大\",{\"1\":{\"300\":1}}],[\"考虑如下的微分方程组\",{\"1\":{\"300\":1}}],[\"要满足微分方程解的唯一性\",{\"1\":{\"300\":1}}],[\"要使用此布局\",{\"1\":{\"0\":1}}],[\"流\",{\"1\":{\"302\":2}}],[\"流形是一种具有局部线性结构的几何对象\",{\"1\":{\"299\":1}}],[\"流形假设在降维\",{\"1\":{\"299\":1}}],[\"流形假设的核心思想是\",{\"1\":{\"299\":1}}],[\"流形假设\",{\"1\":{\"299\":1}}],[\"流程图\",{\"0\":{\"79\":1}}],[\"构造一个新的初值问题\",{\"1\":{\"299\":1}}],[\"构成\",{\"1\":{\"298\":1}}],[\"构成了一个贝叶斯网络\",{\"1\":{\"202\":1}}],[\"保留了对输入空间的拓扑结构\",{\"1\":{\"299\":1}}],[\"子集\",{\"1\":{\"298\":1}}],[\"他的参数梯度如下\",{\"1\":{\"296\":1}}],[\"他的优势如下\",{\"1\":{\"295\":1}}],[\"调用次odesolver\",{\"1\":{\"296\":1}}],[\"调用一次odesolver直接在\",{\"1\":{\"296\":1}}],[\"唯一的区别就是依赖的向量\",{\"1\":{\"296\":1}}],[\"研究和训练的方便\",{\"1\":{\"296\":1}}],[\"实践者通常会尝试不同的初始化策略\",{\"1\":{\"295\":1}}],[\"实际数据的分布往往是非常复杂的\",{\"1\":{\"241\":1}}],[\"实际的文章内容\",{\"1\":{\"54\":1}}],[\"尽管数据点在高维空间中可能看起来很复杂\",{\"1\":{\"299\":1}}],[\"尽管从理论上有这些考虑\",{\"1\":{\"295\":1}}],[\"尽可能大\",{\"1\":{\"221\":1}}],[\"添加随机性可以看作是一种隐式的正则化\",{\"1\":{\"295\":1}}],[\"局部最小和鞍点\",{\"1\":{\"295\":1}}],[\"局部马尔可夫性质\",{\"1\":{\"202\":2}}],[\"增维的node\",{\"0\":{\"300\":1}}],[\"增维的的node\",{\"0\":{\"293\":1}}],[\"增加状态空间的覆盖\",{\"1\":{\"295\":1}}],[\"打破对称性\",{\"1\":{\"295\":1}}],[\"初始化可以极大地影响优化算法的收敛性\",{\"1\":{\"295\":1}}],[\"初始条件的微小变化可能会导致长期行为的显著不同\",{\"1\":{\"295\":1}}],[\"初值问题有唯一解\",{\"1\":{\"290\":1}}],[\"初值问题\",{\"1\":{\"234\":1}}],[\"正则化效果\",{\"1\":{\"295\":1}}],[\"正太分布或区间分布\",{\"1\":{\"295\":1}}],[\"正态分布\",{\"0\":{\"240\":1}}],[\"做到高效的训练\",{\"1\":{\"294\":1}}],[\"建模方法\",{\"1\":{\"290\":1}}],[\"微分方程一直被用来做应用建模\",{\"1\":{\"290\":1}}],[\"微积分\",{\"1\":{\"169\":1}}],[\"后面还会从rnn的角度和ode进行统一建模\",{\"1\":{\"290\":1}}],[\"后者是一种通过在训练过程中引入对抗性样本来提高模型鲁棒性的方法\",{\"1\":{\"221\":1}}],[\"残差神经网络架构就与欧拉法解方程统一起来\",{\"1\":{\"290\":1}}],[\"观察和可以发现每个resnet块类似于欧拉法求数值解微分方程时步长为1的情形\",{\"1\":{\"290\":1}}],[\"经网络函数\",{\"1\":{\"290\":1}}],[\"经常采用最大似然估计来进行学习\",{\"1\":{\"242\":1}}],[\"经常使用的准则为最小化重构错误\",{\"1\":{\"242\":1}}],[\"伴随方法在反模态积分中存在数值误差\",{\"1\":{\"288\":1}}],[\"自适应检查点伴随法\",{\"0\":{\"288\":1}}],[\"自定义它们\",{\"1\":{\"88\":1}}],[\"自定义对齐\",{\"0\":{\"66\":1}}],[\"自定义标题\",{\"1\":{\"63\":5,\"93\":2,\"94\":2,\"95\":2,\"96\":2,\"98\":2,\"176\":2,\"177\":2,\"178\":2,\"179\":2,\"180\":2}}],[\"网络设计\",{\"0\":{\"292\":1}}],[\"网络架构的选取进行讨论\",{\"1\":{\"286\":1}}],[\"网络结构学习比较困难\",{\"1\":{\"189\":1}}],[\"超参数选择\",{\"1\":{\"286\":1}}],[\"弥合了深度学习和动态系统的差距\",{\"1\":{\"286\":1}}],[\"采样动作是离散的\",{\"1\":{\"285\":1}}],[\"采用仿射的目的是\",{\"1\":{\"294\":1}}],[\"采用坐标上升法\",{\"1\":{\"192\":1}}],[\"采用梯度上升方法进行最大似然估计\",{\"1\":{\"192\":1}}],[\"均匀分布等\",{\"1\":{\"283\":1}}],[\"均有\",{\"1\":{\"230\":2}}],[\"通用逼近器\",{\"1\":{\"298\":1}}],[\"通常由可逆神经网络参数化\",{\"1\":{\"283\":1}}],[\"通过将高维数据映射到低维流形上\",{\"1\":{\"299\":1}}],[\"通过随机初始化\",{\"1\":{\"295\":1}}],[\"通过使用euler法来可以求得的每个时刻\",{\"1\":{\"290\":1}}],[\"通过巧妙设计的模型结构\",{\"1\":{\"283\":1}}],[\"通过选择\",{\"1\":{\"221\":1}}],[\"通过迭代的方法来最大化边际似然\",{\"1\":{\"194\":1}}],[\"通过最大化整个训练集的对数边际似然\",{\"1\":{\"194\":1}}],[\"通过路径\",{\"1\":{\"109\":1}}],[\"通过\",{\"1\":{\"59\":1,\"62\":1}}],[\"通过启用\",{\"1\":{\"34\":1}}],[\"星标了的香蕉文章\",{\"1\":{\"268\":1}}],[\"长\",{\"2\":{\"267\":1,\"272\":1,\"277\":1,\"282\":1}}],[\"弯曲的\",{\"2\":{\"267\":1,\"272\":1,\"277\":1,\"282\":1}}],[\"黄\",{\"2\":{\"267\":1,\"272\":1,\"277\":1,\"282\":1}}],[\"香蕉\",{\"0\":{\"263\":1,\"268\":1,\"273\":1,\"278\":1},\"2\":{\"266\":1,\"271\":1,\"276\":1,\"281\":1}}],[\"苹果\",{\"0\":{\"243\":1,\"248\":1,\"253\":1,\"258\":1},\"2\":{\"246\":1,\"251\":1,\"256\":1,\"261\":1}}],[\"学习算法可以通过寻找数据的低维表示来更好地理解和处理高维数据\",{\"1\":{\"299\":1}}],[\"学习准则和优化算法\",{\"1\":{\"242\":1}}],[\"学习问题\",{\"1\":{\"197\":1}}],[\"谱聚类等\",{\"1\":{\"242\":1}}],[\"聚类和分类等机器学习任务中具有重要的应用\",{\"1\":{\"299\":1}}],[\"聚类\",{\"1\":{\"242\":1}}],[\"核密度估计等\",{\"1\":{\"242\":1}}],[\"特征提取\",{\"1\":{\"299\":1}}],[\"特征学习也包含很多的监督学习算法\",{\"1\":{\"242\":1}}],[\"特殊情况\",{\"1\":{\"202\":1}}],[\"典型的无监督学习问题可以分为以下几类\",{\"1\":{\"242\":1}}],[\"估计参数所需要的样本数量指数增加\",{\"1\":{\"241\":1}}],[\"估计每个条件概率分布的参数\",{\"1\":{\"189\":1}}],[\"引入拉格朗日乘子\",{\"1\":{\"241\":1}}],[\"引理1\",{\"1\":{\"299\":1}}],[\"引理\",{\"1\":{\"235\":1,\"299\":1}}],[\"多项分布的参数估计为约束优化问题\",{\"1\":{\"241\":1}}],[\"多项分布\",{\"0\":{\"241\":1}}],[\"多个动画片段\",{\"0\":{\"25\":1}}],[\"差分法\",{\"1\":{\"236\":1}}],[\"产生的近似\",{\"1\":{\"235\":1}}],[\"产生的增量\",{\"1\":{\"112\":1}}],[\"法\",{\"1\":{\"236\":1}}],[\"法对某个正整数\",{\"1\":{\"235\":1}}],[\"法的误差界\",{\"1\":{\"235\":1}}],[\"又假设存在常数\",{\"1\":{\"235\":1}}],[\"又因为\",{\"1\":{\"235\":1}}],[\"又称泛函的变分或变分\",{\"1\":{\"113\":1}}],[\"又称聚宝门\",{\"1\":{\"46\":1}}],[\"位于\",{\"1\":{\"235\":1}}],[\"应用\",{\"1\":{\"235\":1}}],[\"应在所讨论的区间内同时满足下列不等式\",{\"1\":{\"111\":1}}],[\"证明分为三个部分\",{\"1\":{\"299\":1}}],[\"证明\",{\"1\":{\"235\":1,\"290\":1,\"299\":2,\"300\":1}}],[\"人们通常对于使\",{\"1\":{\"234\":1}}],[\"允许将\",{\"1\":{\"234\":1}}],[\"收玫于\",{\"1\":{\"234\":1}}],[\"收敛于数\",{\"1\":{\"234\":1}}],[\"收敛\",{\"1\":{\"195\":1}}],[\"负有理数\",{\"1\":{\"234\":1}}],[\"仅实数系统的一个相对小的子集用于表示所有的实数\",{\"1\":{\"234\":1}}],[\"仅当\",{\"1\":{\"194\":1}}],[\"舍入误差是计算器或计算机进行实数计算时所产生的\",{\"1\":{\"234\":1}}],[\"术语截断误差指的是使用被截的即有限的和来近似计算无穷级数的和所产生的误差\",{\"1\":{\"234\":1}}],[\"预备知识\",{\"0\":{\"234\":1,\"302\":1},\"1\":{\"286\":1}}],[\"预览模式\",{\"0\":{\"36\":1}}],[\"常微分方程数值解\",{\"0\":{\"233\":1}}],[\"常微分方程\",{\"0\":{\"232\":1}}],[\"常见的聚类算法包括k\",{\"1\":{\"242\":1}}],[\"常见的无向图模型\",{\"0\":{\"209\":1}}],[\"常见的有向图模型\",{\"0\":{\"203\":1}}],[\"常见的概率图模型可以分为两类\",{\"1\":{\"201\":1}}],[\"空间中决定了一条曲线\",{\"1\":{\"231\":1}}],[\"空间复杂度o\",{\"1\":{\"148\":1}}],[\"空间复杂度\",{\"1\":{\"143\":1}}],[\"欧氏\",{\"1\":{\"231\":1}}],[\"欧拉\",{\"1\":{\"109\":2}}],[\"零解的稳定形态\",{\"1\":{\"230\":1}}],[\"至少存在某个\",{\"1\":{\"230\":1}}],[\"怎样小\",{\"1\":{\"230\":1}}],[\"称该微分方程为一阶标准方程\",{\"1\":{\"302\":1}}],[\"称该微分方程为神经常微分方程\",{\"1\":{\"290\":1}}],[\"称\",{\"1\":{\"298\":1}}],[\"称它是不稳定的\",{\"1\":{\"230\":1}}],[\"称为终止时间\",{\"1\":{\"290\":1}}],[\"称为是一个适定的问题\",{\"1\":{\"234\":1}}],[\"称为\",{\"1\":{\"230\":1,\"302\":1}}],[\"称为团\",{\"1\":{\"208\":1}}],[\"称为转移概率\",{\"1\":{\"206\":1}}],[\"称为输出概率\",{\"1\":{\"206\":1}}],[\"称为证据下界\",{\"1\":{\"194\":1}}],[\"称为高斯信念网络\",{\"1\":{\"191\":1}}],[\"称为向量\",{\"1\":{\"172\":1}}],[\"称为泛函\",{\"1\":{\"113\":1}}],[\"称为泛函的核\",{\"1\":{\"113\":1}}],[\"称为泛函形式或变分积分\",{\"1\":{\"113\":1}}],[\"称为最简单的积分型泛函\",{\"1\":{\"113\":1}}],[\"称为宗量\",{\"1\":{\"112\":1}}],[\"称为变分记号\",{\"1\":{\"112\":1}}],[\"称为函数\",{\"1\":{\"112\":3}}],[\"称为在函数\",{\"1\":{\"111\":1}}],[\"称为具有\",{\"1\":{\"111\":1}}],[\"约有\",{\"1\":{\"230\":1}}],[\"稳定域或吸引域\",{\"1\":{\"230\":1}}],[\"稳定\",{\"1\":{\"230\":1}}],[\"确定的解\",{\"1\":{\"230\":1}}],[\"方程的初值问题就是在未知函数有初值条件的条件下\",{\"1\":{\"302\":1}}],[\"方程组\",{\"1\":{\"230\":1}}],[\"方法和相应的更广义的证明\",{\"1\":{\"286\":1}}],[\"方便设计新模型等\",{\"1\":{\"198\":1}}],[\"存在唯一解\",{\"1\":{\"234\":1}}],[\"存在一个正常数\",{\"1\":{\"234\":1}}],[\"存在一个随机向量\",{\"1\":{\"167\":1}}],[\"存在\",{\"1\":{\"230\":1}}],[\"动力系统的稳定性\",{\"0\":{\"230\":1}}],[\"动力系统\",{\"0\":{\"229\":1},\"1\":{\"302\":2}}],[\"动画\",{\"0\":{\"21\":1,\"23\":1}}],[\"动画片段用于高亮或显隐幻灯片中的元素\",{\"1\":{\"19\":1}}],[\"动画片段\",{\"0\":{\"18\":1,\"19\":1,\"20\":1,\"22\":1,\"24\":1,\"26\":1}}],[\"抗扰性\",{\"0\":{\"228\":1}}],[\"抗扰性验证\",{\"0\":{\"224\":1},\"1\":{\"228\":1}}],[\"依赖\",{\"1\":{\"226\":1,\"298\":1}}],[\"依赖当前时刻的隐变量\",{\"1\":{\"206\":1}}],[\"激活函数的线性上上下界\",{\"0\":{\"226\":1}}],[\"限制在内\",{\"1\":{\"225\":1}}],[\"行\",{\"1\":{\"225\":1}}],[\"偏置\",{\"1\":{\"225\":1}}],[\"层的第\",{\"1\":{\"225\":1}}],[\"层的激活值\",{\"1\":{\"225\":1}}],[\"层的权值矩阵为\",{\"1\":{\"225\":1}}],[\"层到\",{\"1\":{\"225\":1}}],[\"层\",{\"1\":{\"225\":1}}],[\"线性模型的对抗训练与权重衰减的对比研究\",{\"0\":{\"222\":1}}],[\"线性代数主要包含向量\",{\"1\":{\"170\":1}}],[\"线性代数\",{\"0\":{\"170\":1},\"1\":{\"169\":1}}],[\"达到了87\",{\"1\":{\"221\":1}}],[\"能达到89\",{\"1\":{\"221\":1}}],[\"能够使得两点之间的距离最小呢\",{\"1\":{\"109\":1}}],[\"平均置信度为97\",{\"1\":{\"221\":1}}],[\"平均置信度为79\",{\"1\":{\"221\":1}}],[\"平均权重值为\",{\"1\":{\"220\":1}}],[\"尤其适合于生成大量对抗性样本\",{\"1\":{\"221\":1}}],[\"成为一种计算高效的对抗性攻击方法\",{\"1\":{\"221\":1}}],[\"成立的最大的\",{\"1\":{\"234\":1}}],[\"成立\",{\"1\":{\"111\":1,\"194\":1,\"234\":2,\"235\":1,\"236\":1}}],[\"效率\",{\"1\":{\"221\":1}}],[\"范数\",{\"1\":{\"221\":1,\"298\":1}}],[\"范数不超过\",{\"1\":{\"221\":1}}],[\"符号函数\",{\"1\":{\"221\":1}}],[\"目前可以看出研究和使用node的一个好处在于ode的理论优势\",{\"1\":{\"290\":1}}],[\"目标是找到一个小的扰动\",{\"1\":{\"221\":1}}],[\"目录\",{\"0\":{\"49\":1,\"163\":1,\"169\":1}}],[\"它在包含的某个区间上可微\",{\"1\":{\"302\":1}}],[\"它为我们提供了一种理解高维数据的方法\",{\"1\":{\"299\":1}}],[\"它提出了一个假设\",{\"1\":{\"299\":1}}],[\"它们会接收到相同的梯度更新\",{\"1\":{\"295\":1}}],[\"它们的行为会是对称的\",{\"1\":{\"295\":1}}],[\"它们并不准确相等\",{\"1\":{\"288\":1}}],[\"它度量了相邻轨迹随时间分离的速率\",{\"1\":{\"295\":1}}],[\"它将传统的有限层神经网络架构转变为参数共享的无限层神经网络架构\",{\"1\":{\"286\":1}}],[\"它通过构造一系列可逆映射\",{\"1\":{\"283\":1}}],[\"它假定微分方程有可能有误差\",{\"1\":{\"234\":1}}],[\"它的解\",{\"1\":{\"231\":1}}],[\"它控制了对抗性扰动的强度\",{\"1\":{\"221\":1}}],[\"它是一种生成对抗性样本的技术\",{\"1\":{\"221\":1}}],[\"旨在通过执行单步梯度更新来生成对抗性样本\",{\"1\":{\"221\":1}}],[\"损失增加最快的方向就是梯度的方向\",{\"1\":{\"221\":1}}],[\"损失函数的梯度指向了损失增加最快的方向\",{\"1\":{\"221\":1}}],[\"损失函数\",{\"1\":{\"221\":2}}],[\"施瓦茨不等式\",{\"1\":{\"221\":1}}],[\"加到输入\",{\"1\":{\"221\":1}}],[\"加密展示\",{\"1\":{\"49\":1,\"73\":1}}],[\"具体来说是沿着使损失最大化的方向\",{\"1\":{\"221\":1}}],[\"具体来说\",{\"1\":{\"221\":1}}],[\"具有\",{\"1\":{\"220\":1}}],[\"具有一阶\",{\"1\":{\"112\":1}}],[\"具有零阶接近度\",{\"1\":{\"112\":1}}],[\"梯度方向\",{\"1\":{\"221\":1}}],[\"梯度的数学定义告诉我们在多维空间中函数增长最快的方向\",{\"1\":{\"221\":1}}],[\"梯度流\",{\"1\":{\"113\":1}}],[\"衡量了网络预测\",{\"1\":{\"221\":1}}],[\"像素点\",{\"1\":{\"220\":1}}],[\"却会忽略那些权重大但不相关的振幅\",{\"1\":{\"220\":1}}],[\"却并不会因为维度的增加而增加\",{\"1\":{\"220\":1}}],[\"该篇论文提出了一个能供更简单与更快速的生成对抗样本的方法\",{\"1\":{\"219\":1}}],[\"该公式使用了\",{\"1\":{\"194\":1}}],[\"早期对对抗样本产生的原因的猜测集中于神经网络的非线性性和过拟合\",{\"1\":{\"219\":1}}],[\"jan\",{\"1\":{\"311\":1}}],[\"jonathan\",{\"1\":{\"311\":1}}],[\"just\",{\"1\":{\"218\":1}}],[\"jensen\",{\"1\":{\"194\":3}}],[\"攻击算法\",{\"1\":{\"218\":1}}],[\"攻击者不知道目标模型的内部细节\",{\"1\":{\"217\":1}}],[\"|\",{\"1\":{\"218\":2}}],[\"三\",{\"1\":{\"218\":1}}],[\"三条营的龙灯已经亮了\",{\"1\":{\"46\":1}}],[\"论文地址\",{\"1\":{\"218\":1}}],[\"论文解读\",{\"1\":{\"113\":1}}],[\"非线性系统的敏感性\",{\"1\":{\"295\":1}}],[\"非线性模型的线性扰动\",{\"0\":{\"221\":1}}],[\"非负性或稀释性等\",{\"1\":{\"242\":1}}],[\"非参数密度估计的方法有直方图\",{\"1\":{\"242\":1}}],[\"非参数密度估计是不假设数据服从某个已知分布\",{\"1\":{\"242\":1}}],[\"非定向攻击\",{\"1\":{\"217\":1}}],[\"非常详细\",{\"1\":{\"194\":1}}],[\"把输入分类误判到一个某个特定的错误类别上\",{\"1\":{\"217\":1}}],[\"黑箱攻击\",{\"1\":{\"217\":1}}],[\"结构以及概率分布等\",{\"1\":{\"242\":1}}],[\"结构以及参数\",{\"1\":{\"217\":1}}],[\"结束\",{\"0\":{\"41\":1},\"1\":{\"79\":1}}],[\"类型\",{\"1\":{\"217\":1}}],[\"类别\",{\"1\":{\"205\":1,\"242\":1}}],[\"金山文档\",{\"1\":{\"216\":1,\"224\":1}}],[\"金陵入夜看秦淮\",{\"1\":{\"46\":1}}],[\"让输入\",{\"1\":{\"225\":1}}],[\"让神经网络的分类准确率大幅下降\",{\"1\":{\"215\":1}}],[\"让我们确定每个条件概率所需的参数\",{\"1\":{\"196\":1}}],[\"~bonner\",{\"1\":{\"213\":1}}],[\"代表所有势能函数中的参数\",{\"1\":{\"210\":1}}],[\"代码随想录\",{\"1\":{\"142\":1}}],[\"代码演示\",{\"0\":{\"74\":1}}],[\"代码块\",{\"0\":{\"64\":1}}],[\"代码块会自动高亮\",{\"1\":{\"10\":1}}],[\"代码\",{\"0\":{\"34\":1,\"143\":1},\"1\":{\"63\":1,\"93\":1,\"176\":1}}],[\"势能函数一般定义为\",{\"1\":{\"210\":1}}],[\"受限玻尔兹曼机等\",{\"1\":{\"209\":1}}],[\"玻尔兹曼机\",{\"1\":{\"209\":1}}],[\"吉布斯分布一定满足马尔可夫随机场的条件独立性质\",{\"1\":{\"208\":1}}],[\"吉布斯分布\",{\"1\":{\"208\":1}}],[\"配分函数的计算复杂度是指数的\",{\"1\":{\"208\":1}}],[\"配置\",{\"0\":{\"59\":1}}],[\"用来对输入\",{\"1\":{\"300\":1}}],[\"用来提高模型的表达能力\",{\"1\":{\"298\":1}}],[\"用来做反向传播\",{\"1\":{\"296\":1}}],[\"用来将乘积归一化为概率形式\",{\"1\":{\"208\":1}}],[\"用大饼干优先满足胃口大的\",{\"1\":{\"142\":1}}],[\"包括有效的特征\",{\"1\":{\"242\":1}}],[\"包括模型的训练集\",{\"1\":{\"217\":1}}],[\"包括\",{\"1\":{\"208\":1}}],[\"包含\",{\"1\":{\"63\":1,\"93\":2,\"176\":2}}],[\"进行采样\",{\"1\":{\"285\":1}}],[\"进行比较\",{\"1\":{\"234\":1}}],[\"进行逐一分解\",{\"1\":{\"208\":1}}],[\"进行引用\",{\"1\":{\"87\":2}}],[\"团\",{\"1\":{\"208\":1}}],[\"外的其他变量\",{\"1\":{\"207\":1}}],[\"外其他变量的集合\",{\"1\":{\"207\":1}}],[\"外面的for\",{\"1\":{\"144\":1}}],[\"满足初始条仵\",{\"1\":{\"230\":1}}],[\"满足\",{\"1\":{\"230\":2,\"234\":1}}],[\"满足无向图\",{\"1\":{\"208\":1}}],[\"满足局部马尔可夫性质\",{\"1\":{\"207\":1}}],[\"满足不了\",{\"1\":{\"144\":1}}],[\"马尔可夫随机场\",{\"1\":{\"207\":1}}],[\"虽然node中的输入和输出是固定的维度\",{\"1\":{\"290\":1}}],[\"虽然可以使用ode求解器\",{\"1\":{\"290\":1}}],[\"虽然euler法的精确性不足以保证它在实际中的使用\",{\"1\":{\"235\":1}}],[\"虽然定义\",{\"1\":{\"234\":1}}],[\"虽然朴素贝叶斯分类器的条件独立性假设太强\",{\"1\":{\"205\":1}}],[\"虽然挂南京的名\",{\"1\":{\"44\":1}}],[\"朴素\",{\"1\":{\"205\":1}}],[\"朴素贝叶斯\",{\"1\":{\"205\":1}}],[\"朴素贝叶斯分类器在很多任务上也能得到很好的结果\",{\"1\":{\"205\":1}}],[\"朴素贝叶斯分类器\",{\"0\":{\"205\":1}}],[\"作为node的输出\",{\"1\":{\"294\":1}}],[\"作为微分方程的初值\",{\"1\":{\"294\":1}}],[\"作为一种确定性的参数\",{\"1\":{\"204\":1}}],[\"作者发现\",{\"1\":{\"221\":1}}],[\"作者在使用中\",{\"1\":{\"221\":1}}],[\"作者通过数学公式来解释\",{\"1\":{\"220\":1}}],[\"作者设置为\",{\"1\":{\"86\":1}}],[\"作者\",{\"1\":{\"3\":1}}],[\"回顾下一个resnet网络架构中的resnet块\",{\"1\":{\"290\":1}}],[\"回归模型\",{\"1\":{\"210\":1}}],[\"回归模型只建模条件概率\",{\"1\":{\"204\":1}}],[\"回归模型中的\",{\"1\":{\"204\":1}}],[\"回归模型类似\",{\"1\":{\"204\":1}}],[\"回归模型都采用\",{\"1\":{\"204\":1}}],[\"回到浮池码头\",{\"1\":{\"46\":1}}],[\"值感兴趣\",{\"1\":{\"234\":1}}],[\"值得一提的是\",{\"1\":{\"204\":1}}],[\"值取决于函数\",{\"1\":{\"113\":1}}],[\"信念网络建模联合概率\",{\"1\":{\"204\":1}}],[\"信念网络的网络结构和\",{\"1\":{\"204\":1}}],[\"信念网络中只有一个叶子节点\",{\"1\":{\"204\":1}}],[\"信念网络与\",{\"1\":{\"204\":1}}],[\"信念网络\",{\"1\":{\"204\":1}}],[\"信息容器\",{\"0\":{\"93\":1,\"176\":1},\"1\":{\"63\":1,\"93\":2,\"176\":2}}],[\"隐马尔可夫模型的联合概率可以分解为\",{\"1\":{\"206\":1}}],[\"隐马尔可夫模型\",{\"0\":{\"206\":1},\"1\":{\"203\":1,\"206\":1}}],[\"隐变量\",{\"1\":{\"166\":1}}],[\"条件\",{\"1\":{\"234\":1,\"235\":1}}],[\"条件随机场\",{\"0\":{\"211\":1},\"1\":{\"209\":1}}],[\"条件概率\",{\"1\":{\"206\":2}}],[\"条件概率分布\",{\"1\":{\"205\":1}}],[\"条件独立于它的非后代节点\",{\"1\":{\"202\":1}}],[\"条件分布\",{\"1\":{\"195\":1}}],[\"反而不独立了\",{\"1\":{\"202\":1}}],[\"路径是阻塞的\",{\"1\":{\"202\":1}}],[\"路径导航\",{\"1\":{\"51\":1,\"88\":1}}],[\"默认情况下\",{\"1\":{\"202\":1}}],[\"间接因果关系\",{\"1\":{\"202\":1}}],[\"被观测\",{\"1\":{\"202\":2}}],[\"被积函数\",{\"1\":{\"113\":1}}],[\"共果关系\",{\"1\":{\"202\":1}}],[\"共因关系\",{\"1\":{\"202\":1}}],[\"共需要\",{\"1\":{\"196\":1}}],[\"贝叶斯网络\",{\"1\":{\"202\":1}}],[\"边表示这些随机变量之间的概率依赖关系\",{\"1\":{\"201\":1}}],[\"边际似然也称为证据\",{\"1\":{\"194\":1}}],[\"哔哩哔哩\",{\"1\":{\"200\":1}}],[\"九\",{\"1\":{\"200\":2}}],[\"资料\",{\"0\":{\"200\":1}}],[\"模型的解释性差等\",{\"1\":{\"300\":1}}],[\"模型的参数设为\",{\"1\":{\"221\":1}}],[\"模型可以对数据样本进行精确的最大似然估计\",{\"1\":{\"283\":1}}],[\"模型\",{\"1\":{\"242\":1}}],[\"模型选择问题\",{\"1\":{\"241\":1}}],[\"模型表示\",{\"0\":{\"201\":1},\"1\":{\"199\":1}}],[\"模拟法\",{\"0\":{\"154\":1}}],[\"推断问题\",{\"1\":{\"197\":1}}],[\"推导\",{\"1\":{\"109\":1}}],[\"赖关系\",{\"1\":{\"197\":1}}],[\"每一组都可以单独求解\",{\"1\":{\"300\":1}}],[\"每一层神经元数量为\",{\"1\":{\"225\":1}}],[\"每层参数化\",{\"1\":{\"290\":1}}],[\"每条边代表两个变量之间有概率依赖关系\",{\"1\":{\"201\":1}}],[\"每条连边表示变量之间的依赖关系\",{\"1\":{\"196\":1}}],[\"每个元素的扰动都受到限制\",{\"1\":{\"221\":1}}],[\"每个可观测标量\",{\"1\":{\"206\":1}}],[\"每个随机变量在给定父节点的情况下\",{\"1\":{\"202\":1}}],[\"每个连接\",{\"1\":{\"202\":1}}],[\"每个节点都表示一个随机变量\",{\"1\":{\"201\":1}}],[\"每个局部条件概率的参数是独立的\",{\"1\":{\"192\":1}}],[\"总有一个\",{\"1\":{\"230\":1}}],[\"总结出的规律\",{\"1\":{\"202\":1}}],[\"总共需要9个参数来描述这个系统的联合概率分布\",{\"1\":{\"196\":1}}],[\"总可以找到一个\",{\"1\":{\"111\":1}}],[\"现在讨论二阶微分方程组\",{\"1\":{\"231\":1}}],[\"现在\",{\"1\":{\"196\":1,\"221\":1}}],[\"例如添加一层全连接层或者非线性层\",{\"1\":{\"290\":1}}],[\"例如一个1080p像素点有个\",{\"1\":{\"290\":1}}],[\"例如存在连续偏导数\",{\"1\":{\"231\":1}}],[\"例如它们假定的高度非线性性质\",{\"1\":{\"220\":1}}],[\"例如\",{\"1\":{\"196\":1,\"290\":1,\"296\":1}}],[\"例如图\",{\"1\":{\"111\":1}}],[\"首先给出node的定义\",{\"1\":{\"290\":1}}],[\"首先\",{\"1\":{\"196\":1}}],[\"首先根据多项分布\",{\"1\":{\"195\":1}}],[\"独立性假设的条件下运用贝叶斯公式来计算每个类别的条件概率\",{\"1\":{\"205\":1}}],[\"独立\",{\"1\":{\"196\":2,\"202\":4}}],[\"远远超出了目前计算机的存储能力\",{\"1\":{\"196\":1}}],[\"ul\",{\"1\":{\"242\":1}}],[\"unaugmented\",{\"1\":{\"294\":1}}],[\"unsupervised\",{\"1\":{\"242\":2}}],[\"undirected\",{\"1\":{\"201\":1}}],[\"until\",{\"1\":{\"195\":1}}],[\"up\",{\"1\":{\"21\":1}}],[\"​设\",{\"1\":{\"300\":1}}],[\"​\",{\"1\":{\"195\":1,\"237\":1,\"299\":6,\"300\":15}}],[\"训练过程不可控\",{\"1\":{\"300\":1}}],[\"训练成本过高\",{\"1\":{\"300\":1}}],[\"训练方式有两种\",{\"1\":{\"296\":1}}],[\"训练方法\",{\"1\":{\"286\":1}}],[\"训练思路都是丰富且先进\",{\"1\":{\"290\":1}}],[\"训练样本\",{\"1\":{\"195\":1}}],[\"训练集\",{\"1\":{\"195\":1}}],[\"利用拉格朗日乘数法来求解上面的等式约束优化问题\",{\"1\":{\"195\":1}}],[\"利用采样来近似计算这个期望\",{\"1\":{\"192\":1}}],[\"将终止时间也作为了一个参数\",{\"1\":{\"298\":1}}],[\"将输入的维度增加一倍\",{\"1\":{\"293\":1}}],[\"将区间分割为为单位\",{\"1\":{\"290\":1}}],[\"将将任意复杂的数据分布变换为一个基本的简单分布\",{\"1\":{\"283\":1}}],[\"将原问题转换为无约束优化问题\",{\"1\":{\"241\":1}}],[\"将参数估计问题转为优化问题\",{\"1\":{\"195\":1}}],[\"将两点之间的\",{\"1\":{\"109\":1}}],[\"属于第\",{\"1\":{\"195\":1}}],[\"属性支持\",{\"0\":{\"101\":1,\"183\":1}}],[\"属性局部设置\",{\"1\":{\"29\":1}}],[\"属性改变元素的动画顺序\",{\"1\":{\"27\":1}}],[\"属性自定义幻灯片背景\",{\"1\":{\"17\":1}}],[\"希望能学习其中的参数\",{\"1\":{\"195\":1}}],[\"希望通过一些观测样本来估计其分布\",{\"1\":{\"166\":1}}],[\"服从正态分布\",{\"1\":{\"240\":1}}],[\"服从多项分布\",{\"1\":{\"195\":1}}],[\"服从一个未知的数据分布\",{\"1\":{\"167\":1}}],[\"高斯混合模型的参数学习过程\",{\"1\":{\"195\":1}}],[\"高斯混合模型\",{\"0\":{\"195\":1},\"1\":{\"195\":3}}],[\"高维随机向量一般比较难以直接建模\",{\"1\":{\"167\":1}}],[\"找到一组参数使得证据下界最大\",{\"1\":{\"194\":1}}],[\"找到一个分布\",{\"1\":{\"194\":1}}],[\"固定\",{\"1\":{\"194\":1,\"195\":1}}],[\"固定参数\",{\"1\":{\"194\":1,\"195\":1}}],[\"否则为非自治的\",{\"1\":{\"302\":1}}],[\"否则求解器会在分段点减速再缓慢返回\",{\"1\":{\"296\":1}}],[\"否则\",{\"1\":{\"194\":1}}],[\"否则这个求和难以直接计算\",{\"1\":{\"194\":1}}],[\"问题存在一个唯一的解\",{\"1\":{\"234\":1}}],[\"问题\",{\"1\":{\"194\":1}}],[\"问题就是要找到一个路径函数\",{\"1\":{\"109\":1}}],[\"直到收玫到某个局部最优解\",{\"1\":{\"194\":1}}],[\"直接加入\",{\"1\":{\"154\":1}}],[\"步对问题\",{\"1\":{\"236\":1}}],[\"步运算之后误差的大小\",{\"1\":{\"234\":1}}],[\"步分别为\",{\"1\":{\"194\":1}}],[\"步更新时\",{\"1\":{\"194\":1}}],[\"步\",{\"1\":{\"194\":2,\"195\":2}}],[\"步和\",{\"1\":{\"194\":2}}],[\"再\",{\"1\":{\"293\":2}}],[\"再从高斯分布\",{\"1\":{\"195\":1}}],[\"再寻找参数\",{\"1\":{\"194\":1}}],[\"再次点击即可缩小\",{\"1\":{\"40\":1}}],[\"时满足初始条件\",{\"1\":{\"230\":1}}],[\"时的错误程度\",{\"1\":{\"221\":1}}],[\"时\",{\"1\":{\"194\":2,\"196\":3,\"202\":1,\"230\":2,\"234\":1}}],[\"时间复杂度\",{\"1\":{\"143\":1}}],[\"除非\",{\"1\":{\"194\":1}}],[\"然而\",{\"1\":{\"221\":1}}],[\"然而计算边际似然函数时涉及\",{\"1\":{\"194\":1}}],[\"然后根据连续和双射的证明得出\",{\"1\":{\"299\":1}}],[\"然后根据训练样本去估计概率密度函数的参数\",{\"1\":{\"242\":1}}],[\"然后的到\",{\"1\":{\"294\":1}}],[\"然后在额外的维度上补0\",{\"1\":{\"293\":1}}],[\"然后介绍针对node的一种全新的反向传播方法\",{\"1\":{\"286\":1}}],[\"然后介绍两种深度生成模型\",{\"1\":{\"167\":1}}],[\"然后通过训练样本来估计分布的参数\",{\"1\":{\"239\":1}}],[\"然后应用符号函数和扰动系数\",{\"1\":{\"221\":1}}],[\"然后从后向前遍历小孩数组\",{\"1\":{\"142\":1}}],[\"整个训练集的对数边际似然为\",{\"1\":{\"194\":1}}],[\"带隐变量的贝叶斯网络\",{\"1\":{\"194\":1}}],[\"次\",{\"1\":{\"194\":1}}],[\"令\",{\"1\":{\"194\":1,\"195\":1,\"202\":1,\"239\":1,\"241\":1,\"298\":1,\"299\":1,\"300\":1}}],[\"期望最大\",{\"1\":{\"194\":1}}],[\"含隐变量的参数估计\",{\"0\":{\"193\":1}}],[\"含绝美机位\",{\"1\":{\"46\":1}}],[\"往往很难计算\",{\"1\":{\"192\":1}}],[\"公式\",{\"1\":{\"192\":1,\"195\":1,\"208\":1}}],[\"定向攻击\",{\"1\":{\"217\":1}}],[\"定义函数\",{\"1\":{\"299\":1}}],[\"定义两个线性函数\",{\"1\":{\"226\":1}}],[\"定义\",{\"0\":{\"290\":1},\"1\":{\"202\":1,\"207\":1,\"230\":1,\"234\":2,\"236\":1,\"286\":1,\"290\":1,\"298\":3,\"302\":3}}],[\"定义了样本\",{\"1\":{\"195\":1}}],[\"定义隐变量集合\",{\"1\":{\"194\":1}}],[\"定义可观测变量集合\",{\"1\":{\"194\":1}}],[\"定义为经验分布\",{\"1\":{\"192\":1}}],[\"定理从和定理虽然从理论上证明通过增维可以让模型具有通用逼近的能力\",{\"1\":{\"300\":1}}],[\"定理3\",{\"1\":{\"300\":1}}],[\"定理2\",{\"1\":{\"300\":1}}],[\"定理1\",{\"1\":{\"299\":1}}],[\"定理得\",{\"1\":{\"235\":1}}],[\"定理中涉及的符号\",{\"0\":{\"227\":1}}],[\"定理的证明可以参考\",{\"1\":{\"208\":1}}],[\"定理\",{\"1\":{\"113\":1,\"208\":1,\"234\":1,\"235\":1,\"299\":1,\"300\":1}}],[\"关于这种形式的微分方程\",{\"1\":{\"296\":1}}],[\"关于这些扩展\",{\"1\":{\"61\":1}}],[\"关于输入\",{\"1\":{\"221\":1}}],[\"关于\",{\"1\":{\"195\":1,\"290\":1}}],[\"关于参数\",{\"1\":{\"192\":1}}],[\"还有训练时间指数级增加等\",{\"1\":{\"300\":1}}],[\"还有一些非常关键的变量是无法观测的\",{\"1\":{\"241\":1}}],[\"还可以通过让所有的条件概率分布共享使用同一组参数来进一步减少参数的数量\",{\"1\":{\"191\":1}}],[\"还不会编写\",{\"1\":{\"58\":1}}],[\"简而言之\",{\"1\":{\"299\":1}}],[\"简介\",{\"0\":{\"219\":1}}],[\"简单的方式描述随机变量之间的条件独立性\",{\"1\":{\"196\":1}}],[\"简单直接的方式是在训练集上统计每个变量的条件概率表\",{\"1\":{\"191\":1}}],[\"简称neuralode\",{\"1\":{\"290\":1}}],[\"简称密度估计\",{\"1\":{\"238\":1,\"242\":1}}],[\"简称\",{\"1\":{\"221\":1,\"286\":1}}],[\"简称图模型\",{\"1\":{\"196\":1}}],[\"简称生成模型\",{\"1\":{\"167\":1}}],[\"简称最简泛函\",{\"1\":{\"113\":1}}],[\"𝒙\",{\"1\":{\"191\":1}}],[\"二是网络参数估计\",{\"1\":{\"189\":1}}],[\"参数共享\",{\"1\":{\"300\":1}}],[\"参数量随着层数的增加而增加\",{\"1\":{\"290\":1}}],[\"参数量约为\",{\"1\":{\"196\":1}}],[\"参数密度估计是假设数据服从某个已知概率密度函数形式的分布\",{\"1\":{\"242\":1}}],[\"参数密度估计一般存在以下问题\",{\"1\":{\"241\":1}}],[\"参数密度估计\",{\"0\":{\"239\":1},\"1\":{\"239\":1}}],[\"参数密度估计和非参数密度估计\",{\"1\":{\"238\":1}}],[\"参数估计可以分为两步进行迭代\",{\"1\":{\"195\":1}}],[\"参数估计\",{\"1\":{\"195\":1}}],[\"参数学习\",{\"0\":{\"189\":1}}],[\"参考\",{\"0\":{\"213\":1},\"1\":{\"46\":1}}],[\"参考📍南京｜老门东最强攻略\",{\"1\":{\"46\":1}}],[\"连续函数介质定理\",{\"1\":{\"299\":1}}],[\"连续\",{\"1\":{\"290\":1}}],[\"连续归一化流\",{\"0\":{\"283\":1},\"1\":{\"187\":1}}],[\"连边表示两变量间的条件依赖关系\",{\"1\":{\"201\":1}}],[\"连接的两点之间的距离可以通过积分来表示\",{\"1\":{\"109\":1}}],[\"神经常微分方程\",{\"0\":{\"286\":1},\"1\":{\"187\":1,\"286\":1}}],[\"神经微分方程稳定性概念\",{\"1\":{\"291\":1}}],[\"神经微分方程的稳定性\",{\"0\":{\"291\":1}}],[\"神经微分方程\",{\"0\":{\"187\":1},\"2\":{\"289\":1,\"301\":1}}],[\"绪论\",{\"0\":{\"188\":1},\"1\":{\"187\":1}}],[\"容器\",{\"0\":{\"175\":1}}],[\"等人在\",{\"1\":{\"221\":1}}],[\"等号成立当且仅当\",{\"1\":{\"221\":1}}],[\"等于\",{\"1\":{\"194\":1}}],[\"等于零\",{\"1\":{\"113\":1}}],[\"等来表示\",{\"1\":{\"172\":1}}],[\"维度为\",{\"1\":{\"300\":1}}],[\"维度灾难问题\",{\"1\":{\"241\":1}}],[\"维的结构\",{\"1\":{\"294\":1}}],[\"维特征的样本\",{\"1\":{\"205\":1}}],[\"维随机向量\",{\"1\":{\"196\":2,\"202\":1}}],[\"维\",{\"1\":{\"172\":1,\"220\":1}}],[\"维向量\",{\"1\":{\"172\":1}}],[\"个状态的样本数量\",{\"1\":{\"241\":1}}],[\"个状态的概率\",{\"1\":{\"241\":1}}],[\"个状态的多项分布\",{\"1\":{\"241\":1}}],[\"个状态\",{\"1\":{\"241\":1}}],[\"个神经元\",{\"1\":{\"225\":1}}],[\"个团就是一个最大团\",{\"1\":{\"208\":1}}],[\"个团\",{\"1\":{\"208\":1}}],[\"个节点的无向图\",{\"1\":{\"207\":1}}],[\"个节点的有向非循环图\",{\"1\":{\"202\":1}}],[\"个变量\",{\"1\":{\"207\":1}}],[\"个变量之间的条件独立性的图形化描述\",{\"1\":{\"196\":1}}],[\"个变量的局部条件概率的参数\",{\"1\":{\"191\":1}}],[\"个独立参数\",{\"1\":{\"196\":1}}],[\"个条件概率的话\",{\"1\":{\"196\":1}}],[\"个条件概率的乘积\",{\"1\":{\"196\":1}}],[\"个表格来记录这\",{\"1\":{\"196\":1}}],[\"个局部条件概率的乘积\",{\"1\":{\"196\":1}}],[\"个取值\",{\"1\":{\"196\":1}}],[\"个由高斯混合模型生成的训练样本\",{\"1\":{\"195\":1}}],[\"个高斯分布的后验概率\",{\"1\":{\"195\":1}}],[\"个高斯分布的均值和方差\",{\"1\":{\"195\":1}}],[\"个高斯分布\",{\"1\":{\"195\":1}}],[\"个高斯分布生成的概率\",{\"1\":{\"195\":1}}],[\"个高斯分布中的一个分布生成的\",{\"1\":{\"195\":1}}],[\"个参数才能表示其概率分布\",{\"1\":{\"196\":1}}],[\"个参数\",{\"1\":{\"191\":1,\"196\":1,\"204\":2}}],[\"个训练样本\",{\"1\":{\"191\":1,\"192\":1,\"194\":1,\"239\":1}}],[\"个分量\",{\"1\":{\"172\":1}}],[\"个有序实数组成\",{\"1\":{\"172\":1}}],[\"个人介绍\",{\"0\":{\"1\":1}}],[\"没有方向\",{\"1\":{\"172\":1}}],[\"只利用训练样本对密度进行估计\",{\"1\":{\"242\":1}}],[\"只不过更复杂一些\",{\"1\":{\"235\":1}}],[\"只能够观察目标模型对输入样本的输出结果\",{\"1\":{\"217\":1}}],[\"只需要生成对抗样本\",{\"1\":{\"217\":1}}],[\"只需要知道的一个值的条件概率\",{\"1\":{\"196\":1}}],[\"只需要1个参数\",{\"1\":{\"196\":1}}],[\"只需要\",{\"1\":{\"196\":1}}],[\"只需要分别最大化每个变量的条件似然来估计其参数\",{\"1\":{\"191\":1}}],[\"只有大小\",{\"1\":{\"172\":1}}],[\"只要在最理想的训练情况下\",{\"1\":{\"300\":1}}],[\"只要\",{\"1\":{\"111\":1}}],[\"优化算法\",{\"0\":{\"287\":1},\"1\":{\"286\":1}}],[\"优化\",{\"1\":{\"169\":1}}],[\"优化版本\",{\"1\":{\"148\":1}}],[\"需要假设最少的解释往往是最接近真相解释\",{\"1\":{\"302\":1}}],[\"需要两个计算引理\",{\"1\":{\"235\":1}}],[\"需要4个参数\",{\"1\":{\"196\":1}}],[\"需要2个参数\",{\"1\":{\"196\":1}}],[\"需要通过变分推断的方法来进行近似估计\",{\"1\":{\"194\":1}}],[\"需要通过一些条件独立性来简化模型\",{\"1\":{\"167\":1}}],[\"需要在对数函数的内部进行求和\",{\"1\":{\"194\":1}}],[\"需要估计条件分布\",{\"1\":{\"166\":1}}],[\"原因在于一阶标准方程具有良好表示能力\",{\"1\":{\"302\":1}}],[\"原因在于\",{\"1\":{\"167\":1}}],[\"声音等\",{\"1\":{\"167\":1}}],[\"右图表示估计出了分布\",{\"1\":{\"167\":1}}],[\"样本\",{\"1\":{\"167\":2,\"194\":1,\"241\":1}}],[\"样式化\",{\"0\":{\"75\":1}}],[\"概率密度估计\",{\"0\":{\"238\":1},\"1\":{\"238\":1,\"242\":1}}],[\"概率密度估计和生成样本\",{\"1\":{\"167\":1}}],[\"概率图模型基础笔记\",{\"1\":{\"200\":1}}],[\"概率图模型基础\",{\"1\":{\"200\":1}}],[\"概率图模型\",{\"1\":{\"196\":1}}],[\"概率图\",{\"0\":{\"196\":1}}],[\"概率论\",{\"1\":{\"169\":1}}],[\"概率生成模型\",{\"1\":{\"167\":1}}],[\"真实\",{\"1\":{\"167\":2}}],[\"生成的对抗性样本可以用来测试模型的鲁棒性或用于对抗性训练\",{\"1\":{\"221\":1}}],[\"生成\",{\"1\":{\"167\":2}}],[\"生成模型的应用十分广泛\",{\"1\":{\"167\":1}}],[\"生成模型的两个功能\",{\"1\":{\"167\":1}}],[\"生成模型的功能\",{\"0\":{\"165\":1}}],[\"生成模型通常包含两个基本功能\",{\"1\":{\"167\":1}}],[\"生成模型是根据一些可观测的样本\",{\"1\":{\"167\":1}}],[\"生成模型一般具有两个基本功能\",{\"1\":{\"165\":1}}],[\"指示了哪个方向的微小变化会导致损失最大的增加\",{\"1\":{\"221\":1}}],[\"指一系列用于随机生成可观测数据的模型\",{\"1\":{\"167\":1}}],[\"指向函数值增长最快的方向\",{\"1\":{\"221\":1}}],[\"指向\",{\"1\":{\"144\":1}}],[\"深度信念网络等\",{\"1\":{\"203\":1}}],[\"深度生成模型就是利用深度神经网络可以近似任意函数的能力来建模一个复杂分布\",{\"1\":{\"167\":1}}],[\"深度生成模型\",{\"0\":{\"167\":1}}],[\"深度学习中的神经网络在精心训练后\",{\"1\":{\"215\":1}}],[\"深度学习\",{\"0\":{\"108\":1},\"2\":{\"128\":1}}],[\"算法具体分为两个步骤\",{\"1\":{\"194\":1}}],[\"算法是含隐变量图模型的常用参数估计方法\",{\"1\":{\"194\":1}}],[\"算法\",{\"1\":{\"194\":1,\"195\":1}}],[\"算法进行参数估计\",{\"1\":{\"193\":1}}],[\"算法中\",{\"1\":{\"166\":1}}],[\"算法来进行密度估计\",{\"1\":{\"166\":1}}],[\"量条件独立\",{\"1\":{\"201\":1}}],[\"量\",{\"1\":{\"166\":1}}],[\"比较困难\",{\"1\":{\"166\":1}}],[\"比如全连接神经网络\",{\"1\":{\"290\":1}}],[\"比如独立性\",{\"1\":{\"242\":1}}],[\"比如最大似然估计\",{\"1\":{\"242\":1}}],[\"比如高斯分布\",{\"1\":{\"242\":1}}],[\"比如线性判别分析等\",{\"1\":{\"242\":1}}],[\"比如使x在梯度方向上旋转一定的角度\",{\"1\":{\"221\":1}}],[\"比如sigmoid网络\",{\"1\":{\"221\":1}}],[\"比如sigmoid信念网络\",{\"1\":{\"191\":1}}],[\"比如对数线性模型\",{\"1\":{\"209\":1}}],[\"比如朴素贝叶斯分类器\",{\"1\":{\"203\":1}}],[\"比如了解不同机器学习模型之间的联系\",{\"1\":{\"198\":1}}],[\"比如0或1\",{\"1\":{\"196\":1}}],[\"比如混合高斯模型\",{\"1\":{\"194\":1}}],[\"比如图像\",{\"1\":{\"167\":1}}],[\"比如图中的蓝色曲线\",{\"1\":{\"109\":1}}],[\"比如相邻像素的颜色一般是相似的\",{\"1\":{\"166\":1}}],[\"比如在手写体数字图像的密度估计问题中\",{\"1\":{\"166\":1}}],[\"比如此例中\",{\"1\":{\"109\":1}}],[\"比如说对于下图\",{\"1\":{\"109\":1}}],[\"很多经典的机器学习模型可以使用无向图模型来描述\",{\"1\":{\"209\":1}}],[\"很多经典的机器学习模型可以使用有向图模型来描述\",{\"1\":{\"203\":1}}],[\"很多机器学习模型都可以归结为概率模型\",{\"1\":{\"198\":1}}],[\"很难用一个明确的图模型来描述其依赖关系\",{\"1\":{\"166\":1}}],[\"很小时\",{\"1\":{\"109\":1}}],[\"很小\",{\"1\":{\"109\":1}}],[\"手写体数字图像中不同像素之间存在复杂的依赖关系\",{\"1\":{\"166\":1}}],[\"假设存在微分方程有两个解和满足\",{\"1\":{\"299\":1}}],[\"假设是层神经网络\",{\"1\":{\"290\":1}}],[\"假设全局\",{\"1\":{\"290\":1}}],[\"假设这些样本服从一个概率分布函数\",{\"1\":{\"239\":1}}],[\"假设\",{\"1\":{\"234\":2,\"235\":1,\"290\":1}}],[\"假设方程右端的函数满足解的存在唯一性和连续性定理的条件\",{\"1\":{\"231\":1}}],[\"假设神经网络一共有\",{\"1\":{\"225\":1}}],[\"假设我们有一个小的扰动\",{\"1\":{\"221\":1}}],[\"假设变量\",{\"1\":{\"204\":1}}],[\"假设在给定\",{\"1\":{\"205\":1}}],[\"假设在已知\",{\"1\":{\"196\":1}}],[\"假设在一个连续或离散的高维空间\",{\"1\":{\"167\":1}}],[\"假设有四个二值变量\",{\"1\":{\"196\":1}}],[\"假设每个变量为离散变量并有\",{\"1\":{\"196\":1}}],[\"假设选中第\",{\"1\":{\"195\":1}}],[\"假设样本服从\",{\"1\":{\"241\":1}}],[\"假设样本\",{\"1\":{\"195\":1,\"240\":1}}],[\"假设条件概率\",{\"1\":{\"191\":1}}],[\"假设隐变量\",{\"1\":{\"166\":1}}],[\"假设手写体数字图像都服从一个未知的分布\",{\"1\":{\"166\":1}}],[\"假设它们都是独立地从相同的概率密度函数为\",{\"1\":{\"166\":1}}],[\"来提高训练的高效性\",{\"1\":{\"300\":1}}],[\"来存储前向计算中的\",{\"1\":{\"296\":1}}],[\"来寻找参数\",{\"1\":{\"239\":1}}],[\"来实现这一点\",{\"1\":{\"221\":1}}],[\"来描述变量之间的关系\",{\"1\":{\"201\":2}}],[\"来描述这个条件概率\",{\"1\":{\"196\":1}}],[\"来自于哪个高斯分布\",{\"1\":{\"195\":1}}],[\"来优化一个势能函数的参数\",{\"1\":{\"192\":1}}],[\"来表示第\",{\"1\":{\"241\":1}}],[\"来表示其联合概率\",{\"1\":{\"208\":1}}],[\"来表示样本\",{\"1\":{\"195\":1}}],[\"来表示\",{\"1\":{\"172\":1}}],[\"来近似未知分布\",{\"1\":{\"167\":1}}],[\"来学习一个参数化的模型\",{\"1\":{\"167\":1}}],[\"来简化模型\",{\"1\":{\"166\":1}}],[\"来估计其概率密度函数\",{\"1\":{\"166\":1}}],[\"来解析\",{\"1\":{\"60\":1}}],[\"密度估计可以分为参数密度估计和非参数密度估计\",{\"1\":{\"242\":1}}],[\"密度估计方法可以分为两类\",{\"1\":{\"238\":1}}],[\"密度估计在数据建模\",{\"1\":{\"238\":1}}],[\"密度估计的重点是估计条件分布\",{\"1\":{\"166\":1}}],[\"密度估计是一类无监督学习问题\",{\"1\":{\"166\":1}}],[\"密度估计\",{\"0\":{\"166\":1},\"1\":{\"166\":1}}],[\"密度估计和生成样本通常都不容易实现\",{\"1\":{\"167\":1}}],[\"密度估计和生成样本\",{\"1\":{\"165\":1}}],[\"密码加密的文章\",{\"0\":{\"54\":1}}],[\"字符串\",{\"0\":{\"164\":1},\"1\":{\"163\":1}}],[\"剑指\",{\"1\":{\"161\":1}}],[\"扑克牌中的顺子\",{\"0\":{\"161\":1},\"1\":{\"161\":1}}],[\"旋转图像\",{\"0\":{\"159\":1}}],[\"旋转数组\",{\"0\":{\"149\":1}}],[\"螺旋矩阵\",{\"0\":{\"157\":1,\"158\":1}}],[\"矩阵置零\",{\"0\":{\"160\":1}}],[\"矩阵\",{\"0\":{\"156\":1}}],[\"环形子数组的最大和\",{\"1\":{\"155\":1}}],[\"环形数组\",{\"0\":{\"155\":1}}],[\"开始合并\",{\"1\":{\"154\":1}}],[\"开放时间\",{\"1\":{\"46\":2}}],[\"6给出隐马尔可夫模型的图模型表示\",{\"1\":{\"206\":1}}],[\"65\",{\"1\":{\"195\":1}}],[\"64\",{\"1\":{\"195\":1}}],[\"63\",{\"1\":{\"195\":1}}],[\"61\",{\"1\":{\"161\":1}}],[\"6\",{\"1\":{\"153\":4,\"213\":1,\"221\":2,\"230\":3,\"234\":1}}],[\"无监督特征学习一般用来进行降维\",{\"1\":{\"242\":1}}],[\"无监督特征学习\",{\"1\":{\"242\":1}}],[\"无监督学习的准则非常多\",{\"1\":{\"242\":1}}],[\"无监督学习方法也包含三个基本要素\",{\"1\":{\"242\":1}}],[\"无监督学习算法一般直接从原始数据中学习\",{\"1\":{\"242\":1}}],[\"无监督学习\",{\"0\":{\"242\":1},\"1\":{\"242\":1}}],[\"无穷范数\",{\"1\":{\"221\":1}}],[\"无向图上定义的概率分布可以表示为\",{\"1\":{\"208\":1}}],[\"无向图中的联合概率可以分解为一系列定义在最大团上的非负函数的乘积形式\",{\"1\":{\"208\":1}}],[\"无向图中的一个全连通子图\",{\"1\":{\"208\":1}}],[\"无向图的参数估计通常采用近似的方法\",{\"1\":{\"192\":1}}],[\"无向图模型和吉布斯分布是一致的\",{\"1\":{\"208\":1}}],[\"无向图模型与有向图模型的一个重要区别是有配分函数𝑍\",{\"1\":{\"208\":1}}],[\"无向图模型的联合概率一般以全连通子图为单位进行分解\",{\"1\":{\"208\":1}}],[\"无向图模型的概率分解\",{\"0\":{\"208\":1}}],[\"无向图模型的参数估计要比有向图更为复杂\",{\"1\":{\"192\":1}}],[\"无向图模型使用无向图\",{\"1\":{\"201\":1}}],[\"无向图模型\",{\"0\":{\"192\":1,\"207\":1},\"1\":{\"207\":1}}],[\"无法分解\",{\"1\":{\"192\":1}}],[\"无重叠的\",{\"1\":{\"153\":1}}],[\"无论看展览\",{\"1\":{\"44\":1}}],[\"给定\",{\"1\":{\"191\":1,\"192\":1,\"194\":1,\"195\":2}}],[\"给定一个有\",{\"1\":{\"205\":1}}],[\"给定一个已建模的复杂分布\",{\"1\":{\"167\":1}}],[\"给定一个非负整数数组\",{\"1\":{\"148\":1}}],[\"给定一组数据\",{\"1\":{\"166\":1,\"195\":1}}],[\"给你一个\",{\"1\":{\"153\":1}}],[\"插入区间\",{\"0\":{\"153\":1}}],[\"插件来实现\",{\"1\":{\"60\":1}}],[\"插件\",{\"1\":{\"34\":1}}],[\"插件后\",{\"1\":{\"10\":1,\"11\":1}}],[\"koutn´ık\",{\"1\":{\"311\":1}}],[\"koller\",{\"1\":{\"213\":1}}],[\"kutta法\",{\"0\":{\"237\":1}}],[\"kdocs\",{\"1\":{\"216\":1}}],[\"k=k\",{\"1\":{\"151\":1}}],[\"k\",{\"1\":{\"150\":1,\"151\":1}}],[\"kexue\",{\"1\":{\"113\":2}}],[\"轮转数组\",{\"1\":{\"149\":1,\"155\":1}}],[\"奇数下标\",{\"1\":{\"148\":1}}],[\"偶数下标\",{\"1\":{\"148\":1}}],[\"输出之间的映射关系\",{\"1\":{\"242\":1}}],[\"输出\",{\"1\":{\"148\":1,\"153\":5,\"195\":1}}],[\"输入图像通常都是8bits的\",{\"1\":{\"220\":1}}],[\"输入\",{\"1\":{\"148\":1,\"153\":5,\"225\":1}}],[\"7\",{\"1\":{\"148\":5,\"153\":6,\"208\":1,\"235\":1,\"290\":1}}],[\"4所示\",{\"1\":{\"204\":1}}],[\"4\",{\"0\":{\"258\":1,\"278\":1},\"1\":{\"148\":5,\"153\":3,\"196\":4,\"221\":1}}],[\"455\",{\"1\":{\"141\":1}}],[\"示例\",{\"1\":{\"148\":1,\"153\":5}}],[\"90065\",{\"1\":{\"213\":1}}],[\"92\",{\"1\":{\"213\":1}}],[\"922\",{\"1\":{\"148\":1}}],[\"9\",{\"1\":{\"153\":2,\"221\":1,\"235\":1}}],[\"99\",{\"1\":{\"77\":1}}],[\"按照区间起始端点排序的区间列表\",{\"1\":{\"153\":1}}],[\"按奇偶排序数组\",{\"1\":{\"148\":1}}],[\"按奇偶排序数组ii\",{\"0\":{\"148\":1}}],[\"按下\",{\"1\":{\"36\":1,\"38\":1,\"40\":1}}],[\"寻找数组的中心下标\",{\"0\":{\"147\":1}}],[\"里面的if控制饼干\",{\"1\":{\"144\":1}}],[\"里的\",{\"1\":{\"144\":1}}],[\"走不到s\",{\"1\":{\"144\":1}}],[\"走一走\",{\"1\":{\"45\":1}}],[\"持续向前移动\",{\"1\":{\"144\":2}}],[\"胃口10\",{\"1\":{\"144\":1}}],[\"胃口\",{\"1\":{\"144\":2,\"145\":1}}],[\"控制\",{\"1\":{\"144\":1}}],[\"控制胃口\",{\"1\":{\"144\":1}}],[\"控制的是饼干\",{\"1\":{\"144\":1}}],[\"饼干9\",{\"1\":{\"144\":1}}],[\"饼干\",{\"1\":{\"144\":1,\"145\":1}}],[\"饼干数组的下标\",{\"1\":{\"143\":1}}],[\"先前对对抗样本的解释引用了了神经网络的假设特性\",{\"1\":{\"220\":1}}],[\"先固定参数\",{\"1\":{\"195\":1}}],[\"先找到近似分布\",{\"1\":{\"194\":1}}],[\"先验分布\",{\"1\":{\"166\":1}}],[\"先遍历\",{\"1\":{\"144\":1}}],[\"先将饼干数组和小孩数组排序\",{\"1\":{\"142\":1}}],[\"那样逻辑其实就复杂了\",{\"1\":{\"143\":1}}],[\"那么根据积分曲线的平移不变形\",{\"1\":{\"302\":1}}],[\"那么称微分方程是自治的\",{\"1\":{\"302\":1}}],[\"那么会大幅度降低参数有效性\",{\"1\":{\"300\":1}}],[\"那么显存占比也将翻倍\",{\"1\":{\"300\":1}}],[\"那么得到了直到阶的单项式\",{\"1\":{\"300\":1}}],[\"那么可以逼近任何函数\",{\"1\":{\"300\":1}}],[\"那么可不可以\",{\"1\":{\"144\":1}}],[\"那么h\",{\"1\":{\"299\":1}}],[\"那么需要提前告知分段点\",{\"1\":{\"296\":1}}],[\"那么如果采用的是自适应步长的ode求解器\",{\"1\":{\"296\":1}}],[\"那么它们可能会陷入相同的局部最小或鞍点\",{\"1\":{\"295\":1}}],[\"那么它们的轨迹也将是相同的\",{\"1\":{\"295\":1}}],[\"那么系统的状态将始终局限于一个低维的子空间\",{\"1\":{\"295\":1}}],[\"那么对于所有的初始点\",{\"1\":{\"295\":1}}],[\"那么对于第\",{\"1\":{\"225\":1}}],[\"那么下一章节的连续归一化流就不能使用\",{\"1\":{\"294\":1}}],[\"那么将\",{\"1\":{\"294\":1}}],[\"那么将所有\",{\"1\":{\"109\":1}}],[\"那么初值问题\",{\"1\":{\"290\":1}}],[\"那么在bp反向传播的时候就不会对参数梯度进行更新\",{\"1\":{\"285\":1}}],[\"那么无监督学习就是发现隐藏的数据中的有价值信息\",{\"1\":{\"242\":1}}],[\"那么他就会有对抗样本\",{\"1\":{\"220\":1}}],[\"那么激励就会增长\",{\"1\":{\"220\":1}}],[\"那么\",{\"1\":{\"202\":1,\"204\":1,\"207\":1,\"298\":1,\"299\":2,\"300\":1}}],[\"那么其联合概率\",{\"1\":{\"196\":1}}],[\"那么我们通常可以用高斯混合模型来估计其分布情况\",{\"1\":{\"195\":1}}],[\"那么网络参数一般可以直接通过最大似然来进行估计\",{\"1\":{\"190\":1}}],[\"那么当i\",{\"1\":{\"144\":1}}],[\"那么就有\",{\"1\":{\"290\":1}}],[\"那么就可以撸代码了\",{\"1\":{\"142\":1}}],[\"那么就应该优先满足胃口大的\",{\"1\":{\"142\":1}}],[\"那么积分得到的结果\",{\"1\":{\"109\":1}}],[\"那么弧长便可写为\",{\"1\":{\"109\":1}}],[\"那么哪一条路径\",{\"1\":{\"109\":1}}],[\"遍历饼干并没有再起一个for循环\",{\"1\":{\"143\":1}}],[\"遍历饼干\",{\"1\":{\"143\":1}}],[\"遍历胃口\",{\"1\":{\"143\":1}}],[\"gallieri\",{\"1\":{\"311\":1}}],[\"gaussian\",{\"1\":{\"195\":1}}],[\"generalized\",{\"1\":{\"226\":1}}],[\"generative\",{\"1\":{\"167\":1}}],[\"goodfellow\",{\"1\":{\"221\":1}}],[\"github\",{\"1\":{\"218\":2}}],[\"gibbs\",{\"1\":{\"208\":1}}],[\"gm\",{\"1\":{\"196\":1}}],[\"gmm\",{\"1\":{\"195\":1}}],[\"g\",{\"1\":{\"143\":5,\"144\":1,\"145\":5}}],[\"gronwall\",{\"1\":{\"299\":1}}],[\"grow\",{\"1\":{\"23\":1}}],[\"gradient\",{\"1\":{\"221\":2}}],[\"graph\",{\"1\":{\"201\":2}}],[\"graphical\",{\"1\":{\"196\":2,\"202\":1,\"213\":1}}],[\"green\",{\"1\":{\"23\":2}}],[\"版本一\",{\"1\":{\"143\":1}}],[\"全连接网络等都是全局连续的\",{\"1\":{\"290\":1}}],[\"全局最优就是喂饱尽可能多的小孩\",{\"1\":{\"142\":1}}],[\"全屏模式\",{\"0\":{\"38\":1}}],[\"充分利用饼干尺寸喂饱一个\",{\"1\":{\"142\":1}}],[\"充分小时\",{\"1\":{\"113\":1}}],[\"就有问题\",{\"1\":{\"234\":1}}],[\"就会产生很大的改变\",{\"1\":{\"220\":1}}],[\"就可以产生对抗样本\",{\"1\":{\"221\":1}}],[\"就可以最大化的增加模型的激励\",{\"1\":{\"220\":1}}],[\"就可以在几乎肉眼看不出差距的前提下\",{\"1\":{\"215\":1}}],[\"就可以推断出另一个值\",{\"1\":{\"196\":1}}],[\"就构成了一个马尔可夫随机场\",{\"1\":{\"207\":1}}],[\"就需要用em\",{\"1\":{\"193\":1}}],[\"就需要利用\",{\"1\":{\"166\":1}}],[\"就是从一个分布\",{\"1\":{\"285\":1}}],[\"就是出现如下情况\",{\"1\":{\"144\":1}}],[\"就是函数变分在固定端点应该满足的条件即固定边界条件\",{\"1\":{\"112\":1}}],[\"就想到用两个for循环\",{\"1\":{\"143\":1}}],[\"就不要造成饼干尺寸的浪费\",{\"1\":{\"142\":1}}],[\"排序+贪心法+双指针\",{\"0\":{\"142\":1}}],[\"力扣题目链接\",{\"1\":{\"147\":1}}],[\"力扣\",{\"1\":{\"141\":1,\"148\":1,\"149\":1,\"155\":1,\"161\":1}}],[\"数据点可能分布在一个比观察到的维度更低的流形上\",{\"1\":{\"299\":1}}],[\"数据可视化或监督学习前期的数据预处理\",{\"1\":{\"242\":1}}],[\"数据集\",{\"1\":{\"240\":1,\"241\":1}}],[\"数据结构与算法\",{\"0\":{\"162\":1}}],[\"数组反转\",{\"0\":{\"151\":1}}],[\"数组\",{\"0\":{\"140\":1},\"1\":{\"147\":1,\"163\":1}}],[\"数学领域\",{\"1\":{\"290\":1}}],[\"数学基础\",{\"0\":{\"168\":1}}],[\"数学\",{\"0\":{\"114\":1}}],[\"蔬菜\",{\"2\":{\"138\":1}}],[\"本文只讨论基于一阶标准微分方程的神经网络架构\",{\"1\":{\"302\":1}}],[\"本文摘要\",{\"1\":{\"135\":1}}],[\"本身存在的现象\",{\"1\":{\"202\":1}}],[\"本章首先给出神经常微分方程\",{\"1\":{\"286\":1}}],[\"本章目录\",{\"0\":{\"199\":1}}],[\"本章先介绍概率生成模型的基本概念\",{\"1\":{\"167\":1}}],[\"本节介绍一个em算法的应用例子\",{\"1\":{\"195\":1}}],[\"本节只讨论在给定网络结构条件下的参数估计问题\",{\"1\":{\"189\":1}}],[\"本页面就是一个示例\",{\"1\":{\"51\":1}}],[\"番茄\",{\"0\":{\"135\":1}}],[\"草莓\",{\"0\":{\"130\":1},\"2\":{\"133\":1}}],[\"重参数化技巧可以保证我们从\",{\"1\":{\"285\":1}}],[\"重参数化技巧\",{\"0\":{\"285\":1},\"1\":{\"284\":1,\"285\":1}}],[\"重参数技巧\",{\"0\":{\"127\":1}}],[\"重叠\",{\"1\":{\"153\":1}}],[\"重要信息\",{\"1\":{\"97\":2}}],[\"重要容器\",{\"0\":{\"97\":1}}],[\"重要的内容\",{\"1\":{\"69\":1}}],[\"重要\",{\"1\":{\"44\":1,\"45\":1,\"46\":1,\"97\":1}}],[\"大尺寸的饼干既可以满足胃口大的孩子也可以满足胃口小的孩子\",{\"1\":{\"142\":1}}],[\"大\",{\"2\":{\"126\":1,\"247\":1,\"252\":1,\"257\":1,\"262\":1}}],[\"水果\",{\"2\":{\"125\":1,\"133\":1,\"256\":1,\"261\":1,\"266\":1,\"271\":1}}],[\"水街\",{\"1\":{\"46\":1}}],[\"火龙果\",{\"0\":{\"122\":1},\"2\":{\"125\":1}}],[\"圆\",{\"2\":{\"121\":1,\"139\":1,\"247\":1,\"252\":1,\"257\":1,\"262\":1}}],[\"小饼干先喂饱小胃口\",{\"1\":{\"145\":1}}],[\"小\",{\"2\":{\"121\":1,\"129\":1,\"134\":1}}],[\"小馄饨一定别错过\",{\"1\":{\"46\":1}}],[\"红\",{\"2\":{\"121\":1,\"126\":1,\"129\":1,\"134\":1,\"139\":1,\"247\":1,\"252\":1,\"257\":1,\"262\":1}}],[\"樱桃\",{\"0\":{\"117\":1},\"2\":{\"120\":1}}],[\"机器学习可解释化是优势\",{\"1\":{\"299\":1}}],[\"机器学习中使用广泛\",{\"1\":{\"238\":1}}],[\"机器学习\",{\"0\":{\"116\":1},\"1\":{\"194\":1,\"200\":2},\"2\":{\"128\":1}}],[\"探索通往最小值之路\",{\"1\":{\"113\":1}}],[\"科学空间|scientific\",{\"1\":{\"113\":2}}],[\"从概率论的角度来看\",{\"1\":{\"295\":1}}],[\"从而提高了模型在未见数据上的泛化能力\",{\"1\":{\"295\":1}}],[\"从而限制了网络能够学习的功能\",{\"1\":{\"295\":1}}],[\"从而能够探索状态空间中的更多区域\",{\"1\":{\"295\":1}}],[\"从而\",{\"1\":{\"235\":1}}],[\"从而导致计算只能用实际数值的近似表示式来完成\",{\"1\":{\"234\":1}}],[\"从而降低模型的性能\",{\"1\":{\"221\":1}}],[\"从而给研究高维空间中的概率模型带来了很大的便捷性\",{\"1\":{\"196\":1}}],[\"从对抗样本的线性视角来看\",{\"1\":{\"221\":1}}],[\"从\",{\"1\":{\"202\":2}}],[\"从高斯混合模型中生成一个样本\",{\"1\":{\"195\":1}}],[\"从代码中可以看出我用了一个index来控制饼干数组的遍历\",{\"1\":{\"143\":1}}],[\"从wasserstein距离\",{\"1\":{\"113\":1}}],[\"从夫子庙前天下文枢牌坊下的洋池登船\",{\"1\":{\"46\":1}}],[\"并不能表示所有的同胚\",{\"1\":{\"300\":1}}],[\"并且可以帮助我们设计更有效的学习算法\",{\"1\":{\"299\":1}}],[\"并且可以更好地表示数据的内在结构和特征\",{\"1\":{\"299\":1}}],[\"并且扰动的\",{\"1\":{\"221\":1}}],[\"并且整个扰动的\",{\"1\":{\"221\":1}}],[\"并且马尔可夫随机场的概率分布一定可以表示成吉布斯分布\",{\"1\":{\"208\":1}}],[\"并且模型简单\",{\"1\":{\"205\":1}}],[\"并且这种角度有很多优点\",{\"1\":{\"198\":1}}],[\"并且它的概率只依赖于的值\",{\"1\":{\"196\":1}}],[\"并且它会自动换行\",{\"1\":{\"8\":1}}],[\"并可以将一个复杂的联合概率模型分解为一些简单条件概率模型的组合\",{\"1\":{\"196\":1}}],[\"并可以用这个模型来生成一些样本\",{\"1\":{\"167\":1}}],[\"并将它们相加以得到总数\",{\"1\":{\"196\":1}}],[\"并令其等于\",{\"1\":{\"195\":1,\"240\":1,\"241\":1}}],[\"并满足\",{\"1\":{\"195\":1,\"241\":1}}],[\"并想不出反例\",{\"1\":{\"142\":1}}],[\"并统计满足小孩数量\",{\"1\":{\"142\":1}}],[\"并把\",{\"1\":{\"113\":1}}],[\"更具体点可以设\",{\"1\":{\"294\":1}}],[\"更高阶的无穷小量\",{\"1\":{\"113\":1}}],[\"更新时间等页面元信息\",{\"1\":{\"88\":1}}],[\"更新时间\",{\"1\":{\"51\":1}}],[\"相应的解记作\",{\"1\":{\"302\":1}}],[\"相伴的摄动问题\",{\"1\":{\"234\":1}}],[\"相平面\",{\"0\":{\"231\":1}}],[\"相等\",{\"1\":{\"194\":1}}],[\"相差一个比\",{\"1\":{\"113\":1}}],[\"相关信息\",{\"1\":{\"59\":1}}],[\"相关配置文档请见\",{\"1\":{\"0\":1}}],[\"面\",{\"1\":{\"113\":1}}],[\"趋于零而趋于零\",{\"1\":{\"113\":2}}],[\"随着维度的增加\",{\"1\":{\"241\":1}}],[\"随机噪声初始化允许网络在训练过程中探索更多的状态组合\",{\"1\":{\"295\":1}}],[\"随机噪声允许系统在整个状态空间中以某种概率分布进行采样\",{\"1\":{\"295\":1}}],[\"随机噪声打破了这种对称性\",{\"1\":{\"295\":1}}],[\"随机噪声可以使得系统从一系列不同的初始条件开始演化\",{\"1\":{\"295\":1}}],[\"随机化额外维度的初始值\",{\"0\":{\"295\":1}}],[\"随机初始化参数\",{\"1\":{\"195\":1}}],[\"随机选取一个高斯分布\",{\"1\":{\"195\":1}}],[\"随\",{\"1\":{\"113\":2}}],[\"且满足性质\",{\"1\":{\"302\":1}}],[\"且满足\",{\"1\":{\"302\":1}}],[\"且最终趋向于添加了无穷层时\",{\"1\":{\"290\":1}}],[\"且在\",{\"1\":{\"234\":1,\"235\":1}}],[\"且存储了小数部分和指数部分\",{\"1\":{\"234\":1}}],[\"且存在域\",{\"1\":{\"230\":1}}],[\"且存在这样的\",{\"1\":{\"230\":1}}],[\"且取值为实数\",{\"1\":{\"204\":1}}],[\"且其增量可表示为\",{\"1\":{\"113\":1}}],[\"且\",{\"1\":{\"113\":1,\"234\":1}}],[\"且二阶连续可微\",{\"1\":{\"113\":1}}],[\"必有\",{\"1\":{\"113\":1}}],[\"当然这种增强破坏了node的双射的性质\",{\"1\":{\"294\":1}}],[\"当固定时\",{\"1\":{\"290\":1}}],[\"当零解\",{\"1\":{\"230\":1}}],[\"当使用卷积maxout网络与cifar\",{\"1\":{\"221\":1}}],[\"当我们计算这个损失函数关于输入\",{\"1\":{\"221\":1}}],[\"当我们增加一个很小的扰动的时候\",{\"1\":{\"220\":1}}],[\"当且仅当\",{\"1\":{\"208\":1,\"230\":1}}],[\"当概率模型中的变量数量比较多时\",{\"1\":{\"196\":1}}],[\"当计算参数\",{\"1\":{\"194\":1}}],[\"当模型变量比较多时\",{\"1\":{\"192\":1}}],[\"当这两个分布比较复杂时\",{\"1\":{\"166\":1}}],[\"当\",{\"1\":{\"113\":1,\"148\":1,\"196\":1,\"220\":1,\"225\":1}}],[\"介于\",{\"1\":{\"113\":2}}],[\"介绍\",{\"0\":{\"58\":1,\"174\":1,\"304\":1,\"308\":1},\"1\":{\"58\":1,\"59\":1,\"188\":1}}],[\"于是只是写成\",{\"1\":{\"113\":1}}],[\"其不可微\",{\"1\":{\"285\":1}}],[\"其余\",{\"1\":{\"241\":1}}],[\"其在点\",{\"1\":{\"221\":1}}],[\"其分类准确性可以非常出色\",{\"1\":{\"215\":1}}],[\"其所有的父节点之间没有连接\",{\"1\":{\"204\":1}}],[\"其参数数量又可以大幅减少\",{\"1\":{\"204\":1}}],[\"其参数量就可以大幅减少\",{\"1\":{\"196\":1}}],[\"其局部马尔可夫性质为\",{\"1\":{\"202\":1}}],[\"其条件概率分布表示为\",{\"1\":{\"204\":1}}],[\"其条件概率表需要\",{\"1\":{\"191\":1}}],[\"其条件依赖关系也比较复杂\",{\"1\":{\"196\":1}}],[\"其联合概率为高维空间中的分布\",{\"1\":{\"196\":1}}],[\"其对数边际分布为\",{\"1\":{\"195\":1}}],[\"其对数似然函数为\",{\"1\":{\"191\":1,\"192\":1,\"239\":1}}],[\"其总体密度函数为多个高斯密度函数的加权组合\",{\"1\":{\"195\":1}}],[\"其他\",{\"0\":{\"145\":1}}],[\"其实目前大部分有效的流行的深度学习框架都类似于微分方程\",{\"1\":{\"290\":1}}],[\"其实是不可以的\",{\"1\":{\"144\":1}}],[\"其实不仅仅只是\",{\"1\":{\"113\":1}}],[\"其中是一个维向量\",{\"1\":{\"302\":1}}],[\"其中是紧集\",{\"1\":{\"298\":1}}],[\"其中是神经网络架构\",{\"1\":{\"290\":1}}],[\"其中参数\",{\"1\":{\"296\":1}}],[\"其中参数是需要学习的\",{\"1\":{\"294\":1}}],[\"其中仿射层有界\",{\"1\":{\"290\":1}}],[\"其中函数\",{\"1\":{\"210\":1}}],[\"其中矩形表示其中的变量重复\",{\"1\":{\"194\":1}}],[\"其中左图表示手写体数字图像的真实分布\",{\"1\":{\"167\":1}}],[\"其中每一维都表示一个像素值\",{\"1\":{\"166\":1}}],[\"其中\",{\"1\":{\"109\":2,\"113\":2,\"172\":1,\"191\":2,\"192\":4,\"194\":2,\"195\":5,\"196\":1,\"202\":1,\"204\":1,\"205\":2,\"206\":2,\"207\":2,\"208\":3,\"210\":2,\"221\":2,\"225\":1,\"226\":1,\"234\":1,\"235\":1,\"240\":1,\"241\":2,\"298\":2}}],[\"最终分布的概率密度由变量变换公式给出\",{\"1\":{\"283\":1}}],[\"最小重构错误等\",{\"1\":{\"242\":1}}],[\"最理想的分布\",{\"1\":{\"194\":1}}],[\"最大\",{\"1\":{\"194\":1,\"239\":1}}],[\"最大化\",{\"1\":{\"194\":1}}],[\"最大化对数边际似然函数\",{\"1\":{\"194\":1}}],[\"最大化对数似然\",{\"1\":{\"191\":1}}],[\"最后所有的饼干都匹配不上\",{\"1\":{\"144\":1}}],[\"最后显示\",{\"1\":{\"27\":1}}],[\"最简泛函\",{\"1\":{\"113\":1}}],[\"最简泛函的变分\",{\"0\":{\"113\":1}}],[\"拉布拉斯算子\",{\"1\":{\"112\":1}}],[\"拉格朗日方程\",{\"1\":{\"109\":2}}],[\"哈密顿算子\",{\"1\":{\"112\":1}}],[\"换句话说\",{\"1\":{\"112\":1}}],[\"同一个微分方程的两个轨迹是不想交的\",{\"1\":{\"299\":1}}],[\"同一平面内的两点\",{\"1\":{\"109\":1}}],[\"同胚和流形假设\",{\"0\":{\"299\":1}}],[\"同门可以通过以下的公式来得到对抗扰动\",{\"1\":{\"221\":1}}],[\"同时在后添加了一层终端函数\",{\"1\":{\"298\":1}}],[\"同时也保证了前\",{\"1\":{\"294\":1}}],[\"同时也经常对特征进行一些约束\",{\"1\":{\"242\":1}}],[\"同时对node的输出再进行处理\",{\"1\":{\"290\":1}}],[\"同时对训练用到的超参数的选择\",{\"1\":{\"286\":1}}],[\"同时又能保留梯度信息\",{\"1\":{\"285\":1}}],[\"同时\",{\"1\":{\"219\":1,\"221\":1}}],[\"同时我们可以根据链式法则写出联合概率\",{\"1\":{\"202\":3}}],[\"同时具有大小和方向\",{\"1\":{\"172\":1}}],[\"同样也可以表示当\",{\"1\":{\"112\":1}}],[\"由堆叠而层\",{\"1\":{\"300\":1}}],[\"由引理得出\",{\"1\":{\"299\":1}}],[\"由可测函数\",{\"1\":{\"298\":1}}],[\"由有限的激活函数为relu的仿射层组成\",{\"1\":{\"290\":1}}],[\"由式\",{\"1\":{\"234\":1}}],[\"由第\",{\"1\":{\"195\":1}}],[\"由\",{\"1\":{\"194\":1}}],[\"由于ode的求解比较难\",{\"1\":{\"290\":1}}],[\"由于正模和反模轨迹被视为两个独立的ivp\",{\"1\":{\"288\":1}}],[\"由于势能函数必须为正\",{\"1\":{\"208\":1}}],[\"由于无向图模型并不提供一个变量的拓扑顺序\",{\"1\":{\"208\":1}}],[\"由于和都是二值变量\",{\"1\":{\"196\":1}}],[\"由于概率总和必须为1\",{\"1\":{\"196\":1}}],[\"由于我们无法观测样本\",{\"1\":{\"195\":1}}],[\"由于在最优点时梯度为\",{\"1\":{\"192\":1}}],[\"由于可取函数都通过区间的固定端点\",{\"1\":{\"112\":1}}],[\"由二元函数的泰勒中值定理得\",{\"1\":{\"113\":1}}],[\"由此\",{\"1\":{\"202\":1}}],[\"由此得到变分符号\",{\"1\":{\"112\":1}}],[\"由此也可以看出变分符号\",{\"1\":{\"112\":1}}],[\"取\",{\"1\":{\"302\":1}}],[\"取某一定值时函数的导数的改变\",{\"1\":{\"112\":1}}],[\"取某一定值时函数的微小改变\",{\"1\":{\"112\":2}}],[\"取一个增量而使同一函数\",{\"1\":{\"112\":1}}],[\"取固定值时之差\",{\"1\":{\"112\":1}}],[\"取极小值\",{\"1\":{\"109\":1}}],[\"如单高斯分布\",{\"1\":{\"283\":1}}],[\"如何通过图结构来描述变量之间的依\",{\"1\":{\"197\":1}}],[\"如图11\",{\"1\":{\"204\":1}}],[\"如图可以写出其因子分解的结果\",{\"1\":{\"202\":2}}],[\"如图\",{\"1\":{\"142\":1}}],[\"如图所示\",{\"1\":{\"112\":1}}],[\"如果微分方程是自治的\",{\"1\":{\"302\":1}}],[\"如果与时间无关\",{\"1\":{\"302\":1}}],[\"如果维度增加倍\",{\"1\":{\"300\":1}}],[\"如果所有的路径都从相同的点开始\",{\"1\":{\"295\":1}}],[\"如果增广维度总是初始化为零\",{\"1\":{\"295\":1}}],[\"如果增广的维度被初始化为零\",{\"1\":{\"295\":1}}],[\"如果它们的增广部分是相同的\",{\"1\":{\"295\":1}}],[\"如果我们有一个向量场\",{\"1\":{\"295\":1}}],[\"如果直接进行采样\",{\"1\":{\"285\":1}}],[\"如果监督学习是建立输入\",{\"1\":{\"242\":1}}],[\"如果原问题不是适定的\",{\"1\":{\"234\":1}}],[\"如果存在一个正常数\",{\"1\":{\"234\":1}}],[\"如果零解\",{\"1\":{\"230\":1}}],[\"如果对某个给定的\",{\"1\":{\"230\":1}}],[\"如果对任意给定的\",{\"1\":{\"230\":1}}],[\"如果对不同的变量的条件概率都共享使用一个参数化模型\",{\"1\":{\"204\":1}}],[\"如果梯度为零\",{\"1\":{\"221\":1}}],[\"如果他的输入有着足够的维度\",{\"1\":{\"220\":1}}],[\"如果用对数线性模型来建模条件概率\",{\"1\":{\"210\":1}}],[\"如果一个分布\",{\"1\":{\"208\":1}}],[\"如果一个团不能被其他的团包含\",{\"1\":{\"208\":1}}],[\"如果一个连续随机变量或连续随机向量的分布比较复杂\",{\"1\":{\"195\":1}}],[\"如果假设\",{\"1\":{\"204\":1}}],[\"如果使用参数化模型只需要\",{\"1\":{\"204\":1}}],[\"如果使用表格来记录条件概率需要\",{\"1\":{\"204\":1}}],[\"如果两个节点之间有连边\",{\"1\":{\"201\":1}}],[\"如果分别用\",{\"1\":{\"196\":1}}],[\"如果某些变量之间存在条件独立\",{\"1\":{\"196\":1}}],[\"如果图模型中包含隐变量\",{\"1\":{\"193\":1}}],[\"如果图模型中不包含隐变量\",{\"1\":{\"190\":1}}],[\"如果变量\",{\"1\":{\"191\":2}}],[\"如果要建模含隐变量的分布\",{\"1\":{\"166\":1}}],[\"如果有必要的话\",{\"1\":{\"153\":1}}],[\"如果\",{\"1\":{\"144\":1,\"194\":1,\"202\":1,\"207\":1,\"230\":1,\"234\":4,\"298\":1}}],[\"如果函数\",{\"1\":{\"112\":1}}],[\"如果取不同的路径\",{\"1\":{\"109\":1}}],[\"如果你不了解它\",{\"1\":{\"59\":1}}],[\"如果你是一个新手\",{\"1\":{\"58\":1}}],[\"注意\",{\"1\":{\"202\":1,\"290\":1,\"299\":1}}],[\"注意版本一的代码中\",{\"1\":{\"144\":1}}],[\"注意事项\",{\"0\":{\"144\":1}}],[\"注意函数变分\",{\"1\":{\"112\":1}}],[\"注释之前的内容被视为文章摘要\",{\"1\":{\"85\":1}}],[\"根据机器学习中的奥卡姆剃刀原理\",{\"1\":{\"302\":1}}],[\"根据灵敏度公式\",{\"1\":{\"296\":1}}],[\"根据picard存在定理\",{\"1\":{\"290\":1}}],[\"根据柯西\",{\"1\":{\"221\":1}}],[\"根据马尔可夫性质\",{\"1\":{\"207\":1}}],[\"根据此规律可以直接看图得出结论\",{\"1\":{\"202\":1}}],[\"根据公式\",{\"1\":{\"195\":2}}],[\"根据\",{\"1\":{\"194\":1,\"195\":1,\"208\":1,\"299\":2,\"300\":1}}],[\"根据变分的定义\",{\"1\":{\"112\":1}}],[\"根据上述定义\",{\"1\":{\"111\":1}}],[\"显然\",{\"1\":{\"112\":1}}],[\"显然函数的弱邻域是函数的强邻域的一部分\",{\"1\":{\"111\":1}}],[\"强变分与弱变分统称变分\",{\"1\":{\"112\":1}}],[\"泛函\",{\"1\":{\"113\":1}}],[\"泛函的变量量\",{\"1\":{\"112\":1}}],[\"泛函能够得到一个数\",{\"1\":{\"109\":1}}],[\"故\",{\"1\":{\"113\":1}}],[\"故也可以这样定义变分\",{\"1\":{\"112\":1}}],[\"故在区间的端点\",{\"1\":{\"112\":1}}],[\"因为不同的路径可能会避开某些不良的局部最小\",{\"1\":{\"295\":1}}],[\"因为在没有额外信息的情况下\",{\"1\":{\"295\":1}}],[\"因为它限制了模型在训练数据上的完美拟合能力\",{\"1\":{\"295\":1}}],[\"因为它要求可逆\",{\"1\":{\"294\":1}}],[\"因为它只需要计算一次输入\",{\"1\":{\"221\":1}}],[\"因为最常见和有效的激活函数和网络例如relus\",{\"1\":{\"290\":1}}],[\"因为表示式中的任何舍人误差都使原问题摄动\",{\"1\":{\"234\":1}}],[\"因为另一个值的概率可以通过1减去已知的概率来得到\",{\"1\":{\"196\":1}}],[\"因为另一个可以通过1减去已知的条件概率来得到\",{\"1\":{\"196\":1}}],[\"因为也是二值变量\",{\"1\":{\"196\":1}}],[\"因为涉及在联合概率空间\",{\"1\":{\"192\":1}}],[\"因为所有变量都是可观测的\",{\"1\":{\"191\":1}}],[\"因为\",{\"1\":{\"144\":1}}],[\"因为可取函数\",{\"1\":{\"112\":1}}],[\"因而\",{\"1\":{\"113\":1}}],[\"因对\",{\"1\":{\"113\":1}}],[\"因此定理从和定理实际上只是一种理论上表达通过提高维度可以模型的表达能力\",{\"1\":{\"300\":1}}],[\"因此假设空间\",{\"1\":{\"299\":1}}],[\"因此不存在能表示函数\",{\"1\":{\"299\":1}}],[\"因此如果作为假设空间表示能力和逼近能力都非常有限\",{\"1\":{\"298\":1}}],[\"因此作者猜测\",{\"1\":{\"220\":1}}],[\"因此我们一般定义为\",{\"1\":{\"208\":1}}],[\"因此可以根据有向图写出因子分解的结果\",{\"1\":{\"202\":1}}],[\"因此可以借助于\",{\"1\":{\"60\":1}}],[\"因此无法用链式法则对\",{\"1\":{\"208\":1}}],[\"因此无法直接用最大似然来进行参数估计\",{\"1\":{\"195\":1}}],[\"因此无向图的最大似然估计的优化目标等价于\",{\"1\":{\"192\":1}}],[\"因此在推断和参数学习时都需要重点考虑\",{\"1\":{\"208\":1}}],[\"因此在\",{\"1\":{\"194\":1}}],[\"因此固定的两点之间的距离实际上就是路径函数\",{\"1\":{\"109\":1}}],[\"因此\",{\"1\":{\"57\":1,\"113\":1,\"166\":2,\"192\":2,\"196\":3,\"198\":1,\"202\":3,\"204\":1,\"208\":1,\"221\":2,\"295\":1}}],[\"式\",{\"1\":{\"112\":1}}],[\"式中\",{\"1\":{\"112\":1,\"113\":2}}],[\"也能够在增广空间中产生不同的轨迹\",{\"1\":{\"295\":1}}],[\"也叫最大熵模型\",{\"1\":{\"209\":1}}],[\"也称为簇\",{\"1\":{\"242\":1}}],[\"也称为马尔可夫随机场\",{\"1\":{\"207\":1}}],[\"也称为贝叶斯网络\",{\"1\":{\"202\":1}}],[\"也需要2个参数\",{\"1\":{\"196\":1}}],[\"也和\",{\"1\":{\"196\":1}}],[\"也缺乏有效的采样方法\",{\"1\":{\"167\":1}}],[\"也会被接受\",{\"1\":{\"148\":1}}],[\"也是二值变量\",{\"1\":{\"196\":1}}],[\"也是偶数\",{\"1\":{\"148\":1}}],[\"也是奇数\",{\"1\":{\"148\":1}}],[\"也可以使用服从某个随机分布\",{\"1\":{\"295\":1}}],[\"也可以解释为什么\",{\"1\":{\"220\":1}}],[\"也可以换一个思路\",{\"1\":{\"145\":1}}],[\"也可以通过在特定幻灯片添加\",{\"1\":{\"29\":1}}],[\"也就能求出来了\",{\"1\":{\"113\":1}}],[\"也就是说对于不同的\",{\"1\":{\"226\":1}}],[\"也就是\",{\"1\":{\"112\":1}}],[\"任何初值问题记为\",{\"1\":{\"302\":1}}],[\"任何一个无向图模型都可以用公式\",{\"1\":{\"208\":1}}],[\"任取一曲线\",{\"1\":{\"113\":1}}],[\"任意函数\",{\"1\":{\"112\":1}}],[\"任务列表\",{\"0\":{\"70\":1,\"104\":1,\"186\":1}}],[\"这意味着\",{\"1\":{\"299\":1}}],[\"这会需要额外的\",{\"1\":{\"296\":1}}],[\"这有助于防止过拟合\",{\"1\":{\"295\":1}}],[\"这在数学上意味着\",{\"1\":{\"295\":1}}],[\"这可以通过lyapunov指数来量化\",{\"1\":{\"295\":1}}],[\"这就更进一步降低参数量了\",{\"1\":{\"294\":1}}],[\"这就是为什么\",{\"1\":{\"221\":1}}],[\"这就是期望最大化\",{\"1\":{\"194\":1}}],[\"这就是变分自编码器的思想\",{\"1\":{\"166\":1}}],[\"这导致我们很难准确估计数据的真实分布\",{\"1\":{\"241\":1}}],[\"这曲线称为积分曲线\",{\"1\":{\"231\":1}}],[\"这使得\",{\"1\":{\"221\":1}}],[\"这确保了每个元素的扰动都是等量的\",{\"1\":{\"221\":1}}],[\"这被称为\",{\"1\":{\"220\":1}}],[\"这也对于部分任务例如图形生成任务来\",{\"1\":{\"299\":1}}],[\"这也大幅提高neuralode的表达能力\",{\"1\":{\"290\":1}}],[\"这也就丟失了输入图像的1\",{\"1\":{\"220\":1}}],[\"这也是常用的技巧\",{\"1\":{\"143\":1}}],[\"这\",{\"1\":{\"208\":1}}],[\"这两个模型的区别在于\",{\"1\":{\"204\":1}}],[\"这两步不断重复\",{\"1\":{\"194\":1}}],[\"这种隐藏术的意思是\",{\"1\":{\"220\":1}}],[\"这种形式的无向图模型也称为对数线性模型\",{\"1\":{\"210\":1}}],[\"这种形式的分布又称为玻尔兹曼分布\",{\"1\":{\"208\":1}}],[\"这种情况中\",{\"1\":{\"202\":1}}],[\"这种表示方法称为盘子表示法\",{\"1\":{\"194\":1}}],[\"这一步可以看作全观测变量图模型的参数估计问题\",{\"1\":{\"194\":1}}],[\"这个理论上可以表示任何同胚映射\",{\"1\":{\"293\":1}}],[\"这个子集仅包含了正\",{\"1\":{\"234\":1}}],[\"这个方法由\",{\"1\":{\"221\":1}}],[\"这个公式是快速梯度符号方法\",{\"1\":{\"221\":1}}],[\"这个条件概率与类似\",{\"1\":{\"196\":1}}],[\"这个求和操作依然存在\",{\"1\":{\"194\":1}}],[\"这个计算往往无法实现\",{\"1\":{\"192\":1}}],[\"这个例子可以看出饼干9只有喂给胃口为7的小孩\",{\"1\":{\"142\":1}}],[\"这里\",{\"1\":{\"221\":1,\"225\":1,\"234\":1,\"290\":1}}],[\"这里用\",{\"1\":{\"206\":1}}],[\"这里考虑一维的情况\",{\"1\":{\"195\":1}}],[\"这里的局部最优就是大饼干喂给胃口大的\",{\"1\":{\"142\":1}}],[\"这里是内容\",{\"1\":{\"118\":1,\"119\":1,\"123\":1,\"124\":1,\"131\":1,\"132\":1,\"136\":1,\"137\":1,\"244\":1,\"245\":1,\"249\":1,\"250\":1,\"254\":1,\"255\":1,\"259\":1,\"260\":1,\"264\":1,\"265\":1,\"269\":1,\"270\":1,\"274\":1,\"275\":1,\"279\":1,\"280\":1}}],[\"这样参数估计问题就转化为最优化问题\",{\"1\":{\"239\":1}}],[\"这样的扰动通常很小\",{\"1\":{\"221\":1}}],[\"这样联合概率\",{\"1\":{\"210\":1}}],[\"这样密度估计问题可以转换为估计变\",{\"1\":{\"166\":1}}],[\"这样才是整体最优解\",{\"1\":{\"142\":1}}],[\"这样\",{\"1\":{\"113\":1,\"194\":2,\"220\":1,\"221\":1}}],[\"这时有\",{\"1\":{\"112\":1}}],[\"这是混沌理论的一个关键观点\",{\"1\":{\"295\":1}}],[\"这是梯度的基本性质\",{\"1\":{\"221\":1}}],[\"这是因为新的区间\",{\"1\":{\"153\":1}}],[\"这是由于\",{\"1\":{\"113\":1}}],[\"这是脚注内容\",{\"1\":{\"82\":1,\"104\":1,\"186\":1}}],[\"这是一个条件概率\",{\"1\":{\"196\":2}}],[\"这是一个二值变量\",{\"1\":{\"196\":1}}],[\"这是一个有着\",{\"1\":{\"8\":1}}],[\"这是一个\",{\"0\":{\"8\":1}}],[\"这是一个博客主页的案例\",{\"1\":{\"0\":1}}],[\"处可微\",{\"1\":{\"113\":1}}],[\"处的一阶泰勒展开大约为\",{\"1\":{\"221\":1}}],[\"处的梯度\",{\"1\":{\"221\":1}}],[\"处的值\",{\"1\":{\"113\":1}}],[\"处的弱变分\",{\"1\":{\"112\":1}}],[\"处的强变分\",{\"1\":{\"112\":1}}],[\"处的变分\",{\"1\":{\"112\":1}}],[\"处的变分或函数的变分\",{\"1\":{\"112\":1}}],[\"处具有\",{\"1\":{\"111\":1}}],[\"之所以产生舍人误差是因为机器中进行的算术运算所涉及的数是有限位的\",{\"1\":{\"234\":1}}],[\"之间是条件独立的\",{\"1\":{\"205\":1}}],[\"之间具有非独立的因果关系\",{\"1\":{\"202\":1}}],[\"之间条件独立性的图形化表示\",{\"1\":{\"196\":1}}],[\"之间\",{\"1\":{\"113\":2,\"235\":1}}],[\"之间的依赖关系\",{\"1\":{\"201\":1}}],[\"之间的关系\",{\"1\":{\"112\":1}}],[\"之间的路径\",{\"1\":{\"109\":1}}],[\"之差\",{\"1\":{\"112\":2}}],[\"都存在neuralode\",{\"1\":{\"300\":1}}],[\"都可导\",{\"1\":{\"112\":1}}],[\"都满足\",{\"1\":{\"112\":1}}],[\"都有一个概率分布\",{\"1\":{\"196\":1}}],[\"都有\",{\"1\":{\"111\":1}}],[\"都是来南京的首选\",{\"1\":{\"44\":1}}],[\"设开集\",{\"1\":{\"302\":1}}],[\"设未知函数满足方程\",{\"1\":{\"302\":1}}],[\"设有如下高阶常微分方程和初始条件\",{\"1\":{\"290\":1}}],[\"设计网络架构\",{\"1\":{\"286\":1}}],[\"设它的预激活值为\",{\"1\":{\"225\":1}}],[\"设\",{\"1\":{\"113\":1,\"234\":1,\"235\":1,\"294\":1,\"298\":4,\"299\":1}}],[\"设函数\",{\"1\":{\"111\":1,\"302\":1}}],[\"设已知函数\",{\"1\":{\"111\":1}}],[\"有各种高效的数值解和现成的计算框架\",{\"1\":{\"302\":1}}],[\"有唯一解\",{\"1\":{\"290\":1}}],[\"有局部截断误差\",{\"1\":{\"236\":1}}],[\"有关\",{\"1\":{\"230\":1}}],[\"有一个大小限制\",{\"1\":{\"221\":1}}],[\"有三种情况\",{\"1\":{\"202\":1}}],[\"有向图和无向图直接的转换\",{\"0\":{\"212\":1}}],[\"有向图和无向图示例\",{\"1\":{\"201\":1}}],[\"有向图和无向图\",{\"1\":{\"201\":1}}],[\"有向图模型使用有向非循环图\",{\"1\":{\"201\":1}}],[\"有向图模型和无向图模型\",{\"1\":{\"201\":1}}],[\"有向图模型\",{\"0\":{\"191\":1,\"202\":1},\"1\":{\"202\":1}}],[\"有重叠\",{\"1\":{\"154\":1}}],[\"有效的山脉数组\",{\"0\":{\"146\":1}}],[\"有的同学看到要遍历两个数组\",{\"1\":{\"143\":1}}],[\"有\",{\"1\":{\"113\":1,\"235\":3,\"300\":1}}],[\"有时泛函的增量也称为泛函的全变分\",{\"1\":{\"113\":1}}],[\"有时也称为价值泛函\",{\"1\":{\"113\":1}}],[\"有以下性质\",{\"1\":{\"112\":1}}],[\"有了接近度的概念可以精确地定义泛函的连续\",{\"1\":{\"111\":1}}],[\"有无数条\",{\"1\":{\"109\":1}}],[\"两个向量的点积的最大值是当它们是平行时取得的\",{\"1\":{\"221\":1}}],[\"两曲线的接近程度就越近\",{\"1\":{\"111\":1}}],[\"两点在路径上\",{\"1\":{\"109\":1}}],[\"接近度的连续泛函\",{\"1\":{\"111\":1}}],[\"接近度\",{\"1\":{\"111\":3,\"112\":1}}],[\"则对给定的\",{\"1\":{\"302\":1}}],[\"则对于\",{\"1\":{\"235\":1}}],[\"则初值问题\",{\"1\":{\"234\":1}}],[\"则没有理由期望摄动问题的数值解会精确地近原问题的解\",{\"1\":{\"234\":1}}],[\"则误差的增长称为是指数的\",{\"1\":{\"234\":1}}],[\"则误差的增长称为是线性的\",{\"1\":{\"234\":1}}],[\"则域\",{\"1\":{\"230\":1}}],[\"则称\",{\"1\":{\"234\":1}}],[\"则称零解\",{\"1\":{\"230\":2}}],[\"则称方程组\",{\"1\":{\"230\":2}}],[\"则路径是通的\",{\"1\":{\"202\":1}}],[\"则路径被堵塞\",{\"1\":{\"202\":1}}],[\"则需要\",{\"1\":{\"196\":1}}],[\"则在它在\",{\"1\":{\"113\":1}}],[\"则这个泛函称为在\",{\"1\":{\"113\":1}}],[\"则有\",{\"1\":{\"113\":1}}],[\"则函数的变分\",{\"1\":{\"112\":1}}],[\"则泛函\",{\"1\":{\"111\":1,\"113\":1}}],[\"则它们具有任何低于\",{\"1\":{\"111\":1}}],[\"则\",{\"1\":{\"111\":1,\"112\":2}}],[\"则所有与\",{\"1\":{\"111\":1}}],[\"若干个世纪以来\",{\"1\":{\"290\":1}}],[\"若稳定域为全空间\",{\"1\":{\"230\":1}}],[\"若泛函\",{\"1\":{\"113\":1}}],[\"若该泛函具有二阶连续性\",{\"1\":{\"113\":1}}],[\"若可取函数\",{\"1\":{\"112\":1}}],[\"若对于任意给定的一个正数\",{\"1\":{\"111\":1}}],[\"若\",{\"1\":{\"111\":1,\"112\":1,\"202\":2,\"205\":2}}],[\"曲线\",{\"1\":{\"111\":1}}],[\"而其他问题则可能需要随机噪声来提高性能\",{\"1\":{\"295\":1}}],[\"而神经微分方程就是参数共享的resnet网络模型的连续化\",{\"1\":{\"290\":1}}],[\"而该分布是带有参数\",{\"1\":{\"285\":1}}],[\"而不是简单的正态分布或多项分布\",{\"1\":{\"241\":1}}],[\"而不是直接使用梯度值\",{\"1\":{\"221\":1}}],[\"而非线性的模型\",{\"1\":{\"221\":1}}],[\"而非变量\",{\"1\":{\"204\":1}}],[\"而如果对抗扰动足够小的话\",{\"1\":{\"220\":1}}],[\"而且模型简单更容易设计\",{\"1\":{\"302\":1}}],[\"而且它的概率依赖于的值\",{\"1\":{\"196\":1}}],[\"而且还是\",{\"1\":{\"113\":1}}],[\"而计算后验分布\",{\"1\":{\"194\":1}}],[\"而在密度估计中\",{\"1\":{\"242\":1}}],[\"而在无向图中\",{\"1\":{\"192\":1}}],[\"而在\",{\"1\":{\"166\":1}}],[\"而index\",{\"1\":{\"144\":1}}],[\"而if里面的下标\",{\"1\":{\"144\":1}}],[\"而是采用自减的方式\",{\"1\":{\"143\":1}}],[\"而\",{\"1\":{\"112\":1,\"204\":1}}],[\"而下图是具有零阶和一阶接近度的两条曲线\",{\"1\":{\"111\":1}}],[\"而函数\",{\"1\":{\"111\":1}}],[\"而泛函代表了函数到数的映射\",{\"1\":{\"109\":1}}],[\"记作\",{\"1\":{\"112\":1,\"113\":1}}],[\"记为神经网络使用的损失函数\",{\"1\":{\"221\":1}}],[\"记为模型得到的标签\",{\"1\":{\"221\":1}}],[\"记为模型的输入\",{\"1\":{\"221\":1}}],[\"记为\",{\"1\":{\"111\":1,\"113\":1,\"234\":1}}],[\"记得放辣油哦\",{\"1\":{\"46\":1}}],[\"邻域由所有位于\",{\"1\":{\"111\":1}}],[\"邻域由所有满足\",{\"1\":{\"111\":1}}],[\"邻域称为该函数的弱\",{\"1\":{\"111\":1}}],[\"邻域称为该函数的强\",{\"1\":{\"111\":1}}],[\"邻域的一部分\",{\"1\":{\"111\":1}}],[\"邻域是\",{\"1\":{\"111\":1}}],[\"邻域则由所有满足\",{\"1\":{\"111\":1}}],[\"邻域内的任一函数\",{\"1\":{\"111\":1}}],[\"邻域\",{\"1\":{\"111\":1}}],[\"邻域或弱邻域\",{\"1\":{\"111\":1}}],[\"邻域或强邻域\",{\"1\":{\"111\":1}}],[\"邻域或\",{\"1\":{\"111\":1}}],[\"级\",{\"1\":{\"111\":1}}],[\"所定义的问题称作和原问题\",{\"1\":{\"234\":1}}],[\"所确定的解\",{\"1\":{\"230\":1}}],[\"所有的\",{\"1\":{\"296\":1}}],[\"所有的隐变量构成一个马尔可夫链\",{\"1\":{\"206\":1}}],[\"所有的参数都是相关的\",{\"1\":{\"192\":1}}],[\"所有变量为二值变量\",{\"1\":{\"191\":1}}],[\"所有变量\",{\"1\":{\"191\":1,\"192\":1}}],[\"所以node能与流行假设优雅的进行交互\",{\"1\":{\"299\":1}}],[\"所以矛盾\",{\"1\":{\"299\":1}}],[\"所以若我们在renet层级间加入更多的层\",{\"1\":{\"290\":1}}],[\"所以关于\",{\"1\":{\"290\":1}}],[\"所以数值方法总与求解摄动问题有关\",{\"1\":{\"234\":1}}],[\"所以比较容易优化\",{\"1\":{\"221\":1}}],[\"所以称为head\",{\"1\":{\"202\":2}}],[\"所以可知\",{\"1\":{\"202\":3}}],[\"所以有4种组合\",{\"1\":{\"196\":1}}],[\"所以我们需要2个值\",{\"1\":{\"196\":1}}],[\"所以它可以取两个值\",{\"1\":{\"196\":1}}],[\"所以直接建模\",{\"1\":{\"166\":1}}],[\"所以index不会移动\",{\"1\":{\"144\":1}}],[\"所以\",{\"1\":{\"111\":1,\"144\":2,\"196\":2,\"235\":1}}],[\"所以你无需担心它的长度\",{\"1\":{\"8\":1}}],[\"所组成\",{\"1\":{\"111\":2}}],[\"所组成的集合称为函数\",{\"1\":{\"111\":1}}],[\"阶的\",{\"1\":{\"111\":3}}],[\"阶\",{\"1\":{\"111\":3}}],[\"阶距离小于正数\",{\"1\":{\"111\":1}}],[\"阶导数\",{\"1\":{\"111\":1}}],[\"函数不能被表示\",{\"1\":{\"299\":1}}],[\"函数是带参数的神\",{\"1\":{\"290\":1}}],[\"函数是为了得到一个大小为\",{\"1\":{\"221\":1}}],[\"函数以作为特征输入可以看成一个神经网络\",{\"1\":{\"290\":1}}],[\"函数取梯度的符号\",{\"1\":{\"221\":1}}],[\"函数来计算条件概率\",{\"1\":{\"204\":1}}],[\"函数仍是原来的函数\",{\"1\":{\"112\":1}}],[\"函数的增量\",{\"1\":{\"112\":1}}],[\"函数的变分\",{\"1\":{\"112\":1}}],[\"函数发生了改变\",{\"1\":{\"112\":1}}],[\"函数\",{\"1\":{\"111\":5,\"112\":1,\"204\":1,\"300\":1}}],[\"函数间的距离\",{\"0\":{\"111\":1}}],[\"函数代表了数到数的映射\",{\"1\":{\"109\":1}}],[\"基本概念\",{\"0\":{\"110\":1,\"215\":1}}],[\"基础概论\",{\"0\":{\"107\":1}}],[\"即高维数据通常存在于低维流形中\",{\"1\":{\"299\":1}}],[\"即高维数据的参数估计十分困难\",{\"1\":{\"241\":1}}],[\"即初始条件的微小扰动可能导致完全不同的轨迹\",{\"1\":{\"295\":1}}],[\"即我们用来训练的样本只包含部分的可观测变量\",{\"1\":{\"241\":1}}],[\"即如何选择数据分布的密度函数\",{\"1\":{\"241\":1}}],[\"即是说\",{\"1\":{\"230\":1}}],[\"即向量中的最大元素\",{\"1\":{\"221\":1}}],[\"即通过对输入图片进行一个微小的扰动\",{\"1\":{\"215\":1}}],[\"即团内的所有节点之间都连边\",{\"1\":{\"208\":1}}],[\"即一个变量\",{\"1\":{\"207\":1}}],[\"即不存在其他变量使得这两个节点对应的变\",{\"1\":{\"201\":1}}],[\"即建模输入和输出之间的条件概率分布\",{\"1\":{\"198\":1}}],[\"即参数估计问题\",{\"1\":{\"197\":1}}],[\"即有\",{\"1\":{\"196\":2}}],[\"即有部分变量是不可观测的\",{\"1\":{\"193\":1}}],[\"即对于凹函数\",{\"1\":{\"194\":1}}],[\"即固定其他参数\",{\"1\":{\"192\":1}}],[\"即所有变量都是可观测的\",{\"1\":{\"190\":1}}],[\"即已知网络结构\",{\"1\":{\"189\":1}}],[\"即寻找最优的网络结构\",{\"1\":{\"189\":1}}],[\"即采样\",{\"1\":{\"167\":1}}],[\"即\",{\"1\":{\"112\":1,\"113\":1,\"194\":2,\"195\":2,\"196\":1,\"202\":2,\"205\":1,\"208\":1,\"221\":1,\"230\":1,\"241\":1,\"290\":1}}],[\"即函数导数的变分等于函数变分的导数\",{\"1\":{\"112\":1}}],[\"即它们在区间的端点的值都相等\",{\"1\":{\"112\":1}}],[\"即距离也会不同\",{\"1\":{\"109\":1}}],[\"即给定一个函数\",{\"1\":{\"109\":1}}],[\"即可以向此元素进行放大\",{\"1\":{\"40\":1}}],[\"即可在幻灯片获得焦点时进入全屏模式\",{\"1\":{\"38\":1}}],[\"即可在幻灯片获得焦点时进入预览模式\",{\"1\":{\"36\":1}}],[\"便得到\",{\"1\":{\"109\":1}}],[\"表明\",{\"1\":{\"299\":1}}],[\"表示和逼近\",{\"1\":{\"293\":4}}],[\"表示和逼近能力\",{\"0\":{\"297\":1},\"1\":{\"286\":1}}],[\"表示网络的深度\",{\"1\":{\"290\":1}}],[\"表示能力等均和传统的神经网络例如cnn\",{\"1\":{\"286\":1}}],[\"表示初值问题\",{\"1\":{\"235\":1}}],[\"表示初始误差\",{\"1\":{\"234\":1}}],[\"表示在以后的\",{\"1\":{\"234\":1}}],[\"表示矩阵\",{\"1\":{\"225\":1}}],[\"表示第\",{\"1\":{\"225\":1}}],[\"表示目标输出\",{\"1\":{\"221\":1}}],[\"表示输入样本\",{\"1\":{\"221\":1}}],[\"表示模型参数\",{\"1\":{\"221\":1}}],[\"表示除\",{\"1\":{\"207\":1}}],[\"表示随机变量\",{\"1\":{\"207\":1}}],[\"表示每个随机变量的局部条件概率分布\",{\"1\":{\"202\":1}}],[\"表示两个随机变量\",{\"1\":{\"202\":1}}],[\"表示对应的两个变量为因果关系\",{\"1\":{\"201\":1}}],[\"表示问题\",{\"1\":{\"197\":1}}],[\"表示变量\",{\"1\":{\"196\":1,\"202\":1}}],[\"表示样本\",{\"1\":{\"195\":1}}],[\"表示为\",{\"1\":{\"172\":1}}],[\"表示\",{\"1\":{\"109\":1,\"202\":1,\"206\":1}}],[\"表格和分割线\",{\"1\":{\"12\":1}}],[\"区间问题\",{\"0\":{\"152\":1}}],[\"区间内的弧长相加\",{\"1\":{\"109\":1}}],[\"区间\",{\"1\":{\"109\":1}}],[\"区间分成很多个小的\",{\"1\":{\"109\":1}}],[\"许多数学物理问题都涉及到能够以积分形式表示的量的最小化或者最大化\",{\"1\":{\"109\":1}}],[\"zhihu\",{\"1\":{\"109\":2,\"113\":1,\"194\":1,\"200\":1,\"284\":1}}],[\"zoom\",{\"1\":{\"29\":1}}],[\"知乎\",{\"1\":{\"109\":2,\"113\":1,\"194\":1,\"200\":1,\"284\":1}}],[\"变量\",{\"1\":{\"196\":1}}],[\"变分自编码器和生成对抗网络\",{\"1\":{\"167\":1}}],[\"变分被积函数或拉格朗日函数\",{\"1\":{\"113\":1}}],[\"变分符号\",{\"1\":{\"112\":1}}],[\"变分符号或变分算子\",{\"1\":{\"112\":1}}],[\"变分\",{\"0\":{\"112\":1}}],[\"变分计算1\",{\"1\":{\"109\":1}}],[\"变分法\",{\"0\":{\"109\":1}}],[\"变红\",{\"1\":{\"25\":1}}],[\"跳转单词\",{\"1\":{\"102\":1,\"184\":1}}],[\"number\",{\"1\":{\"290\":1}}],[\"nums\",{\"1\":{\"150\":4,\"151\":9}}],[\"nf\",{\"1\":{\"283\":1}}],[\"n\",{\"1\":{\"213\":1}}],[\"nb\",{\"1\":{\"205\":1}}],[\"naive\",{\"1\":{\"205\":1}}],[\"negative\",{\"1\":{\"226\":1}}],[\"neural综述\",{\"1\":{\"296\":1}}],[\"neuralode的一个优点就在于\",{\"1\":{\"300\":1}}],[\"neuralode\",{\"1\":{\"286\":1}}],[\"neural\",{\"1\":{\"224\":1,\"286\":1,\"290\":1,\"294\":2,\"296\":1,\"311\":1}}],[\"networks\",{\"1\":{\"213\":1}}],[\"network\",{\"1\":{\"202\":1,\"204\":1,\"207\":1,\"224\":1}}],[\"newinterval\",{\"1\":{\"153\":5,\"154\":1}}],[\"new\",{\"1\":{\"147\":1}}],[\"n=k\",{\"1\":{\"150\":1}}],[\"nlogn\",{\"1\":{\"143\":1}}],[\"npm\",{\"1\":{\"99\":2,\"181\":2}}],[\"not\",{\"1\":{\"294\":1}}],[\"notation\",{\"0\":{\"225\":1,\"298\":1},\"1\":{\"194\":1}}],[\"now\",{\"1\":{\"290\":1}}],[\"node描述了输入的流形如何随着深度流动到输出流形\",{\"1\":{\"299\":1}}],[\"node会连续的变形输入空间而不会撕裂一个连接的区域\",{\"1\":{\"299\":1}}],[\"node的输入是被前神经网络处理过的维度较低的数据\",{\"1\":{\"290\":1}}],[\"node模型的思想来源\",{\"1\":{\"286\":1}}],[\"node从网络架构\",{\"1\":{\"286\":1}}],[\"non\",{\"1\":{\"217\":1}}],[\"none\",{\"1\":{\"29\":1}}],[\"no\",{\"1\":{\"79\":1}}],[\"选项卡\",{\"0\":{\"99\":1,\"181\":1}}],[\"选项全局设置\",{\"1\":{\"29\":1}}],[\"```bash\",{\"1\":{\"99\":3,\"181\":3}}],[\"```\",{\"1\":{\"93\":1,\"99\":3,\"176\":1,\"181\":3}}],[\"```js\",{\"1\":{\"93\":1,\"176\":1}}],[\"`代码`\",{\"1\":{\"93\":1,\"176\":1}}],[\"测试\",{\"0\":{\"91\":1}}],[\"此表达式读作\",{\"1\":{\"234\":1}}],[\"此时空间的每一点都有一条且只有一条积分曲线经过\",{\"1\":{\"231\":1}}],[\"此规则并不是强加上去的\",{\"1\":{\"202\":1}}],[\"此页面应当包含\",{\"1\":{\"88\":1}}],[\"此文字有脚注\",{\"1\":{\"68\":1,\"103\":1,\"185\":1}}],[\"徽章\",{\"1\":{\"87\":1}}],[\"徽章文字\",{\"1\":{\"87\":1}}],[\"但实际定理只是给出了充分条件\",{\"1\":{\"300\":1}}],[\"但对某些任务来说是劣势\",{\"1\":{\"299\":1}}],[\"但对于一个高维空间中的复杂分布\",{\"1\":{\"167\":1}}],[\"但它们实际上可以由较少的自由度来描述\",{\"1\":{\"299\":1}}],[\"但它不是\",{\"1\":{\"112\":1}}],[\"但在实际操作中\",{\"1\":{\"295\":1}}],[\"但足以欺骗神经网络\",{\"1\":{\"221\":1}}],[\"但其的鲁棒性却可能很差\",{\"1\":{\"215\":1}}],[\"但是维数也变成了一个不可控量\",{\"1\":{\"300\":1}}],[\"但是可以通过增加维度来做到\",{\"1\":{\"300\":1}}],[\"但是为了表示\",{\"1\":{\"296\":1}}],[\"但是它也有很大的缺陷\",{\"1\":{\"293\":1}}],[\"但是网络架构是任意的\",{\"1\":{\"290\":1}}],[\"但是由于维度巨大\",{\"1\":{\"290\":1}}],[\"但是在训练过程中\",{\"1\":{\"300\":1}}],[\"但是在分析从它的应用所产生的误差方面它是一个重要的基础\",{\"1\":{\"235\":1}}],[\"但是在实际应用中\",{\"1\":{\"205\":1}}],[\"但是几乎在每一种情况下都使用\",{\"1\":{\"234\":1}}],[\"但是线性\",{\"1\":{\"221\":1}}],[\"但是这篇论文证明神经网络的线性性质是造成神经网络具有对抗样本的主要原因\",{\"1\":{\"219\":1}}],[\"但是并不一定是因果关系\",{\"1\":{\"201\":1}}],[\"但是无法观测到具体由哪个分布生成\",{\"1\":{\"195\":1}}],[\"但是条件概率表需要的参数比较多\",{\"1\":{\"191\":1}}],[\"但是\",{\"1\":{\"166\":1,\"196\":2,\"204\":1,\"220\":1}}],[\"但是你需要使用相对链接\",{\"1\":{\"87\":1}}],[\"但只要求出了\",{\"1\":{\"113\":1}}],[\"但相当于其他的省博物馆\",{\"1\":{\"44\":1}}],[\"分段的node\",{\"0\":{\"296\":1}}],[\"分类器是一类简单的概率分类器\",{\"1\":{\"205\":1}}],[\"分类为\",{\"1\":{\"86\":1}}],[\"分别是初值问题和的两个解\",{\"1\":{\"299\":1}}],[\"分别计算在每个求解相应的ode\",{\"1\":{\"296\":1}}],[\"分别求上式关于\",{\"1\":{\"240\":1,\"241\":1}}],[\"分别求拉格朗日函数\",{\"1\":{\"195\":1}}],[\"分别为正态分布的均值和方差\",{\"1\":{\"240\":1}}],[\"分别为可观测变量和隐变量的取值\",{\"1\":{\"206\":1}}],[\"分别为第\",{\"1\":{\"195\":1}}],[\"分别表示两类条件概率的参数\",{\"1\":{\"206\":1}}],[\"分别表示了四个变量\",{\"1\":{\"201\":1}}],[\"分别表示\",{\"1\":{\"113\":1}}],[\"分发饼干\",{\"0\":{\"141\":1},\"1\":{\"141\":1}}],[\"分割垂直幻灯片\",{\"1\":{\"5\":1}}],[\"日\",{\"1\":{\"86\":1}}],[\"日场\",{\"1\":{\"46\":1}}],[\"月\",{\"1\":{\"86\":1}}],[\"年提出\",{\"1\":{\"221\":1}}],[\"年\",{\"1\":{\"86\":1}}],[\"写作日期为\",{\"1\":{\"86\":1}}],[\"↩︎\",{\"1\":{\"82\":1,\"104\":1,\"186\":1,\"213\":2,\"286\":1,\"296\":1}}],[\"操作\",{\"1\":{\"79\":1}}],[\"with\",{\"1\":{\"311\":1}}],[\"window\",{\"1\":{\"147\":1}}],[\"weierstrass\",{\"1\":{\"300\":1}}],[\"we\",{\"1\":{\"226\":1,\"296\":2}}],[\"wed\",{\"1\":{\"78\":1}}],[\"which\",{\"1\":{\"296\":1}}],[\"white\",{\"1\":{\"217\":1}}],[\"whether\",{\"1\":{\"294\":1}}],[\"where\",{\"1\":{\"72\":1}}],[\"www\",{\"1\":{\"213\":1}}],[\"word\",{\"1\":{\"102\":2}}],[\"warning\",{\"1\":{\"95\":1,\"178\":1}}],[\"p\",{\"1\":{\"298\":1}}],[\"parametric\",{\"1\":{\"239\":1}}],[\"partition\",{\"1\":{\"208\":1}}],[\"paper\",{\"1\":{\"226\":1}}],[\"pgd\",{\"0\":{\"223\":1},\"1\":{\"216\":1,\"218\":1}}],[\"pgm\",{\"1\":{\"196\":1}}],[\"preprint\",{\"1\":{\"311\":1}}],[\"preliminaries\",{\"0\":{\"284\":1}}],[\"principles\",{\"1\":{\"213\":1}}],[\"proof\",{\"1\":{\"296\":2}}],[\"property\",{\"1\":{\"294\":1}}],[\"probability\",{\"1\":{\"202\":1}}],[\"probabilistic\",{\"1\":{\"167\":1,\"196\":1,\"213\":1,\"238\":1,\"242\":1}}],[\"programmercarl\",{\"1\":{\"142\":1}}],[\"process=>operation\",{\"1\":{\"79\":1}}],[\"pdf\",{\"1\":{\"213\":1}}],[\"pii\",{\"1\":{\"213\":1}}],[\"pietra\",{\"1\":{\"210\":1}}],[\"posts\",{\"0\":{\"312\":1}}],[\"position\",{\"1\":{\"77\":1}}],[\"potential\",{\"1\":{\"208\":1}}],[\"push\",{\"1\":{\"154\":5}}],[\"public\",{\"1\":{\"87\":1,\"143\":1,\"145\":1,\"148\":2,\"150\":1,\"151\":1,\"154\":1}}],[\"pnpm\",{\"1\":{\"99\":2,\"181\":2}}],[\"plate\",{\"1\":{\"194\":1}}],[\"placed=true\",{\"1\":{\"154\":1}}],[\"placed=false\",{\"1\":{\"154\":1}}],[\"placed\",{\"1\":{\"154\":2}}],[\"playground\",{\"1\":{\"82\":1}}],[\"plugin\",{\"1\":{\"62\":1}}],[\"58\",{\"1\":{\"195\":1}}],[\"5\",{\"1\":{\"77\":4,\"148\":5,\"153\":10,\"234\":1,\"235\":2,\"236\":1}}],[\"yarn\",{\"1\":{\"99\":2,\"181\":2}}],[\"yaxis\",{\"1\":{\"78\":1}}],[\"yes\",{\"1\":{\"79\":1}}],[\"y\",{\"1\":{\"77\":4}}],[\"散点数据集\",{\"1\":{\"77\":1}}],[\"图\",{\"1\":{\"207\":1}}],[\"图中带阴影的节点表示可观测到的变量\",{\"1\":{\"201\":1}}],[\"图中每个节点表示一个变量\",{\"1\":{\"196\":1}}],[\"图1给出了两个代表性图模型\",{\"1\":{\"201\":1}}],[\"图11\",{\"1\":{\"196\":1,\"206\":1,\"210\":1}}],[\"图由一组节点和节点之间的边组成\",{\"1\":{\"201\":1}}],[\"图模型越来越多地用来设计和分析各种学习算法\",{\"1\":{\"198\":1}}],[\"图模型提供了一种新的角度来解释机器学习模型\",{\"1\":{\"198\":1}}],[\"图模型与机器学习\",{\"0\":{\"198\":1}}],[\"图模型有三个基本问题\",{\"1\":{\"197\":1}}],[\"图模型的学习包括图结构的学习和参数的学习\",{\"1\":{\"197\":1}}],[\"图模型的学习可以分为两部分\",{\"1\":{\"189\":1}}],[\"图模型的基本问题\",{\"0\":{\"197\":1}}],[\"图模型的参数估计问题又分为不包含隐变量时的参数估计问题和包含隐变量时的参数估计问题\",{\"1\":{\"189\":1}}],[\"图表\",{\"0\":{\"77\":1}}],[\"图片增强\",{\"0\":{\"71\":1}}],[\"交互演示\",{\"0\":{\"76\":1,\"82\":1},\"1\":{\"82\":1}}],[\"捐赠一杯咖啡\",{\"1\":{\"75\":1}}],[\"向图中的局部马尔可夫性质可以表示为\",{\"1\":{\"207\":1}}],[\"向量符号一般用黑斜体小写英文字母\",{\"1\":{\"172\":1}}],[\"向量\",{\"0\":{\"172\":1},\"1\":{\"172\":1,\"241\":1}}],[\"向量和向量空间\",{\"0\":{\"171\":1}}],[\"向量空间\",{\"0\":{\"173\":1},\"1\":{\"170\":1}}],[\"向\",{\"1\":{\"75\":1}}],[\"导致梯度估计的误差\",{\"1\":{\"288\":1}}],[\"导致推出循环\",{\"1\":{\"148\":1}}],[\"导入文件\",{\"0\":{\"73\":1}}],[\"导航栏\",{\"1\":{\"51\":1,\"88\":1}}],[\"组件\",{\"0\":{\"72\":1}}],[\"支持为图片设置颜色模式和大小\",{\"1\":{\"71\":1}}],[\"计算的时间复杂仍然很高\",{\"1\":{\"290\":1}}],[\"计算其他变量的条件概率分布\",{\"1\":{\"197\":1}}],[\"计算过程\",{\"1\":{\"196\":1}}],[\"计算过程如下\",{\"1\":{\"196\":1}}],[\"计算\",{\"1\":{\"195\":2}}],[\"计算后验分布\",{\"1\":{\"195\":1}}],[\"计算起来还比较容易\",{\"1\":{\"194\":1}}],[\"计算期望\",{\"1\":{\"192\":1}}],[\"计算机\",{\"0\":{\"47\":1}}],[\"计划二\",{\"1\":{\"104\":1,\"186\":1}}],[\"计划1\",{\"1\":{\"104\":1,\"186\":1}}],[\"计划\",{\"1\":{\"70\":2}}],[\"脚注\",{\"0\":{\"68\":1,\"103\":1,\"185\":1}}],[\"单词\",{\"1\":{\"67\":1,\"102\":2,\"184\":1}}],[\"我们支持\",{\"1\":{\"304\":1,\"308\":1}}],[\"我们从输出出发得到隐状态层\",{\"1\":{\"290\":1}}],[\"我们证明了其性能较差的一个原因是现有的梯度估计方法的不准确性\",{\"1\":{\"288\":1}}],[\"我们通常希望对输入的扰动\",{\"1\":{\"221\":1}}],[\"我们通常通过引入隐变量\",{\"1\":{\"166\":1}}],[\"我们希望\",{\"1\":{\"221\":1}}],[\"我们来形式化这个直觉\",{\"1\":{\"221\":1}}],[\"我们把这个叫做fgsm\",{\"1\":{\"221\":1}}],[\"我们会很难优化\",{\"1\":{\"221\":1}}],[\"我们已知的一些模型\",{\"1\":{\"221\":1}}],[\"我们假设神经网络是十分线性的\",{\"1\":{\"221\":1}}],[\"我们得出了一个很快的生成对抗样本的方法\",{\"1\":{\"221\":1}}],[\"我们基于线性的假设更简单\",{\"1\":{\"220\":1}}],[\"我们知道\",{\"1\":{\"220\":1}}],[\"我们举个例子说明一下\",{\"1\":{\"202\":1}}],[\"我们在证明此现象存在后\",{\"1\":{\"202\":1}}],[\"我们只要将\",{\"1\":{\"220\":1}}],[\"我们只关注在给定图结构时的参数学习\",{\"1\":{\"197\":1}}],[\"我们只需要知道其中一个值\",{\"1\":{\"196\":1}}],[\"我们可以减少增加维数\",{\"1\":{\"300\":1}}],[\"我们可以提高找到全局最小或更好局部最小的机会\",{\"1\":{\"295\":1}}],[\"我们可以总结一个规律\",{\"1\":{\"202\":1}}],[\"我们可以使用图结构的方式将概率模型可视化\",{\"1\":{\"196\":1}}],[\"我们可以利用神经网络来进行建模\",{\"1\":{\"166\":1}}],[\"我们需要知道的一个值的概率\",{\"1\":{\"196\":1}}],[\"我们将这些参数加起来得到总数\",{\"1\":{\"196\":1}}],[\"我们将分别计算每个条件概率所需的参数数量\",{\"1\":{\"196\":1}}],[\"我们将图像表示为一个随机向量\",{\"1\":{\"166\":1}}],[\"我们用两个高斯分布来估计这组数据的分布情况\",{\"1\":{\"195\":1}}],[\"我们引入一个隐变量\",{\"1\":{\"195\":1}}],[\"我们引入一个额外的变分函数\",{\"1\":{\"194\":1}}],[\"我在右对齐\",{\"1\":{\"66\":1,\"100\":2,\"182\":2}}],[\"我是居中的\",{\"1\":{\"66\":1,\"100\":2,\"182\":2}}],[\"我是coding\",{\"1\":{\"1\":1}}],[\"查看详情\",{\"1\":{\"63\":1,\"64\":1,\"65\":1,\"66\":1,\"67\":1,\"68\":1,\"69\":1,\"70\":1,\"71\":1,\"72\":1,\"73\":1,\"74\":1,\"75\":1,\"76\":1,\"77\":1,\"78\":1,\"79\":1,\"80\":1,\"81\":1,\"104\":1,\"186\":1}}],[\"危险容器\",{\"0\":{\"96\":1,\"179\":1},\"1\":{\"63\":1,\"96\":2,\"179\":2}}],[\"警告容器\",{\"0\":{\"95\":1,\"178\":1},\"1\":{\"63\":1,\"95\":2,\"178\":2}}],[\"与任意序列\",{\"1\":{\"234\":1}}],[\"与前两种情况刚好相反\",{\"1\":{\"202\":1}}],[\"与导数符号\",{\"1\":{\"112\":1}}],[\"与函数增量\",{\"1\":{\"112\":1}}],[\"与另一函数\",{\"1\":{\"112\":1}}],[\"与另一宗量\",{\"1\":{\"112\":1}}],[\"与另一可取函数\",{\"1\":{\"112\":2}}],[\"与\",{\"1\":{\"63\":1,\"93\":2,\"111\":1,\"112\":2,\"113\":5,\"153\":1,\"176\":2,\"202\":4,\"218\":1}}],[\"vector\",{\"1\":{\"172\":1}}],[\"vector<vector<int>>\",{\"1\":{\"154\":3}}],[\"vector<int>\",{\"1\":{\"143\":2,\"145\":2,\"148\":4,\"150\":2,\"151\":1,\"154\":1}}],[\"void\",{\"1\":{\"150\":1,\"151\":1}}],[\"vue\",{\"0\":{\"82\":1},\"1\":{\"82\":2}}],[\"vuepress\",{\"0\":{\"61\":1},\"1\":{\"57\":2,\"59\":2,\"60\":1,\"61\":2,\"62\":1,\"87\":1,\"99\":3,\"181\":3}}],[\"validmountainarray\",{\"1\":{\"148\":1}}],[\"value\",{\"1\":{\"78\":1,\"294\":1}}],[\"variable\",{\"1\":{\"63\":1}}],[\"安全的在\",{\"1\":{\"63\":1}}],[\"提高模型的表达能力的同时\",{\"1\":{\"294\":1}}],[\"提示\",{\"1\":{\"87\":1}}],[\"提示容器\",{\"0\":{\"63\":1,\"92\":1,\"94\":1,\"177\":1},\"1\":{\"63\":1,\"93\":1,\"94\":2,\"176\":1,\"177\":2}}],[\"提供更加丰富的写作功能\",{\"1\":{\"62\":1}}],[\"主题包含了一个自定义徽章可以使用\",{\"1\":{\"87\":1}}],[\"主题扩展了更多\",{\"1\":{\"62\":1}}],[\"主题扩展\",{\"0\":{\"62\":1}}],[\"主要从\",{\"1\":{\"57\":1}}],[\"主要功能与配置演示\",{\"0\":{\"48\":1}}],[\"对单项式进行组合以逼近目标连续函数\",{\"1\":{\"300\":1}}],[\"对模型的表达能力很低\",{\"1\":{\"299\":1}}],[\"对输入做仿射变换\",{\"0\":{\"294\":1}}],[\"对ode的研究不管是理论还是数值分析\",{\"1\":{\"290\":1}}],[\"对所有\",{\"1\":{\"235\":1}}],[\"对一切\",{\"1\":{\"234\":1,\"235\":1}}],[\"对一个简单的线性网络来说\",{\"1\":{\"220\":1}}],[\"对一个更一般的贝叶斯网络\",{\"1\":{\"202\":1}}],[\"对任何同胚映射\",{\"1\":{\"300\":1}}],[\"对任何\",{\"1\":{\"234\":1}}],[\"对大的\",{\"1\":{\"234\":1}}],[\"对某个\",{\"1\":{\"234\":1}}],[\"对maxout网络\",{\"1\":{\"221\":1}}],[\"对softmax分类器攻击达到了99\",{\"1\":{\"221\":1}}],[\"对人类的感知影响不大\",{\"1\":{\"221\":1}}],[\"对抗扰动让网络的激励增加了\",{\"1\":{\"220\":1}}],[\"对抗样本的线性解释\",{\"0\":{\"220\":1}}],[\"对抗样本\",{\"1\":{\"218\":1,\"228\":1}}],[\"对抗鲁棒性\",{\"0\":{\"216\":1}}],[\"对抗攻击篇\",{\"1\":{\"218\":1}}],[\"对抗攻击的分类\",{\"0\":{\"217\":1}}],[\"对抗攻击\",{\"0\":{\"214\":1}}],[\"对每个样本\",{\"1\":{\"195\":1}}],[\"对数线性模型也称为条件最大熵模型或softmax\",{\"1\":{\"210\":1}}],[\"对数线性模型\",{\"0\":{\"210\":1}}],[\"对数边际分布\",{\"1\":{\"195\":1}}],[\"对数边际似然函数\",{\"1\":{\"194\":1}}],[\"对数组进行排序\",{\"1\":{\"148\":1}}],[\"对比公式\",{\"1\":{\"192\":1}}],[\"对偶理论到wgan\",{\"1\":{\"113\":1}}],[\"对齐\",{\"0\":{\"100\":1,\"182\":1}}],[\"对于可测映射\",{\"1\":{\"298\":1}}],[\"对于非线性系统\",{\"1\":{\"295\":1}}],[\"对于给定的初值\",{\"1\":{\"294\":1}}],[\"对于node的表示能力用严格的数学理论进行详细论述\",{\"1\":{\"286\":1}}],[\"对于图中的\",{\"1\":{\"207\":1}}],[\"对于变量\",{\"1\":{\"204\":1}}],[\"对于每种组合\",{\"1\":{\"196\":1}}],[\"对于每个的值\",{\"1\":{\"196\":1}}],[\"对于每个团\",{\"1\":{\"192\":1}}],[\"对于的每个值\",{\"1\":{\"196\":1}}],[\"对于一般的无向图模型\",{\"1\":{\"192\":1}}],[\"对于一个可微分函数\",{\"1\":{\"221\":1}}],[\"对于一个多分类网络\",{\"1\":{\"217\":1}}],[\"对于一个随机向量\",{\"1\":{\"207\":1}}],[\"对于一个概率模型\",{\"1\":{\"197\":1}}],[\"对于一个\",{\"1\":{\"109\":1,\"196\":1,\"202\":1}}],[\"对于任意\",{\"1\":{\"113\":1}}],[\"对于任意定值\",{\"1\":{\"112\":2}}],[\"对于任意一条路径\",{\"1\":{\"109\":1}}],[\"对于\",{\"1\":{\"87\":1,\"236\":1}}],[\"对\",{\"1\":{\"61\":1,\"109\":1,\"235\":1,\"236\":1}}],[\"为数据集中取值为第\",{\"1\":{\"241\":1}}],[\"为从某个未知分布中独立抽取的\",{\"1\":{\"239\":1}}],[\"为推导\",{\"1\":{\"235\":1}}],[\"为大于零的某个数\",{\"1\":{\"234\":1}}],[\"为坐标的\",{\"1\":{\"231\":1}}],[\"为不稳定的\",{\"1\":{\"230\":1}}],[\"为全局渐近稳定的或简称全局稳定的\",{\"1\":{\"230\":1}}],[\"为渐近稳定的\",{\"1\":{\"230\":1}}],[\"为稳定的\",{\"1\":{\"230\":1}}],[\"为权重向量\",{\"1\":{\"210\":1}}],[\"为能量函数\",{\"1\":{\"208\":1}}],[\"为随机向量\",{\"1\":{\"208\":1}}],[\"为除\",{\"1\":{\"207\":1}}],[\"为变量\",{\"1\":{\"207\":1}}],[\"为隐变量\",{\"1\":{\"206\":1}}],[\"为可观测变量\",{\"1\":{\"206\":1}}],[\"为离散值\",{\"1\":{\"205\":1}}],[\"为连续值\",{\"1\":{\"205\":1}}],[\"为概率分布的参数\",{\"1\":{\"205\":1}}],[\"为和参数无关的常数\",{\"1\":{\"195\":1}}],[\"为高斯分布\",{\"1\":{\"195\":1}}],[\"为多项分布的参数\",{\"1\":{\"195\":1}}],[\"为对数边际似然函数\",{\"1\":{\"194\":1}}],[\"为定义在\",{\"1\":{\"210\":1}}],[\"为定义在隐变量\",{\"1\":{\"194\":1}}],[\"为定义在团\",{\"1\":{\"192\":1}}],[\"为模型参数\",{\"1\":{\"194\":1}}],[\"为模型中的所有参数\",{\"1\":{\"191\":1}}],[\"为第\",{\"1\":{\"191\":1,\"241\":1}}],[\"为偶数时\",{\"1\":{\"148\":1}}],[\"为奇数时\",{\"1\":{\"148\":1}}],[\"为了最大化损失增量\",{\"1\":{\"221\":1}}],[\"为了描述方便\",{\"1\":{\"206\":1}}],[\"为了减少模型参数\",{\"1\":{\"204\":1}}],[\"为了减少参数数量\",{\"1\":{\"191\":1}}],[\"为了计算\",{\"1\":{\"194\":1}}],[\"为了满足更多的小孩\",{\"1\":{\"142\":1}}],[\"为了丰富文档写作\",{\"1\":{\"61\":1}}],[\"为\",{\"1\":{\"112\":1,\"113\":1,\"194\":1,\"202\":1,\"208\":1}}],[\"为拉格朗日引进的一个小参数\",{\"1\":{\"112\":1}}],[\"为每个元素返回\",{\"1\":{\"221\":1}}],[\"为每个\",{\"1\":{\"59\":1}}],[\"内容\",{\"1\":{\"60\":1}}],[\"会带来如下问题\",{\"1\":{\"300\":1}}],[\"会导致模型的损失增加\",{\"1\":{\"221\":1}}],[\"会让模型更容易受到攻击\",{\"1\":{\"221\":1}}],[\"会让文字在不超出幻灯片范围的情况下尽可能大\",{\"1\":{\"14\":1}}],[\"会使用\",{\"1\":{\"60\":1}}],[\"扩展\",{\"0\":{\"60\":1,\"61\":1},\"1\":{\"61\":1}}],[\"是机器学习和模式识别领域中的一个重要概念\",{\"1\":{\"299\":1}}],[\"是双射\",{\"1\":{\"299\":1}}],[\"是同胚映射\",{\"1\":{\"299\":1}}],[\"是的意义下的通用逼近器\",{\"1\":{\"298\":1}}],[\"是euclidean\",{\"1\":{\"298\":1}}],[\"是否使用随机噪声以及具体的噪声水平通常是基于经验和实验调整的\",{\"1\":{\"295\":1}}],[\"是否执行操作\",{\"1\":{\"79\":1}}],[\"是网络层数\",{\"1\":{\"290\":1}}],[\"是网络参数\",{\"1\":{\"221\":1}}],[\"是连续是非常合理的\",{\"1\":{\"290\":1}}],[\"是连续的且在\",{\"1\":{\"234\":1}}],[\"是连续的\",{\"1\":{\"191\":1,\"234\":1,\"235\":1,\"290\":1,\"299\":3}}],[\"是该网络架构的参数\",{\"1\":{\"290\":1}}],[\"是没有梯度信息的\",{\"1\":{\"285\":1}}],[\"是将一组样本根据一定的准则划分到不同的组\",{\"1\":{\"242\":1}}],[\"是指从无标签的数据中学习出一些有用的模式\",{\"1\":{\"242\":1}}],[\"是指一种用图结构来描述多元随机变量之间条件独立关系的概率模型\",{\"1\":{\"196\":1}}],[\"是根据一组训练样本来估计样本空间的概率密度\",{\"1\":{\"242\":1}}],[\"是根据先验知识假设随机变量服从某种分布\",{\"1\":{\"239\":1}}],[\"是根据数据集\",{\"1\":{\"166\":1}}],[\"是基于一些观测样本来估计一个随机变量的概率密度函数\",{\"1\":{\"238\":1}}],[\"是适定的\",{\"1\":{\"234\":1}}],[\"是预激活值\",{\"1\":{\"225\":1}}],[\"是损失函数\",{\"1\":{\"221\":1}}],[\"是在给定大小限制下\",{\"1\":{\"221\":1}}],[\"是真实标签\",{\"1\":{\"221\":1}}],[\"是会被忽略的\",{\"1\":{\"220\":1}}],[\"是配分函数\",{\"1\":{\"208\":1}}],[\"是定义在团\",{\"1\":{\"208\":1}}],[\"是定义域为\",{\"1\":{\"111\":1}}],[\"是用来表示一种含有隐变量的马尔可夫过程\",{\"1\":{\"206\":1}}],[\"是条件概率分布\",{\"1\":{\"205\":1}}],[\"是可学习的参数\",{\"1\":{\"204\":1}}],[\"是我们用概率图的因子分解表示联合概率后\",{\"1\":{\"202\":1}}],[\"是一制度\",{\"1\":{\"290\":1}}],[\"是一系列高维数据上灵活可重参数化概率分布的方法\",{\"1\":{\"283\":1}}],[\"是一类用无向图来描述一组具有局部马尔可夫性质的随机向量\",{\"1\":{\"207\":1}}],[\"是一类用有向图来描述随机向量概率分布的模型\",{\"1\":{\"202\":1}}],[\"是一种通过使用神经网络参数化向量场来结合常微分方程和神经网络的一种全新的深度学习模型\",{\"1\":{\"286\":1}}],[\"是一种快速而简单的方法\",{\"1\":{\"221\":1}}],[\"是一种生成模型\",{\"1\":{\"204\":1}}],[\"是一种判别模型\",{\"1\":{\"204\":1}}],[\"是一个\",{\"1\":{\"302\":2}}],[\"是一个含参的仿射变换\",{\"1\":{\"294\":1}}],[\"是一个收玫于\",{\"1\":{\"234\":1}}],[\"是一个不依赖于\",{\"1\":{\"234\":1}}],[\"是一个小的扰动量\",{\"1\":{\"221\":1}}],[\"是一个小常数\",{\"1\":{\"221\":1}}],[\"是一个推断\",{\"1\":{\"194\":1}}],[\"是一个实数\",{\"1\":{\"172\":1}}],[\"是从无标签的训练数据中挖掘有效的特征或表示\",{\"1\":{\"242\":1}}],[\"是从哪个高斯分布生成的\",{\"1\":{\"195\":1}}],[\"是从\",{\"1\":{\"195\":1}}],[\"是有限的一维离散变量\",{\"1\":{\"194\":1}}],[\"是等于后验分布\",{\"1\":{\"194\":1}}],[\"是图模型中表示重复变量的方法\",{\"1\":{\"194\":1}}],[\"是离散的\",{\"1\":{\"191\":1}}],[\"是由于模型的线性所导致的\",{\"1\":{\"220\":1}}],[\"是由于自变量\",{\"1\":{\"112\":1}}],[\"是由多个高斯分布组成的模型\",{\"1\":{\"195\":1}}],[\"是由\",{\"1\":{\"172\":1,\"235\":1}}],[\"是由一组实数组成的有序数组\",{\"1\":{\"172\":1}}],[\"是概率统计和机器学习领域的一类重要模型\",{\"1\":{\"167\":1}}],[\"是符合条件才移动的\",{\"1\":{\"144\":1}}],[\"是固定移动的\",{\"1\":{\"144\":1}}],[\"是里的下标i\",{\"1\":{\"144\":1}}],[\"是先遍历的胃口\",{\"1\":{\"144\":1}}],[\"是关于\",{\"1\":{\"113\":1}}],[\"是三个独立变量\",{\"1\":{\"113\":1}}],[\"是两个不同函数\",{\"1\":{\"112\":1}}],[\"是泛函增量的主要部分\",{\"1\":{\"113\":1}}],[\"是泛函\",{\"1\":{\"112\":1}}],[\"是仅具有零阶接近度的两条曲线\",{\"1\":{\"111\":1}}],[\"是\",{\"1\":{\"59\":1,\"112\":1,\"113\":2,\"202\":2,\"204\":1,\"205\":1,\"221\":1,\"298\":1}}],[\"是国内保存zui完好的城墙堡垒\",{\"1\":{\"46\":1}}],[\"演示\",{\"1\":{\"58\":1}}],[\"请使用绝对链接\",{\"1\":{\"87\":1}}],[\"请阅读\",{\"1\":{\"61\":1}}],[\"请先阅读\",{\"1\":{\"58\":1}}],[\"请滚动鼠标滚轮进入下一页\",{\"1\":{\"3\":1}}],[\"以为输入\",{\"1\":{\"298\":1}}],[\"以找到最适合他们特定问题的方法\",{\"1\":{\"295\":1}}],[\"以后简称node\",{\"1\":{\"286\":1}}],[\"以收玫速度\",{\"1\":{\"234\":1}}],[\"以确保扰动是微小的\",{\"1\":{\"221\":1}}],[\"以下\",{\"1\":{\"202\":1}}],[\"以一种直观\",{\"1\":{\"196\":1}}],[\"以对数线性模型为例\",{\"1\":{\"192\":1}}],[\"以及初值条件也有可能存在误差\",{\"1\":{\"234\":1}}],[\"以及96\",{\"1\":{\"221\":1}}],[\"以及向量的线性变换和有限维的线性方程组\",{\"1\":{\"170\":1}}],[\"以及从中采样的\",{\"1\":{\"167\":1}}],[\"以及从中采样的一些\",{\"1\":{\"167\":1}}],[\"以及近似后验分布\",{\"1\":{\"166\":1}}],[\"以及所有不在\",{\"1\":{\"12\":1}}],[\"以上概念可以推广到多元函数的情形\",{\"1\":{\"111\":1}}],[\"以便当\",{\"1\":{\"148\":1}}],[\"以便\",{\"1\":{\"57\":1}}],[\"段落\",{\"1\":{\"54\":2}}],[\"返回顶部按钮\",{\"1\":{\"51\":1,\"88\":1}}],[\"页脚\",{\"1\":{\"51\":1,\"88\":1}}],[\"页面结构\",{\"0\":{\"88\":1}}],[\"页面内容\",{\"0\":{\"87\":1}}],[\"页面配置\",{\"0\":{\"85\":1},\"1\":{\"86\":1},\"2\":{\"90\":1}}],[\"页面引入配置\",{\"1\":{\"59\":1}}],[\"页面信息\",{\"0\":{\"86\":1},\"1\":{\"51\":1}}],[\"页面展示\",{\"1\":{\"49\":1,\"73\":1}}],[\"评论\",{\"1\":{\"51\":1,\"88\":1}}],[\"链接\",{\"1\":{\"51\":1,\"63\":1,\"93\":2,\"176\":2}}],[\"下一节中证明\",{\"1\":{\"298\":1}}],[\"下一篇\",{\"1\":{\"51\":1}}],[\"下图中\",{\"1\":{\"202\":1}}],[\"下图给出一个高斯混合模型训练过程的简单示例\",{\"1\":{\"195\":1}}],[\"下图给出了朴素贝叶斯分类器的图模型表示\",{\"1\":{\"205\":1}}],[\"下图给出了高斯混合模型的图模型表示\",{\"1\":{\"195\":1}}],[\"下图给出了带隐变量的贝叶斯网络的图模型结构\",{\"1\":{\"194\":1}}],[\"下图以手写体数字图像为例给出了生成模型的两个功能示例\",{\"1\":{\"167\":1}}],[\"下的期望\",{\"1\":{\"192\":1}}],[\"下的期望等于其在模型分布\",{\"1\":{\"192\":1}}],[\"下面这个定理说明通过将维度增加一倍数可以表示任何同胚映射\",{\"1\":{\"300\":1}}],[\"下面个各小节介绍一些方法来提高模型的表达能力\",{\"1\":{\"299\":1}}],[\"下面子节是其他的用于增维的方法来解决\",{\"1\":{\"293\":1}}],[\"下面给出一个最简单的一维上不能表示的函数\",{\"1\":{\"299\":1}}],[\"下面给出neural更一般形式的假设空间\",{\"1\":{\"298\":1}}],[\"下面给出更一般形式的含有node的网络架构\",{\"1\":{\"290\":1}}],[\"下面给出线性泛函的定义\",{\"1\":{\"113\":1}}],[\"下面将进一步证明\",{\"1\":{\"113\":1}}],[\"下午场\",{\"1\":{\"44\":1}}],[\"上计算\",{\"1\":{\"296\":1}}],[\"上满足常数为\",{\"1\":{\"235\":1}}],[\"上关于变量\",{\"1\":{\"234\":1}}],[\"上时\",{\"1\":{\"221\":1}}],[\"上\",{\"1\":{\"221\":1,\"234\":1}}],[\"上述的解释说明\",{\"1\":{\"220\":1}}],[\"上述变分的定义也可以推广到多元函数的情形\",{\"1\":{\"112\":1}}],[\"上达到极值\",{\"1\":{\"113\":1}}],[\"上面的性质可推广到高阶导数的变分情形\",{\"1\":{\"112\":1}}],[\"上下宽为\",{\"1\":{\"111\":1}}],[\"上下角标\",{\"0\":{\"65\":1}}],[\"上的势能函数\",{\"1\":{\"208\":1}}],[\"上的势能函数的参数\",{\"1\":{\"192\":1}}],[\"上的分布\",{\"1\":{\"194\":1}}],[\"上的特征向量\",{\"1\":{\"210\":1}}],[\"上的特征\",{\"1\":{\"192\":1}}],[\"上的变分\",{\"1\":{\"113\":1}}],[\"上的一阶变分或一次变分\",{\"1\":{\"113\":1}}],[\"上的已知函数\",{\"1\":{\"113\":1}}],[\"上的\",{\"1\":{\"111\":2,\"302\":1}}],[\"上的弧长\",{\"1\":{\"109\":1}}],[\"上有连续的\",{\"1\":{\"111\":1}}],[\"上一篇\",{\"1\":{\"51\":1}}],[\"上使用\",{\"1\":{\"40\":1}}],[\"编辑此页链接\",{\"1\":{\"51\":1}}],[\"贡献者\",{\"1\":{\"51\":1,\"88\":1}}],[\"侧边栏\",{\"1\":{\"51\":1,\"88\":1}}],[\"禁用\",{\"2\":{\"53\":1}}],[\"禁用了如下功能\",{\"1\":{\"51\":1}}],[\"禁用展示\",{\"1\":{\"49\":1,\"73\":1}}],[\"展示\",{\"0\":{\"57\":1},\"1\":{\"49\":1,\"73\":1}}],[\"感受下夫子庙接踵摩肩的热闹\",{\"1\":{\"46\":1}}],[\"感受南京人的后花园～\",{\"1\":{\"45\":1}}],[\"文本\",{\"1\":{\"167\":1}}],[\"文章标题列表\",{\"1\":{\"88\":1}}],[\"文章加密\",{\"2\":{\"56\":1}}],[\"文件夹的图片\",{\"1\":{\"87\":1}}],[\"文件放置在一起\",{\"1\":{\"87\":1}}],[\"文件\",{\"1\":{\"57\":1}}],[\"文件生成页面\",{\"1\":{\"57\":1}}],[\"文字结尾应该有深蓝色的\",{\"1\":{\"87\":1}}],[\"文字\",{\"1\":{\"54\":2}}],[\"文字段落\",{\"1\":{\"54\":24}}],[\"文字并包含\",{\"1\":{\"8\":1}}],[\"文德桥\",{\"1\":{\"46\":1}}],[\"乌衣巷\",{\"1\":{\"46\":1}}],[\"江南贡院\",{\"1\":{\"46\":1}}],[\"夫子庙大成殿\",{\"1\":{\"46\":1}}],[\"夫子庙秦淮河\",{\"1\":{\"46\":1}}],[\"游船上岸线路\",{\"1\":{\"46\":1}}],[\"游园卡可用\",{\"1\":{\"46\":1}}],[\"晚上80💰\",{\"1\":{\"46\":1}}],[\"白盒非指向性\",{\"1\":{\"218\":1}}],[\"白箱攻击\",{\"1\":{\"217\":1}}],[\"白板推导系列\",{\"1\":{\"200\":2}}],[\"白天60💰\",{\"1\":{\"46\":1}}],[\"白鹭洲公园\",{\"1\":{\"46\":1}}],[\"船票\",{\"1\":{\"46\":1}}],[\"夜场需要在下午16\",{\"1\":{\"46\":1}}],[\"夜18\",{\"1\":{\"46\":1}}],[\"07038\",{\"1\":{\"311\":1}}],[\"07366\",{\"1\":{\"286\":1}}],[\"06083\",{\"1\":{\"223\":1}}],[\"01\",{\"1\":{\"196\":1}}],[\"02\",{\"1\":{\"113\":1}}],[\"0\",{\"1\":{\"72\":1,\"77\":3,\"143\":3,\"145\":2,\"148\":4,\"154\":3,\"192\":1,\"195\":1,\"234\":1,\"235\":1,\"240\":1,\"241\":1,\"299\":1}}],[\"09\",{\"1\":{\"46\":1}}],[\"0004\",{\"1\":{\"213\":1}}],[\"00前预约\",{\"1\":{\"46\":1}}],[\"00\",{\"1\":{\"44\":2,\"45\":2,\"46\":4,\"196\":1}}],[\"东水关\",{\"1\":{\"46\":1}}],[\"沿途经过桃叶渡\",{\"1\":{\"46\":1}}],[\"坐画舫夜游\",{\"1\":{\"46\":1}}],[\"亮灯时间\",{\"1\":{\"46\":1}}],[\"不妨设\",{\"1\":{\"300\":1}}],[\"不需要增加一倍的维度\",{\"1\":{\"294\":1}}],[\"不依赖与\",{\"1\":{\"290\":1}}],[\"不借助于任何人工给出标签或者反馈等指导信息\",{\"1\":{\"242\":1}}],[\"不可观测变量问题\",{\"1\":{\"241\":1}}],[\"不管\",{\"1\":{\"230\":1}}],[\"不是稳定时\",{\"1\":{\"230\":1}}],[\"不超过\",{\"1\":{\"221\":1}}],[\"不独立\",{\"1\":{\"202\":1}}],[\"不知的情况下\",{\"1\":{\"202\":1}}],[\"不带阴影的节点表示隐变量\",{\"1\":{\"201\":1}}],[\"不失一般性\",{\"1\":{\"195\":1}}],[\"不等式的性质\",{\"1\":{\"194\":1}}],[\"不等式的性质可知\",{\"1\":{\"194\":1}}],[\"不等式\",{\"1\":{\"194\":1}}],[\"不含隐变量的参数估计\",{\"0\":{\"190\":1}}],[\"不规则序列建模\",{\"1\":{\"187\":1}}],[\"不用管\",{\"1\":{\"154\":1}}],[\"不仅可以表示当自变量\",{\"1\":{\"112\":1}}],[\"不如趁人少先睹为快吧\",{\"1\":{\"46\":1}}],[\"不同元素可以有相同的动画顺序\",{\"1\":{\"27\":1}}],[\"今年的灯会声势浩大\",{\"1\":{\"46\":1}}],[\"城墙边的巨型龙灯也已经就位\",{\"1\":{\"46\":1}}],[\"老门东\",{\"1\":{\"46\":1}}],[\"门票50💰\",{\"1\":{\"46\":1}}],[\"🎫\",{\"1\":{\"46\":2}}],[\"8a所示是一个常用的最大熵模型\",{\"1\":{\"210\":1}}],[\"8\",{\"1\":{\"46\":1,\"153\":4,\"230\":3}}],[\"⏰\",{\"1\":{\"46\":3}}],[\"⏰2小时起\",{\"1\":{\"45\":1}}],[\"看一看夕阳下的南京城真的绝了\",{\"1\":{\"46\":1}}],[\"瓮城高大雄伟\",{\"1\":{\"46\":1}}],[\"中描述的缺陷\",{\"1\":{\"293\":1}}],[\"中已经表述过\",{\"1\":{\"293\":1}}],[\"中采用的方法非常简单\",{\"1\":{\"293\":1}}],[\"中提到过对输入进行维度增加可以提高模型的表达能力\",{\"1\":{\"293\":1}}],[\"中进行采样\",{\"1\":{\"285\":1}}],[\"中定义的分布形式也称为吉布斯分布\",{\"1\":{\"208\":1}}],[\"中选取一个样本\",{\"1\":{\"195\":1}}],[\"中\",{\"1\":{\"167\":1}}],[\"中没有参数\",{\"1\":{\"166\":1}}],[\"中一半整数是奇数\",{\"1\":{\"148\":1}}],[\"中设置页面信息\",{\"1\":{\"86\":1}}],[\"中使用\",{\"1\":{\"63\":1}}],[\"中的都是同胚映射\",{\"1\":{\"298\":1}}],[\"中的最大团集合\",{\"1\":{\"208\":1}}],[\"中的局部马尔可夫性质\",{\"1\":{\"208\":1}}],[\"中的节点\",{\"1\":{\"207\":1}}],[\"中的变量取值为\",{\"1\":{\"204\":1}}],[\"中的每个节点都对应一个随机变量\",{\"1\":{\"202\":1}}],[\"中的\",{\"1\":{\"61\":1,\"192\":1}}],[\"中很重要的一个概念\",{\"1\":{\"59\":1}}],[\"中华门城堡\",{\"1\":{\"46\":1}}],[\"中华门城堡➠老门东➠乌衣巷➠秦淮河➠夫子庙➠江南贡院\",{\"1\":{\"46\":1}}],[\"中国三大博物馆之一\",{\"1\":{\"44\":1}}],[\"明城墙+夫子庙秦淮河夜游\",{\"0\":{\"46\":1}}],[\"情侣园等一同游玩\",{\"1\":{\"45\":1}}],[\"场次\",{\"1\":{\"45\":1}}],[\"已知的条件下\",{\"1\":{\"202\":2}}],[\"已经预约\",{\"1\":{\"45\":1}}],[\"已预约\",{\"1\":{\"44\":1}}],[\"地铁1号线玄武湖\",{\"1\":{\"45\":1}}],[\"湖边看看花\",{\"1\":{\"45\":1}}],[\"南京|夜游秦淮河攻略\",{\"1\":{\"46\":1}}],[\"南京小吃鸭血粉丝汤\",{\"1\":{\"46\":1}}],[\"南京城墙\",{\"1\":{\"45\":1}}],[\"南京站南广场正对面\",{\"1\":{\"45\":1}}],[\"南京最知名的一个湖\",{\"1\":{\"45\":1}}],[\"南京博物馆\",{\"0\":{\"44\":1},\"1\":{\"44\":1}}],[\"玄武湖\",{\"0\":{\"45\":1}}],[\"广陵王玺金印等等\",{\"1\":{\"44\":1}}],[\"鎏金镶嵌兽形铜盒砚\",{\"1\":{\"44\":1}}],[\"错银铜牛灯\",{\"1\":{\"44\":1}}],[\"镇馆之宝有\",{\"1\":{\"44\":1}}],[\"拍照还是集章\",{\"1\":{\"44\":1}}],[\"一定存在着\",{\"1\":{\"299\":1}}],[\"一定要for\",{\"1\":{\"144\":1}}],[\"一些问题可能会从零初始化中受益\",{\"1\":{\"295\":1}}],[\"一种简单的参数化模型为sigmoid信念网络\",{\"1\":{\"204\":1}}],[\"一种有效减少参数量的方法是独立性假设\",{\"1\":{\"196\":1}}],[\"一般将neural以图像或者词向量作为输入在增加维度\",{\"1\":{\"300\":1}}],[\"一般将node作为深度学习网络框架中的一部分\",{\"1\":{\"290\":1}}],[\"一般与\",{\"1\":{\"230\":1}}],[\"一般难以直接建模\",{\"1\":{\"196\":1}}],[\"一般情况下很难计算\",{\"1\":{\"194\":1}}],[\"一般是由领域专家来构建\",{\"1\":{\"189\":1}}],[\"一般为了简化模型\",{\"1\":{\"166\":1}}],[\"一是网络结构学习\",{\"1\":{\"189\":1}}],[\"一直到最后也没有找到index\",{\"1\":{\"148\":1}}],[\"一半整数是偶数\",{\"1\":{\"148\":1}}],[\"一院六馆各有特色\",{\"1\":{\"44\":1}}],[\"一个resnet块具体定义为\",{\"1\":{\"290\":1}}],[\"一个被数字\",{\"1\":{\"268\":1}}],[\"一个被星标了的苹果文章\",{\"1\":{\"248\":1}}],[\"一个比较通用的准则是组内样本的相似性要高于组间样本的相似性\",{\"1\":{\"242\":1}}],[\"一个线性模型被迫只关注与权重相接近的信号\",{\"1\":{\"220\":1}}],[\"一个网络模型的权重为\",{\"1\":{\"220\":1}}],[\"一个样本\",{\"1\":{\"194\":1}}],[\"一个\",{\"1\":{\"172\":1,\"196\":1}}],[\"一个折线图案例\",{\"1\":{\"78\":1}}],[\"一个散点图案例\",{\"1\":{\"77\":1}}],[\"一个拥有\",{\"1\":{\"67\":1,\"102\":2,\"184\":1}}],[\"一个链接\",{\"1\":{\"8\":1}}],[\"一个简单的幻灯片演示与各种小贴士\",{\"1\":{\"3\":1}}],[\"ian\",{\"1\":{\"221\":1}}],[\"io\",{\"1\":{\"218\":2}}],[\"i+n\",{\"1\":{\"150\":1}}],[\"i++\",{\"1\":{\"145\":1,\"148\":3,\"150\":1}}],[\"i<nums\",{\"1\":{\"150\":1}}],[\"i<arr\",{\"1\":{\"148\":2}}],[\"i=0\",{\"1\":{\"150\":1}}],[\"i=index+1\",{\"1\":{\"148\":1}}],[\"i=1\",{\"1\":{\"148\":1}}],[\"ii\",{\"0\":{\"158\":1},\"1\":{\"148\":1}}],[\"if\",{\"1\":{\"143\":1,\"144\":2,\"145\":1,\"148\":5,\"151\":1,\"154\":4}}],[\"i\",{\"1\":{\"99\":1,\"143\":4,\"144\":2,\"145\":3,\"148\":15,\"150\":1,\"181\":1,\"225\":1}}],[\"img\",{\"1\":{\"109\":1,\"142\":1,\"144\":1,\"202\":4,\"215\":1}}],[\"important\",{\"1\":{\"97\":1}}],[\"image\",{\"1\":{\"42\":1,\"44\":2,\"45\":2,\"46\":2,\"109\":1,\"111\":1,\"166\":1,\"195\":1,\"202\":2,\"205\":1,\"207\":1,\"208\":1,\"236\":1,\"237\":3}}],[\"is\",{\"1\":{\"72\":2,\"290\":1,\"294\":1}}],[\"id\",{\"0\":{\"102\":1,\"184\":1},\"1\":{\"67\":1,\"102\":2,\"184\":1}}],[\"item\",{\"1\":{\"300\":4}}],[\"it\",{\"1\":{\"60\":2}}],[\"inequality\",{\"1\":{\"221\":1}}],[\"input\",{\"1\":{\"195\":1}}],[\"inference\",{\"1\":{\"194\":1}}],[\"info\",{\"1\":{\"93\":1,\"176\":1}}],[\"insert\",{\"1\":{\"154\":1}}],[\"into\",{\"1\":{\"294\":1}}],[\"interval在左边\",{\"1\":{\"154\":1}}],[\"interval\",{\"1\":{\"154\":8}}],[\"intervals\",{\"1\":{\"153\":5,\"154\":2}}],[\"int\",{\"1\":{\"143\":4,\"145\":3,\"148\":6,\"150\":4,\"151\":1,\"154\":2}}],[\"index=\",{\"1\":{\"150\":1}}],[\"index==0||index==arr\",{\"1\":{\"148\":1}}],[\"index==0表明arr\",{\"1\":{\"148\":1}}],[\"index=arr\",{\"1\":{\"148\":1}}],[\"index=0\",{\"1\":{\"148\":1}}],[\"index++\",{\"1\":{\"145\":1,\"148\":1}}],[\"index\",{\"1\":{\"27\":1,\"143\":4,\"144\":3,\"145\":4,\"150\":1}}],[\"in\",{\"1\":{\"21\":3,\"226\":1,\"294\":1}}],[\"inline\",{\"1\":{\"9\":1}}],[\"旅游之南京篇\",{\"0\":{\"42\":1}}],[\"的可测子集\",{\"1\":{\"298\":1}}],[\"的基本架构是一样的\",{\"1\":{\"296\":1}}],[\"的随机变量来初始化额外维度的初始值\",{\"1\":{\"295\":1}}],[\"的定义\",{\"1\":{\"286\":1}}],[\"的概率密度函数为\",{\"1\":{\"241\":1}}],[\"的概率依赖于和的组合\",{\"1\":{\"196\":1}}],[\"的局部截断误差是\",{\"1\":{\"236\":1}}],[\"的局部条件概率的连乘形式\",{\"1\":{\"202\":1}}],[\"的局部条件概率\",{\"1\":{\"191\":1}}],[\"的唯一解\",{\"1\":{\"235\":1}}],[\"的大\",{\"1\":{\"234\":1}}],[\"的已知序列\",{\"1\":{\"234\":1}}],[\"的常数\",{\"1\":{\"234\":1}}],[\"的解\",{\"1\":{\"230\":2}}],[\"的零解\",{\"1\":{\"230\":2}}],[\"的零阶\",{\"1\":{\"111\":4}}],[\"的由初始条件\",{\"1\":{\"230\":1}}],[\"的上界和下界\",{\"1\":{\"225\":1}}],[\"的平均置信度\",{\"1\":{\"221\":1}}],[\"的攻击成功率\",{\"1\":{\"221\":3}}],[\"的扰动值\",{\"1\":{\"225\":1}}],[\"的扰动\",{\"1\":{\"221\":1}}],[\"的设计基于以下直觉\",{\"1\":{\"221\":1}}],[\"的梯度\",{\"1\":{\"221\":2}}],[\"的梯度时\",{\"1\":{\"194\":1,\"221\":1}}],[\"的核心\",{\"1\":{\"221\":1}}],[\"的正比例\",{\"1\":{\"221\":1}}],[\"的对数似然函数为\",{\"1\":{\"240\":1,\"241\":1}}],[\"的对数形式为\",{\"1\":{\"210\":1}}],[\"的对数边际似然函数为\",{\"1\":{\"194\":1}}],[\"的邻居集合\",{\"1\":{\"207\":1}}],[\"的参数\",{\"1\":{\"205\":1}}],[\"的先验概率分布的参数\",{\"1\":{\"205\":1}}],[\"的先验分布为标准高斯分布\",{\"1\":{\"166\":1}}],[\"的情况下\",{\"1\":{\"205\":1}}],[\"的条件概率为\",{\"1\":{\"205\":1}}],[\"的非后代变量\",{\"1\":{\"202\":1}}],[\"的头相连\",{\"1\":{\"202\":1}}],[\"的头和\",{\"1\":{\"202\":2}}],[\"的尾巴相连\",{\"1\":{\"202\":1}}],[\"的角度来看\",{\"1\":{\"202\":2}}],[\"的所有父节点变量集合\",{\"1\":{\"202\":1}}],[\"的父节点数量为\",{\"1\":{\"204\":1}}],[\"的父节点数量为𝑀\",{\"1\":{\"191\":1}}],[\"的父类\",{\"1\":{\"202\":1}}],[\"的示例\",{\"1\":{\"201\":1}}],[\"的取值空间\",{\"1\":{\"208\":1}}],[\"的取值\",{\"1\":{\"196\":1}}],[\"的联合概率分布的模型\",{\"1\":{\"207\":1}}],[\"的联合概率分布可以分解为定义在最大团上的势能函数的连乘形式\",{\"1\":{\"192\":1}}],[\"的联合概率分布可以分解为每个随机变量\",{\"1\":{\"191\":1,\"202\":1}}],[\"的联合概率分解为\",{\"1\":{\"196\":1}}],[\"的偏导数\",{\"1\":{\"195\":1,\"240\":1,\"241\":1}}],[\"的偏导数为\",{\"1\":{\"192\":1}}],[\"的证据下界为\",{\"1\":{\"195\":1}}],[\"的过程可以分为两步\",{\"1\":{\"195\":1}}],[\"的过程可以分解为两个步骤\",{\"1\":{\"194\":1}}],[\"的下界\",{\"1\":{\"194\":1}}],[\"的推断问题\",{\"1\":{\"194\":1}}],[\"的边际似然函数\",{\"1\":{\"194\":1}}],[\"的连乘形式\",{\"1\":{\"191\":1}}],[\"的第\",{\"1\":{\"172\":1,\"225\":1}}],[\"的样本\",{\"1\":{\"167\":1}}],[\"的样本尽可能地相似\",{\"1\":{\"167\":1}}],[\"的样本和\",{\"1\":{\"167\":1}}],[\"的每一维之间都是独立的\",{\"1\":{\"166\":1}}],[\"的两个局部条件概率\",{\"1\":{\"166\":1}}],[\"的未知分布中产生的\",{\"1\":{\"166\":1}}],[\"的逻辑\",{\"1\":{\"144\":1}}],[\"的高阶无穷小量\",{\"1\":{\"113\":1}}],[\"的线性泛函\",{\"1\":{\"113\":1}}],[\"的增量为\",{\"1\":{\"113\":1}}],[\"的形式如下\",{\"1\":{\"300\":1}}],[\"的形式非常简单\",{\"1\":{\"194\":1}}],[\"的形式\",{\"1\":{\"113\":2}}],[\"的积分得到的\",{\"1\":{\"113\":1}}],[\"的末知函数\",{\"1\":{\"113\":1}}],[\"的作用是用以表示相应于自变量\",{\"1\":{\"112\":1}}],[\"的区别\",{\"1\":{\"112\":1}}],[\"的变分\",{\"1\":{\"112\":1}}],[\"的变量\",{\"1\":{\"112\":1}}],[\"的任意函数\",{\"1\":{\"112\":1}}],[\"的带状区域内的曲线组成\",{\"1\":{\"111\":1}}],[\"的一阶邻域内\",{\"1\":{\"113\":1}}],[\"的一阶\",{\"1\":{\"111\":3}}],[\"的一阶导数\",{\"1\":{\"109\":1}}],[\"的函数\",{\"1\":{\"111\":3,\"112\":2,\"113\":2}}],[\"的泛函\",{\"1\":{\"109\":1,\"111\":1,\"113\":1}}],[\"的距离为\",{\"1\":{\"109\":1}}],[\"的\",{\"1\":{\"67\":1,\"86\":1,\"102\":2,\"111\":1,\"184\":1,\"235\":1,\"285\":1}}],[\"的同时点击幻灯片的任何元素\",{\"1\":{\"40\":1}}],[\"的段落\",{\"1\":{\"8\":1}}],[\"缩放\",{\"0\":{\"40\":1}}],[\"odes\",{\"1\":{\"296\":1,\"311\":1}}],[\"ode\",{\"1\":{\"290\":1,\"294\":1,\"299\":1}}],[\"oddindex\",{\"1\":{\"148\":3}}],[\"or\",{\"1\":{\"294\":1}}],[\"ordinary\",{\"1\":{\"286\":1}}],[\"org\",{\"1\":{\"223\":1,\"286\":1}}],[\"oh\",{\"1\":{\"234\":1}}],[\"of\",{\"1\":{\"213\":1,\"226\":2,\"290\":2,\"294\":3,\"296\":2}}],[\"offer\",{\"1\":{\"161\":1}}],[\"opens\",{\"1\":{\"147\":1}}],[\"options\",{\"1\":{\"77\":1}}],[\"one\",{\"1\":{\"80\":2,\"241\":1,\"296\":1}}],[\"o\",{\"1\":{\"36\":1,\"143\":2}}],[\"output\",{\"1\":{\"294\":2}}],[\"out\",{\"1\":{\"21\":3}}],[\"或者其他激活函数例如sigmoid\",{\"1\":{\"290\":1}}],[\"或最大熵模型\",{\"1\":{\"210\":1}}],[\"或马尔可夫网络\",{\"1\":{\"207\":1}}],[\"或信念网络\",{\"1\":{\"202\":1}}],[\"或一组随机变量\",{\"1\":{\"201\":1}}],[\"或一阶以上\",{\"1\":{\"112\":1}}],[\"或积分\",{\"1\":{\"194\":1}}],[\"或小写希腊字母\",{\"1\":{\"172\":1}}],[\"或第\",{\"1\":{\"172\":1}}],[\"或称线性空间\",{\"1\":{\"170\":1}}],[\"或直接生成符合分布\",{\"1\":{\"167\":1}}],[\"或\",{\"1\":{\"36\":1,\"38\":1,\"113\":1,\"221\":2,\"302\":1}}],[\"easily\",{\"1\":{\"226\":1}}],[\"ease\",{\"1\":{\"226\":1}}],[\"exposition\",{\"1\":{\"226\":1}}],[\"expectation\",{\"1\":{\"194\":1}}],[\"expectationmaximum\",{\"1\":{\"194\":1}}],[\"e\",{\"1\":{\"225\":1}}],[\"efficient\",{\"1\":{\"224\":1}}],[\"edu\",{\"1\":{\"213\":2}}],[\"et\",{\"1\":{\"206\":1,\"210\":2}}],[\"e步中\",{\"1\":{\"194\":1}}],[\"e步\",{\"1\":{\"194\":1,\"195\":2}}],[\"evidence\",{\"1\":{\"194\":2}}],[\"evenindex\",{\"1\":{\"148\":3}}],[\"em算法\",{\"0\":{\"194\":1}}],[\"empirical\",{\"1\":{\"192\":1}}],[\"em\",{\"1\":{\"166\":2,\"194\":4}}],[\"estimation\",{\"1\":{\"166\":1,\"238\":2,\"239\":2,\"242\":1}}],[\"esc\",{\"1\":{\"36\":1}}],[\"elbo\",{\"1\":{\"194\":1}}],[\"else\",{\"1\":{\"148\":2,\"154\":2}}],[\"elements\",{\"1\":{\"311\":1}}],[\"element\",{\"1\":{\"5\":1}}],[\"equations\",{\"1\":{\"286\":1}}],[\"equation\",{\"1\":{\"109\":1}}],[\"euler法\",{\"0\":{\"235\":1}}],[\"euler\",{\"1\":{\"109\":1,\"235\":2,\"236\":1}}],[\"enumerate\",{\"1\":{\"300\":2}}],[\"entropy\",{\"1\":{\"210\":1}}],[\"energy\",{\"1\":{\"208\":1}}],[\"end\",{\"1\":{\"80\":3,\"143\":2,\"145\":2,\"151\":2,\"300\":2}}],[\"enhance\",{\"1\":{\"62\":1}}],[\"e=>end\",{\"1\":{\"79\":1}}],[\"echarts\",{\"0\":{\"78\":1},\"1\":{\"78\":1}}],[\"+k\",{\"1\":{\"151\":2}}],[\"+=\",{\"1\":{\"148\":2}}],[\"+\",{\"1\":{\"34\":2}}],[\"xaxis\",{\"1\":{\"78\":1}}],[\"xiaohongshu\",{\"1\":{\"46\":2}}],[\"x\",{\"1\":{\"34\":2,\"77\":5}}],[\"语法\",{\"0\":{\"81\":1},\"1\":{\"62\":1}}],[\"语法进行了扩展\",{\"1\":{\"61\":1}}],[\"语法扩展\",{\"1\":{\"60\":1}}],[\"语法来分布高亮特定行\",{\"1\":{\"34\":1}}],[\"语法的各种标记\",{\"1\":{\"6\":1,\"7\":1,\"9\":1,\"10\":1,\"11\":1,\"12\":1}}],[\"功能详情\",{\"1\":{\"306\":1,\"310\":1}}],[\"功能\",{\"0\":{\"32\":1,\"33\":1,\"35\":1,\"37\":1,\"39\":1,\"303\":1,\"307\":1},\"1\":{\"304\":1,\"308\":1}}],[\"过渡动画\",{\"0\":{\"31\":1}}],[\"可能会轻易被对抗攻击打破\",{\"1\":{\"215\":1}}],[\"可能的值\",{\"1\":{\"29\":1}}],[\"可得\",{\"1\":{\"195\":1,\"240\":1,\"241\":1}}],[\"可取函数\",{\"1\":{\"112\":1}}],[\"可表示为\",{\"1\":{\"111\":1}}],[\"可以找到仿射函数\",{\"1\":{\"300\":1}}],[\"可以减少数据的维度\",{\"1\":{\"299\":1}}],[\"可以让一个node由若干的不同范围的的node表示\",{\"1\":{\"296\":1}}],[\"可以出现更高维度\",{\"1\":{\"290\":1}}],[\"可以帮助我们更好的进行深度学习的研究\",{\"1\":{\"290\":1}}],[\"可以进行任意形状密度的估计\",{\"1\":{\"242\":1}}],[\"可以选择不同的参数\",{\"1\":{\"226\":1}}],[\"可以引入任意一个错误类别\",{\"1\":{\"217\":1}}],[\"可以表示为一系列定义在最大团上的非负函数的乘积形式\",{\"1\":{\"208\":1}}],[\"可以得到\",{\"1\":{\"207\":1}}],[\"可以存在循环\",{\"1\":{\"207\":1}}],[\"可以有效防止过拟合\",{\"1\":{\"205\":1}}],[\"可以写出其因子分解的结果\",{\"1\":{\"202\":1}}],[\"可以分解为\",{\"1\":{\"196\":1,\"205\":1}}],[\"可以用多项分布建模\",{\"1\":{\"205\":1}}],[\"可以用高斯分布建模\",{\"1\":{\"205\":1}}],[\"可以用一个联合概率表来记录每一种取值的概率\",{\"1\":{\"196\":1}}],[\"可以用来建模不同的数据\",{\"1\":{\"167\":1}}],[\"可以估计出最优的参数\",{\"1\":{\"194\":1}}],[\"可以看出\",{\"1\":{\"192\":1}}],[\"可以看出来\",{\"1\":{\"144\":1}}],[\"可以使用参数化模型来建模有向图模型中的条件概率分布\",{\"1\":{\"204\":1}}],[\"可以使用参数化的模型\",{\"1\":{\"191\":1}}],[\"可以使用第11\",{\"1\":{\"194\":1}}],[\"可以使用高斯函数来表示条件概率分布\",{\"1\":{\"191\":1}}],[\"可以合并区间\",{\"1\":{\"153\":1}}],[\"可以尝试使用贪心策略\",{\"1\":{\"142\":1}}],[\"可以近似成线段\",{\"1\":{\"109\":1}}],[\"可以根据文件结构将它们转换为不同的页面\",{\"1\":{\"57\":1}}],[\"可以搭配鸡鸣寺\",{\"1\":{\"45\":1}}],[\"可以通过配置中的\",{\"1\":{\"29\":1}}],[\"第\",{\"1\":{\"225\":1}}],[\"第一天\",{\"0\":{\"43\":1}}],[\"第一个显示\",{\"1\":{\"27\":1}}],[\"第二个显示\",{\"1\":{\"27\":2}}],[\"顺序\",{\"0\":{\"27\":1}}],[\"渐近\",{\"1\":{\"230\":1}}],[\"渐近稳定\",{\"1\":{\"230\":1}}],[\"渐变\",{\"0\":{\"28\":1,\"29\":1,\"30\":1}}],[\"渐出\",{\"1\":{\"25\":1}}],[\"渐入\",{\"1\":{\"25\":1}}],[\"元素产生过渡动画效果\",{\"1\":{\"31\":1}}],[\"元素使其拥有多个动画片段\",{\"1\":{\"25\":1}}],[\"元素上添加属性\",{\"1\":{\"5\":1}}],[\"does\",{\"1\":{\"294\":2}}],[\"down\",{\"1\":{\"21\":1}}],[\"dag\",{\"1\":{\"201\":1}}],[\"datasets\",{\"1\":{\"77\":1}}],[\"data\",{\"1\":{\"17\":1,\"27\":1,\"29\":1,\"31\":1,\"77\":2,\"78\":2}}],[\"dimensional\",{\"1\":{\"294\":1}}],[\"differential\",{\"1\":{\"286\":1}}],[\"directed\",{\"1\":{\"201\":1,\"202\":1}}],[\"distribution\",{\"1\":{\"192\":1,\"202\":1,\"208\":2}}],[\"dsw2718314\",{\"1\":{\"109\":1}}],[\"design\",{\"1\":{\"300\":1}}],[\"desc\",{\"1\":{\"72\":1}}],[\"determine\",{\"1\":{\"294\":2}}],[\"details\",{\"1\":{\"98\":1,\"180\":1}}],[\"deep\",{\"1\":{\"223\":1}}],[\"della\",{\"1\":{\"210\":1}}],[\"density\",{\"1\":{\"166\":1,\"238\":2,\"239\":1,\"242\":1}}],[\"d\",{\"1\":{\"34\":1,\"99\":3,\"181\":3,\"213\":1}}],[\"foo\",{\"0\":{\"307\":1},\"1\":{\"308\":1}}],[\"follows\",{\"1\":{\"296\":1}}],[\"for里的i指向饼干9\",{\"1\":{\"144\":1}}],[\"for\",{\"1\":{\"143\":1,\"144\":1,\"145\":1,\"148\":3,\"150\":1,\"154\":1,\"218\":1,\"226\":1,\"294\":1}}],[\"flows\",{\"1\":{\"283\":1}}],[\"flowchart\",{\"1\":{\"80\":2}}],[\"feature\",{\"1\":{\"242\":1}}],[\"fgsm\",{\"0\":{\"218\":1},\"1\":{\"218\":2,\"221\":7}}],[\"function\",{\"1\":{\"208\":3}}],[\"fast\",{\"1\":{\"221\":2}}],[\"false\",{\"1\":{\"148\":3}}],[\"fade\",{\"1\":{\"21\":8,\"29\":1}}],[\"finite\",{\"1\":{\"290\":1}}],[\"findcontentchildren\",{\"1\":{\"143\":1,\"145\":1}}],[\"field\",{\"1\":{\"207\":1}}],[\"fit\",{\"1\":{\"14\":1}}],[\"fm\",{\"1\":{\"113\":2}}],[\"from\",{\"1\":{\"296\":1}}],[\"frontmatter\",{\"1\":{\"51\":1,\"59\":3,\"86\":1,\"88\":1}}],[\"friedman\",{\"1\":{\"213\":1}}],[\"fri\",{\"1\":{\"78\":1}}],[\"fragment\",{\"1\":{\"19\":1,\"27\":1}}],[\"f11\",{\"1\":{\"38\":1}}],[\"f\",{\"1\":{\"38\":1}}],[\"bi\",{\"1\":{\"299\":1}}],[\"bilibili\",{\"1\":{\"200\":1}}],[\"by\",{\"1\":{\"296\":2}}],[\"bn\",{\"1\":{\"202\":1}}],[\"be\",{\"1\":{\"226\":1,\"294\":1}}],[\"berger\",{\"1\":{\"210\":1}}],[\"belief\",{\"1\":{\"202\":1,\"204\":1,\"213\":1}}],[\"begin\",{\"1\":{\"143\":2,\"145\":2,\"151\":4,\"300\":1}}],[\"banana\",{\"0\":{\"314\":1}}],[\"baz\",{\"0\":{\"306\":1},\"1\":{\"305\":1}}],[\"bar\",{\"0\":{\"303\":1},\"1\":{\"304\":1}}],[\"baidinghub\",{\"1\":{\"218\":1}}],[\"baiding\",{\"1\":{\"218\":1}}],[\"baum\",{\"1\":{\"206\":1}}],[\"bayes\",{\"1\":{\"205\":1}}],[\"bayesiannetwork\",{\"1\":{\"202\":1}}],[\"back\",{\"1\":{\"154\":5}}],[\"backgroundcolor\",{\"1\":{\"77\":1}}],[\"background\",{\"1\":{\"17\":1}}],[\"break\",{\"1\":{\"148\":1}}],[\"box\",{\"1\":{\"217\":2}}],[\"boltzmann\",{\"1\":{\"208\":1}}],[\"bound\",{\"1\":{\"194\":1}}],[\"bool\",{\"1\":{\"148\":1,\"154\":1}}],[\"bottom\",{\"1\":{\"77\":1}}],[\"b1\",{\"1\":{\"80\":1}}],[\"b\",{\"1\":{\"34\":1}}],[\"b|c\",{\"1\":{\"34\":1}}],[\"black\",{\"1\":{\"217\":1}}],[\"blue\",{\"1\":{\"23\":2}}],[\"blog\",{\"1\":{\"218\":1}}],[\"bloghome\",{\"1\":{\"0\":1}}],[\"block\",{\"1\":{\"9\":1}}],[\"背景\",{\"0\":{\"17\":1}}],[\"使由初始条件\",{\"1\":{\"230\":1}}],[\"使当\",{\"1\":{\"230\":1}}],[\"使当任一\",{\"1\":{\"230\":1}}],[\"使得即使是相同的原始输入\",{\"1\":{\"295\":1}}],[\"使得只要当\",{\"1\":{\"234\":1}}],[\"使得当它加到输入样本\",{\"1\":{\"221\":1}}],[\"使得损失函数增加最快的方向\",{\"1\":{\"221\":1}}],[\"使得证据下界\",{\"1\":{\"194\":1}}],[\"使得其在经验分布\",{\"1\":{\"192\":1}}],[\"使得\",{\"1\":{\"167\":1,\"194\":1,\"226\":1,\"230\":1,\"234\":1,\"235\":1,\"239\":1,\"299\":1,\"300\":2}}],[\"使得泛函\",{\"1\":{\"109\":1}}],[\"使相同的\",{\"1\":{\"31\":1}}],[\"使它们填充满幻灯片垂直方向上的剩余空间\",{\"1\":{\"15\":1}}],[\"使用最大似然估计\",{\"1\":{\"239\":1}}],[\"使用其他的简单的方法也可以产生对抗样本\",{\"1\":{\"221\":1}}],[\"使用相同的配置\",{\"1\":{\"221\":1}}],[\"使用了\",{\"1\":{\"221\":1}}],[\"使用这种特定形式的扰动来生成对抗样本的原因\",{\"1\":{\"221\":1}}],[\"使用额外的空间\",{\"0\":{\"150\":1}}],[\"使用指南\",{\"1\":{\"86\":2},\"2\":{\"50\":1,\"52\":1,\"55\":1,\"83\":1,\"89\":1,\"90\":1,\"105\":1}}],[\"使用\",{\"1\":{\"5\":3,\"221\":2}}],[\"帮助你控制注入图片或视频的大小\",{\"1\":{\"15\":1}}],[\"spectral\",{\"1\":{\"311\":1}}],[\"space\",{\"1\":{\"294\":1}}],[\"spaces\",{\"1\":{\"113\":2}}],[\"s都表示不同的网络\",{\"1\":{\"296\":1}}],[\"schwarz\",{\"1\":{\"221\":1}}],[\"scalar\",{\"1\":{\"172\":1}}],[\"scales\",{\"1\":{\"77\":1}}],[\"scatter\",{\"1\":{\"77\":1}}],[\"sbn\",{\"1\":{\"204\":1}}],[\"since\",{\"1\":{\"296\":1}}],[\"similar\",{\"1\":{\"296\":1}}],[\"sign\",{\"1\":{\"221\":2}}],[\"sigmoid\",{\"1\":{\"204\":6}}],[\"sigmoid信念网络和logistic回归模型的比较\",{\"1\":{\"204\":1}}],[\"sigmoid信念网络\",{\"0\":{\"204\":1}}],[\"sidebar\",{\"1\":{\"201\":1}}],[\"size\",{\"1\":{\"143\":2,\"145\":2,\"148\":6,\"150\":3,\"151\":2}}],[\"softmax回归容易受到对抗性例子的影响\",{\"1\":{\"220\":1}}],[\"sortarraybyparityii\",{\"1\":{\"148\":1}}],[\"sort\",{\"1\":{\"143\":2,\"145\":2}}],[\"solution\",{\"1\":{\"143\":1,\"145\":1,\"148\":2,\"150\":1,\"151\":1,\"154\":1,\"296\":1}}],[\"stone\",{\"1\":{\"300\":1}}],[\"stacked\",{\"1\":{\"296\":1}}],[\"steganography\",{\"1\":{\"220\":1}}],[\"step\",{\"1\":{\"194\":2}}],[\"stein变分梯度下降详细解读\",{\"1\":{\"113\":1}}],[\"strike\",{\"1\":{\"23\":1}}],[\"stretch\",{\"1\":{\"15\":1}}],[\"s\",{\"1\":{\"113\":1,\"143\":5,\"145\":5,\"218\":1,\"298\":1,\"299\":1}}],[\"sup范数\",{\"1\":{\"298\":1}}],[\"suppose\",{\"1\":{\"290\":1}}],[\"subgraph\",{\"1\":{\"80\":3}}],[\"sun\",{\"1\":{\"78\":1}}],[\"sec\",{\"1\":{\"300\":1}}],[\"sensitivity\",{\"1\":{\"286\":1}}],[\"series\",{\"1\":{\"78\":1}}],[\"semi\",{\"1\":{\"21\":1}}],[\"satisfy\",{\"1\":{\"296\":1}}],[\"sat\",{\"1\":{\"78\":1}}],[\"svg\",{\"1\":{\"72\":1}}],[\"shrink\",{\"1\":{\"23\":1}}],[\"slideend\",{\"1\":{\"41\":1}}],[\"slide\",{\"1\":{\"5\":1,\"29\":1}}],[\"slidestart\",{\"1\":{\"2\":1}}],[\"lstm\",{\"1\":{\"221\":1}}],[\"l\",{\"1\":{\"216\":1}}],[\"log\",{\"1\":{\"210\":1}}],[\"logistic\",{\"1\":{\"204\":6}}],[\"logo\",{\"1\":{\"15\":1,\"72\":2}}],[\"local\",{\"1\":{\"202\":1}}],[\"lower\",{\"1\":{\"194\":1}}],[\"latent\",{\"1\":{\"294\":1}}],[\"lagrange\",{\"1\":{\"109\":1}}],[\"label\",{\"1\":{\"77\":1}}],[\"layout\",{\"1\":{\"0\":1}}],[\"lifting\",{\"1\":{\"294\":1}}],[\"life\",{\"1\":{\"218\":1}}],[\"likelihood\",{\"1\":{\"194\":1,\"239\":1}}],[\"line\",{\"1\":{\"78\":1}}],[\"linear\",{\"1\":{\"77\":1,\"210\":1,\"290\":1}}],[\"link\",{\"1\":{\"72\":1}}],[\"linux\",{\"1\":{\"40\":1}}],[\"light\",{\"1\":{\"72\":1}}],[\"leads\",{\"1\":{\"296\":1}}],[\"learning\",{\"1\":{\"213\":1,\"223\":1,\"242\":2}}],[\"lemma\",{\"1\":{\"290\":1}}],[\"leetcode\",{\"1\":{\"141\":1,\"148\":1,\"149\":1,\"155\":1,\"161\":1}}],[\"let\",{\"1\":{\"34\":3,\"225\":1}}],[\"left=min\",{\"1\":{\"154\":1}}],[\"left=newinterval\",{\"1\":{\"154\":1}}],[\"left\",{\"1\":{\"21\":1,\"154\":3}}],[\"certification\",{\"1\":{\"224\":1}}],[\"center\",{\"1\":{\"100\":1,\"182\":1}}],[\"cnn\",{\"1\":{\"290\":1}}],[\"cn\",{\"1\":{\"216\":1}}],[\"csbkpthtet4r\",{\"1\":{\"216\":1}}],[\"csc321\",{\"1\":{\"213\":1}}],[\"cs\",{\"1\":{\"213\":1}}],[\"cluster\",{\"1\":{\"242\":1}}],[\"clustering\",{\"1\":{\"242\":1}}],[\"clifford\",{\"1\":{\"208\":2}}],[\"clifford定理\",{\"1\":{\"208\":1}}],[\"clique\",{\"1\":{\"208\":2}}],[\"class\",{\"0\":{\"21\":1,\"23\":1},\"1\":{\"14\":1,\"15\":1,\"19\":1,\"143\":1,\"145\":1,\"148\":2,\"150\":1,\"151\":1,\"154\":1}}],[\"case\",{\"1\":{\"226\":1}}],[\"can\",{\"1\":{\"226\":1,\"296\":1}}],[\"cauchy\",{\"1\":{\"221\":1}}],[\"caution\",{\"1\":{\"96\":1,\"179\":1}}],[\"category\",{\"1\":{\"78\":1}}],[\"c2\",{\"1\":{\"80\":1}}],[\"c1\",{\"1\":{\"80\":2}}],[\"chart\",{\"1\":{\"77\":1}}],[\"ctrl\",{\"1\":{\"40\":1}}],[\"c\",{\"1\":{\"34\":2}}],[\"current\",{\"1\":{\"23\":3}}],[\"courses\",{\"1\":{\"213\":1}}],[\"cout<<index<<endl\",{\"1\":{\"148\":1}}],[\"code\",{\"1\":{\"99\":1,\"181\":1}}],[\"coding是我\",{\"1\":{\"1\":1}}],[\"color\",{\"1\":{\"72\":1}}],[\"completely\",{\"1\":{\"294\":1}}],[\"composed\",{\"1\":{\"290\":1}}],[\"com\",{\"1\":{\"46\":2,\"72\":2,\"109\":2,\"113\":1,\"142\":1,\"194\":1,\"200\":1,\"284\":1}}],[\"contrast\",{\"1\":{\"294\":1}}],[\"continue\",{\"1\":{\"154\":1}}],[\"consider\",{\"1\":{\"290\":1}}],[\"const\",{\"1\":{\"10\":1,\"63\":1,\"93\":2,\"154\":1,\"176\":2}}],[\"connectionist\",{\"1\":{\"213\":1}}],[\"conditional\",{\"1\":{\"202\":1}}],[\"cond\",{\"1\":{\"79\":2}}],[\"cond=>condition\",{\"1\":{\"79\":1}}],[\"concave\",{\"1\":{\"29\":1}}],[\"convex\",{\"1\":{\"29\":1}}],[\"ray\",{\"0\":{\"310\":1},\"1\":{\"309\":1}}],[\"random\",{\"1\":{\"207\":1}}],[\"rnn等有所不同\",{\"1\":{\"286\":1}}],[\"runge\",{\"0\":{\"237\":1}}],[\"robustness\",{\"1\":{\"224\":1}}],[\"rotate\",{\"1\":{\"150\":1,\"151\":1}}],[\"ref\",{\"1\":{\"299\":2,\"300\":1}}],[\"recover\",{\"1\":{\"296\":1}}],[\"recalling\",{\"1\":{\"296\":1}}],[\"relation\",{\"1\":{\"296\":1}}],[\"relaxation\",{\"1\":{\"294\":1}}],[\"relu\",{\"1\":{\"221\":1,\"290\":2}}],[\"regarded\",{\"1\":{\"294\":1}}],[\"readings\",{\"1\":{\"213\":1}}],[\"repeat\",{\"1\":{\"195\":1}}],[\"resnet前向传播可以看做时欧拉法解神经微分方程\",{\"1\":{\"290\":1}}],[\"restrict\",{\"1\":{\"226\":1}}],[\"resistant\",{\"1\":{\"223\":1}}],[\"res\",{\"1\":{\"154\":7}}],[\"result++\",{\"1\":{\"143\":1}}],[\"result\",{\"1\":{\"143\":2,\"148\":4,\"296\":1}}],[\"reverse\",{\"1\":{\"151\":3}}],[\"return\",{\"1\":{\"143\":1,\"145\":1,\"148\":5,\"150\":1,\"151\":1,\"154\":1}}],[\"red\",{\"1\":{\"23\":2}}],[\"rgb\",{\"1\":{\"77\":1}}],[\"rgba\",{\"1\":{\"72\":1}}],[\"right=max\",{\"1\":{\"154\":1}}],[\"right=newinterval\",{\"1\":{\"154\":1}}],[\"right\",{\"1\":{\"21\":1,\"100\":1,\"154\":3,\"182\":1}}],[\"r\",{\"1\":{\"14\":1,\"15\":1}}],[\"👆\",{\"1\":{\"14\":1,\"15\":1}}],[\"👇\",{\"1\":{\"4\":1}}],[\"布局与功能禁用\",{\"0\":{\"51\":1}}],[\"布局\",{\"0\":{\"13\":1,\"14\":1,\"15\":1,\"16\":1}}],[\"⚠请注意\",{\"1\":{\"12\":1}}],[\"格式使用数学公式\",{\"1\":{\"11\":1}}],[\"t\",{\"1\":{\"298\":1,\"299\":1}}],[\"tongpei\",{\"1\":{\"299\":1}}],[\"towards\",{\"1\":{\"223\":1}}],[\"toronto\",{\"1\":{\"213\":2}}],[\"to\",{\"1\":{\"202\":2,\"223\":1,\"226\":1,\"296\":3}}],[\"toc\",{\"1\":{\"88\":1}}],[\"talyot法\",{\"0\":{\"236\":1}}],[\"taylor\",{\"1\":{\"235\":1}}],[\"target\",{\"1\":{\"217\":2}}],[\"tail\",{\"1\":{\"202\":1}}],[\"tail2tail\",{\"1\":{\"202\":1}}],[\"tab\",{\"1\":{\"99\":3,\"181\":3}}],[\"tabs\",{\"1\":{\"99\":1,\"181\":1}}],[\"techniques\",{\"1\":{\"213\":1}}],[\"temp=nums\",{\"1\":{\"150\":1}}],[\"text\",{\"1\":{\"14\":1}}],[\"tex\",{\"0\":{\"81\":1},\"1\":{\"11\":1}}],[\"tip\",{\"1\":{\"94\":1,\"177\":1}}],[\"title\",{\"1\":{\"72\":1,\"80\":1}}],[\"two\",{\"1\":{\"80\":4}}],[\"tb\",{\"1\":{\"80\":1}}],[\"this\",{\"1\":{\"226\":1}}],[\"three\",{\"1\":{\"80\":2}}],[\"thu\",{\"1\":{\"78\":1}}],[\"the\",{\"1\":{\"226\":1,\"290\":1,\"294\":4,\"296\":5}}],[\"theorems\",{\"1\":{\"296\":1}}],[\"theorem\",{\"1\":{\"226\":1,\"300\":1}}],[\"theme\",{\"1\":{\"99\":3,\"181\":3}}],[\"there\",{\"1\":{\"72\":2}}],[\"then\",{\"1\":{\"21\":2,\"294\":1}}],[\"tue\",{\"1\":{\"78\":1}}],[\"type\",{\"1\":{\"77\":2,\"78\":3}}],[\"transforms\",{\"1\":{\"290\":1}}],[\"transition\",{\"1\":{\"29\":3}}],[\"true\",{\"1\":{\"0\":1,\"148\":1}}],[\"=2\",{\"1\":{\"299\":1}}],[\"=temp\",{\"1\":{\"150\":1}}],[\"==arr\",{\"1\":{\"148\":1}}],[\"==\",{\"1\":{\"148\":1}}],[\"==接近度的阶数越高\",{\"1\":{\"111\":1}}],[\"==若两曲线具有\",{\"1\":{\"111\":1}}],[\"=>\",{\"1\":{\"34\":1}}],[\"=\",{\"1\":{\"10\":1,\"34\":3,\"63\":1,\"93\":2,\"143\":3,\"145\":2,\"148\":5,\"153\":10,\"176\":2,\"299\":1}}],[\"apple\",{\"0\":{\"313\":1}}],[\"augmented\",{\"1\":{\"294\":1}}],[\"auto\",{\"1\":{\"31\":1,\"154\":1}}],[\"assuming\",{\"1\":{\"296\":1}}],[\"as\",{\"1\":{\"294\":1}}],[\"arxiv\",{\"1\":{\"223\":1,\"286\":1,\"311\":2}}],[\"arr\",{\"1\":{\"148\":4}}],[\"adjoint\",{\"1\":{\"286\":1}}],[\"adversarial\",{\"1\":{\"223\":1}}],[\"add\",{\"1\":{\"99\":2,\"181\":2}}],[\"attacks\",{\"1\":{\"223\":1}}],[\"attack\",{\"1\":{\"217\":4}}],[\"attrs\",{\"0\":{\"67\":1}}],[\"an\",{\"1\":{\"294\":2}}],[\"and\",{\"1\":{\"213\":1,\"226\":2,\"290\":1,\"296\":1,\"311\":1}}],[\"animate\",{\"1\":{\"31\":1}}],[\"alessio\",{\"1\":{\"311\":1}}],[\"also\",{\"1\":{\"226\":1}}],[\"al\",{\"1\":{\"206\":1,\"210\":2}}],[\"alt\",{\"1\":{\"40\":1}}],[\"accelerating\",{\"1\":{\"311\":1}}],[\"accidental\",{\"1\":{\"220\":1}}],[\"activations\",{\"1\":{\"290\":1}}],[\"active\",{\"1\":{\"99\":1,\"181\":1}}],[\"acyclic\",{\"1\":{\"201\":1}}],[\"a>\",{\"1\":{\"102\":1}}],[\"a1\",{\"1\":{\"80\":1}}],[\"a\",{\"1\":{\"10\":1,\"34\":2,\"63\":1,\"93\":2,\"147\":1,\"148\":10,\"176\":2,\"290\":1,\"294\":2,\"296\":1}}],[\"3702\",{\"1\":{\"213\":1}}],[\"30\",{\"1\":{\"46\":3}}],[\"3\",{\"0\":{\"119\":1,\"124\":1,\"132\":1,\"137\":1,\"245\":1,\"250\":1,\"253\":1,\"255\":1,\"260\":1,\"265\":1,\"270\":1,\"273\":1,\"275\":1,\"280\":1},\"1\":{\"9\":1,\"34\":1,\"113\":1,\"153\":6,\"221\":1,\"226\":1}}],[\"260\",{\"1\":{\"78\":1}}],[\"218\",{\"1\":{\"78\":1}}],[\"255之间的信息\",{\"1\":{\"220\":1}}],[\"255\",{\"1\":{\"77\":1}}],[\"253\",{\"1\":{\"72\":1}}],[\"230\",{\"1\":{\"72\":1,\"78\":1}}],[\"224\",{\"1\":{\"78\":1}}],[\"22\",{\"1\":{\"46\":1}}],[\"2019\",{\"1\":{\"311\":1}}],[\"2014\",{\"1\":{\"221\":1}}],[\"2016s\",{\"1\":{\"213\":1}}],[\"2009\",{\"1\":{\"213\":1}}],[\"20231211152930899\",{\"1\":{\"237\":1}}],[\"20231211152029421\",{\"1\":{\"237\":1}}],[\"20231211152044454\",{\"1\":{\"237\":1}}],[\"20231211151256664\",{\"1\":{\"236\":1}}],[\"20231216153149245\",{\"1\":{\"208\":1}}],[\"20231216152932251\",{\"1\":{\"207\":1}}],[\"20231216150648772\",{\"1\":{\"205\":1}}],[\"20231216143808424\",{\"1\":{\"202\":1}}],[\"20231216143312896\",{\"1\":{\"202\":1}}],[\"20231216231326318\",{\"1\":{\"166\":1}}],[\"20231224235548436\",{\"1\":{\"195\":1}}],[\"20231228182958923\",{\"1\":{\"111\":1}}],[\"20231229114511324\",{\"1\":{\"109\":1}}],[\"2020\",{\"1\":{\"86\":1}}],[\"20240102215553996\",{\"1\":{\"46\":1}}],[\"20240102215526117\",{\"1\":{\"46\":1}}],[\"20240102214118865\",{\"1\":{\"45\":1}}],[\"20240102214059042\",{\"1\":{\"45\":1}}],[\"20240102213837629\",{\"1\":{\"44\":1}}],[\"20240102213830537\",{\"1\":{\"44\":1}}],[\"20240102213952358\",{\"1\":{\"42\":1}}],[\"20\",{\"1\":{\"46\":1}}],[\"2\",{\"0\":{\"118\":1,\"123\":1,\"131\":1,\"136\":1,\"244\":1,\"248\":1,\"249\":1,\"254\":1,\"259\":1,\"264\":1,\"268\":1,\"269\":1,\"274\":1,\"279\":1},\"1\":{\"9\":1,\"34\":2,\"54\":14,\"70\":1,\"111\":2,\"113\":1,\"148\":8,\"153\":6,\"194\":1,\"201\":1,\"208\":1,\"226\":1,\"299\":1}}],[\"1给出了上述例子中\",{\"1\":{\"196\":1}}],[\"11\",{\"1\":{\"195\":4,\"196\":1,\"236\":1}}],[\"1节中方法进行参数估计\",{\"1\":{\"194\":1}}],[\"147\",{\"1\":{\"78\":1}}],[\"135\",{\"1\":{\"78\":1}}],[\"132\",{\"1\":{\"77\":1}}],[\"138\",{\"1\":{\"72\":1}}],[\"10数据集时\",{\"1\":{\"221\":1}}],[\"10\",{\"1\":{\"77\":3,\"144\":1,\"153\":3,\"196\":1,\"268\":1}}],[\"150\",{\"1\":{\"78\":1}}],[\"15\",{\"1\":{\"72\":1,\"221\":1}}],[\"1806\",{\"1\":{\"286\":1}}],[\"189\",{\"1\":{\"149\":1,\"155\":1}}],[\"18\",{\"1\":{\"46\":1,\"234\":2}}],[\"1706\",{\"1\":{\"223\":1}}],[\"17\",{\"1\":{\"46\":1,\"234\":1}}],[\"1906\",{\"1\":{\"311\":1}}],[\"1997\",{\"1\":{\"210\":1}}],[\"1996\",{\"1\":{\"210\":1}}],[\"1966\",{\"1\":{\"206\":1}}],[\"19th\",{\"1\":{\"65\":1}}],[\"19\",{\"1\":{\"45\":1}}],[\"16\",{\"1\":{\"44\":1,\"153\":2}}],[\"12\",{\"1\":{\"44\":1,\"45\":1,\"153\":2,\"296\":1}}],[\"1\",{\"0\":{\"243\":1,\"263\":1},\"1\":{\"9\":1,\"10\":1,\"34\":2,\"54\":12,\"63\":1,\"68\":1,\"70\":1,\"86\":2,\"93\":2,\"103\":1,\"111\":1,\"113\":1,\"143\":3,\"148\":8,\"153\":9,\"154\":3,\"176\":2,\"185\":1,\"204\":2,\"234\":3,\"286\":1,\"290\":1,\"294\":1,\"296\":2}}],[\"项目\",{\"1\":{\"9\":6}}],[\"列表默认为\",{\"1\":{\"9\":1}}],[\"删除线\",{\"1\":{\"8\":1}}],[\"斜体\",{\"1\":{\"8\":1}}],[\"粗体\",{\"1\":{\"8\":1}}],[\"标准化流normalizing\",{\"1\":{\"283\":1}}],[\"标准语法中的内容均不受支持\",{\"1\":{\"12\":1}}],[\"标量一般用斜体小写英文字母\",{\"1\":{\"172\":1}}],[\"标量\",{\"1\":{\"172\":1}}],[\"标题\",{\"0\":{\"118\":1,\"119\":1,\"123\":1,\"124\":1,\"131\":1,\"132\":1,\"136\":1,\"137\":1,\"244\":1,\"245\":1,\"249\":1,\"250\":1,\"254\":1,\"255\":1,\"259\":1,\"260\":1,\"264\":1,\"265\":1,\"269\":1,\"270\":1,\"274\":1,\"275\":1,\"279\":1,\"280\":1}}],[\"标题和页面信息\",{\"1\":{\"88\":1}}],[\"标题默认会自动转换为大写\",{\"1\":{\"8\":1}}],[\"标签为\",{\"1\":{\"86\":1}}],[\"标记\",{\"0\":{\"69\":1}}],[\"标注水平幻灯片\",{\"1\":{\"5\":1}}],[\"标注幻灯片\",{\"0\":{\"4\":1,\"5\":1}}],[\"你需要确保列表中的区间仍然有序且不重叠\",{\"1\":{\"153\":1}}],[\"你需要阅读\",{\"1\":{\"59\":1}}],[\"你需要在元素上添加\",{\"1\":{\"19\":1}}],[\"你应该创建和编写\",{\"1\":{\"57\":1}}],[\"你应该在页面前端设置\",{\"1\":{\"0\":1}}],[\"你可以返回任何满足上述条件的数组作为答案\",{\"1\":{\"148\":1}}],[\"你可以将图片和\",{\"1\":{\"87\":1}}],[\"你可以自由在这里书写你的\",{\"1\":{\"87\":1}}],[\"你可以标记\",{\"1\":{\"69\":1}}],[\"你可以通过主题选项和页面\",{\"1\":{\"88\":1}}],[\"你可以通过设置页面的\",{\"1\":{\"51\":1}}],[\"你可以通过向特定幻灯片添加\",{\"1\":{\"17\":1}}],[\"你可以对代码块进行高亮\",{\"1\":{\"34\":1}}],[\"你可以在\",{\"1\":{\"86\":1}}],[\"你可以在相邻的幻灯片上添加\",{\"1\":{\"31\":1}}],[\"你可以在幻灯片中使用\",{\"1\":{\"6\":1,\"7\":1,\"9\":1,\"10\":1,\"11\":1,\"12\":1}}],[\"你可以使用它轻松生成文档或博客站点\",{\"1\":{\"57\":1}}],[\"你可以使用\",{\"1\":{\"27\":1,\"34\":1}}],[\"你可以按照顺序包裹一个\",{\"1\":{\"25\":1}}],[\"你也可以使用\",{\"1\":{\"11\":1}}],[\"mlp\",{\"1\":{\"286\":1}}],[\"mle\",{\"1\":{\"239\":1}}],[\"means算法\",{\"1\":{\"242\":1}}],[\"method\",{\"1\":{\"221\":2}}],[\"mermaid\",{\"0\":{\"80\":1}}],[\"muyuuuu\",{\"1\":{\"218\":1}}],[\"mitpress\",{\"1\":{\"213\":1}}],[\"mixture\",{\"1\":{\"195\":1}}],[\"mister\",{\"1\":{\"72\":2}}],[\"m\",{\"1\":{\"194\":1,\"195\":1,\"213\":1}}],[\"ms\",{\"1\":{\"86\":1}}],[\"models\",{\"1\":{\"213\":1,\"223\":1}}],[\"model\",{\"1\":{\"167\":1,\"195\":1,\"196\":2,\"202\":1,\"206\":1,\"210\":2}}],[\"more\",{\"1\":{\"85\":1}}],[\"mon\",{\"1\":{\"78\":1}}],[\"md\",{\"1\":{\"62\":1,\"169\":4,\"187\":4,\"286\":5,\"293\":4}}],[\"masci\",{\"1\":{\"311\":1}}],[\"manifold\",{\"1\":{\"299\":1}}],[\"may\",{\"1\":{\"294\":1}}],[\"max\",{\"1\":{\"290\":1}}],[\"maxout网络都是被设计用线性的方式来运作的\",{\"1\":{\"221\":1}}],[\"maximum\",{\"1\":{\"210\":1,\"239\":1}}],[\"maximal\",{\"1\":{\"208\":1}}],[\"maximization\",{\"1\":{\"194\":1}}],[\"marco\",{\"1\":{\"311\":1}}],[\"markov\",{\"1\":{\"206\":1,\"207\":2,\"294\":1}}],[\"markdown\",{\"0\":{\"6\":1,\"7\":1,\"9\":1,\"10\":1,\"11\":1,\"12\":1,\"57\":1,\"58\":1,\"59\":1,\"60\":1,\"91\":1},\"1\":{\"6\":1,\"7\":1,\"9\":1,\"10\":1,\"11\":1,\"12\":2,\"49\":1,\"57\":2,\"58\":3,\"59\":1,\"60\":3,\"61\":2,\"62\":1,\"63\":1,\"73\":1,\"86\":1,\"87\":2},\"2\":{\"84\":1,\"106\":1}}],[\"marginal\",{\"1\":{\"194\":1}}],[\"math\",{\"1\":{\"11\":1}}],[\"mrf\",{\"1\":{\"207\":1}}],[\"mr\",{\"1\":{\"3\":1,\"72\":1,\"75\":1}}],[\"hypothesis\",{\"1\":{\"299\":1}}],[\"h\",{\"1\":{\"299\":1}}],[\"have\",{\"1\":{\"296\":1}}],[\"hammersley\",{\"1\":{\"208\":3}}],[\"hmm\",{\"1\":{\"206\":1}}],[\"higher\",{\"1\":{\"294\":1}}],[\"highlight\",{\"1\":{\"10\":1,\"23\":6,\"34\":1}}],[\"hidden\",{\"1\":{\"206\":1}}],[\"head\",{\"1\":{\"202\":1}}],[\"head2head\",{\"1\":{\"202\":1}}],[\"head2tail\",{\"1\":{\"202\":1}}],[\"heading\",{\"1\":{\"201\":1}}],[\"headerdepth\",{\"1\":{\"201\":1}}],[\"href=\",{\"1\":{\"102\":1}}],[\"https\",{\"1\":{\"72\":2,\"213\":1,\"216\":1}}],[\"html\",{\"1\":{\"5\":1,\"25\":1,\"31\":1}}],[\"h2o\",{\"1\":{\"65\":1}}],[\"h3\",{\"0\":{\"8\":1}}],[\"hot\",{\"1\":{\"241\":1}}],[\"however\",{\"1\":{\"226\":1}}],[\"hope\",{\"1\":{\"3\":1,\"72\":4,\"75\":1,\"86\":1,\"99\":3,\"181\":3}}],[\"home\",{\"1\":{\"0\":1}}],[\"在优化算法上有丰富的理论来源\",{\"1\":{\"302\":1}}],[\"在优化理论中\",{\"1\":{\"295\":1}}],[\"在$\",{\"1\":{\"300\":1}}],[\"在定理中虽然对的要求降低了\",{\"1\":{\"300\":1}}],[\"在意义上逼近\",{\"1\":{\"300\":1}}],[\"在增加维度的neuralode后添加一个线性层\",{\"1\":{\"300\":1}}],[\"在高维空间中\",{\"1\":{\"299\":1}}],[\"在统计学习理论中\",{\"1\":{\"295\":1}}],[\"在数学上\",{\"1\":{\"295\":1}}],[\"在数学上阐述为什么使用随机噪声初始化增广维度比使用零初始化更有利的原因可以从以下几个角度来考虑\",{\"1\":{\"295\":1}}],[\"在后续会深入讨论网络架构的选择和设计\",{\"1\":{\"290\":1}}],[\"在常规神经网络中\",{\"1\":{\"290\":1}}],[\"在上关于\",{\"1\":{\"290\":1}}],[\"在2018年首次被提出和使用\",{\"1\":{\"286\":1}}],[\"在无监督特征学习中\",{\"1\":{\"242\":1}}],[\"在无向图模型中\",{\"1\":{\"192\":1}}],[\"在样本不足时会出现过拟合\",{\"1\":{\"241\":1}}],[\"在实际应用中\",{\"1\":{\"241\":1}}],[\"在下面各节所考虑的更精确方法的误差分析按照同样的模式\",{\"1\":{\"235\":1}}],[\"在下图所示的无向图中共有\",{\"1\":{\"208\":1}}],[\"在典型的计算机中\",{\"1\":{\"234\":1}}],[\"在以\",{\"1\":{\"231\":1}}],[\"在平面上的示意图如下图\",{\"1\":{\"230\":1}}],[\"在二维情形零解的稳定形态\",{\"1\":{\"230\":1}}],[\"在minst测试集上\",{\"1\":{\"221\":1}}],[\"在对抗性攻击的背景下\",{\"1\":{\"221\":1}}],[\"在这个更加广义的集合中\",{\"1\":{\"298\":1}}],[\"在这个公式中\",{\"1\":{\"221\":1}}],[\"在这个假设下\",{\"1\":{\"166\":1}}],[\"在黑箱攻击中\",{\"1\":{\"217\":1}}],[\"在白箱攻击中攻击者知道目标模型的所有信息\",{\"1\":{\"217\":1}}],[\"在所有团中\",{\"1\":{\"208\":1}}],[\"在朴素贝叶斯分类器中\",{\"1\":{\"205\":1}}],[\"在强\",{\"1\":{\"205\":1}}],[\"在给定它的邻居的情况下独立于所有其他变量\",{\"1\":{\"207\":1}}],[\"在给定\",{\"1\":{\"202\":1}}],[\"在概率图模型中\",{\"1\":{\"201\":1}}],[\"在本章中\",{\"1\":{\"197\":1}}],[\"在已知部分变量时\",{\"1\":{\"197\":1}}],[\"在已知\",{\"1\":{\"196\":1}}],[\"在不知道这几个变量依赖关系的情况下\",{\"1\":{\"196\":1}}],[\"在不作任何独立假设条件下\",{\"1\":{\"196\":1}}],[\"在第\",{\"1\":{\"194\":1,\"236\":1}}],[\"在一个包含隐变量的图模型中\",{\"1\":{\"194\":1}}],[\"在有向图中\",{\"1\":{\"192\":1}}],[\"在有向图模型中\",{\"1\":{\"191\":1}}],[\"在此基础上\",{\"1\":{\"191\":1}}],[\"在机器学习中\",{\"1\":{\"166\":1,\"198\":1,\"221\":1}}],[\"在列表中插入一个新的区间\",{\"1\":{\"153\":1}}],[\"在遍历胃口呢\",{\"1\":{\"144\":1}}],[\"在遍历的饼干\",{\"1\":{\"144\":1}}],[\"在进行变分法的有关推导时要经常用到变分的这个性质\",{\"1\":{\"112\":1}}],[\"在自变量\",{\"1\":{\"112\":1}}],[\"在\",{\"1\":{\"112\":4,\"113\":4,\"202\":3,\"221\":1,\"293\":1}}],[\"在区间\",{\"1\":{\"111\":3,\"113\":1}}],[\"在页面禁用功能与布局\",{\"1\":{\"51\":1}}],[\"在过年期间也会有也灯会夜游\",{\"1\":{\"46\":1}}],[\"在日落时分登城\",{\"1\":{\"46\":1}}],[\"在你启用\",{\"1\":{\"10\":1,\"11\":1}}],[\"在前一个\",{\"1\":{\"5\":1}}],[\"在幻灯片上添加属性\",{\"1\":{\"5\":1}}],[\"在水平幻灯片中使用\",{\"1\":{\"5\":1}}],[\">right\",{\"1\":{\"154\":1}}],[\">arr\",{\"1\":{\"148\":1}}],[\">a2\",{\"1\":{\"80\":2}}],[\">=arr\",{\"1\":{\"148\":1}}],[\">=\",{\"1\":{\"143\":3,\"144\":1}}],[\">跳转单词<\",{\"1\":{\"102\":1}}],[\">c2\",{\"1\":{\"80\":1}}],[\">b2\",{\"1\":{\"80\":1}}],[\">e\",{\"1\":{\"79\":2}}],[\">process\",{\"1\":{\"79\":1}}],[\">\",{\"1\":{\"5\":2,\"25\":2,\"80\":3}}],[\"<left\",{\"1\":{\"154\":1}}],[\"<=1\",{\"1\":{\"151\":1}}],[\"<=\",{\"1\":{\"145\":1}}],[\"<arrp\",{\"1\":{\"148\":1}}],[\"<a\",{\"1\":{\"102\":1}}],[\"<\",{\"1\":{\"5\":2,\"145\":2,\"148\":1}}],[\"幻灯片演示\",{\"0\":{\"3\":1}}],[\"幻灯片页\",{\"0\":{\"2\":1}}],[\"啦啦啦啦\",{\"1\":{\"1\":1}}],[\"和是直接设为0相比\",{\"1\":{\"295\":1}}],[\"和监督学习一样\",{\"1\":{\"242\":1}}],[\"和任何正整数\",{\"1\":{\"235\":1}}],[\"和类别\",{\"1\":{\"205\":1}}],[\"和它的父节点集合\",{\"1\":{\"204\":1}}],[\"和一个有\",{\"1\":{\"202\":1,\"207\":1}}],[\"和其下界\",{\"1\":{\"194\":1}}],[\"和公式\",{\"1\":{\"192\":1,\"195\":1}}],[\"和动画\",{\"1\":{\"19\":1}}],[\"和\",{\"1\":{\"0\":1,\"58\":1,\"86\":1,\"111\":1,\"113\":2,\"166\":1,\"195\":1,\"196\":3,\"202\":3,\"206\":2,\"207\":2,\"230\":1,\"235\":2,\"240\":1,\"299\":1}}],[\"博客主页\",{\"0\":{\"0\":1},\"1\":{\"0\":1}}]],\"serializationVersion\":2}}")).map(([e,t])=>[e,zt(t,{fields:["h","t","c"],storeFields:["h","t","c"]})]));self.onmessage=({data:{type:e="all",query:t,locale:s,options:n}})=>{e==="suggest"?self.postMessage(st(t,v[s],n)):e==="search"?self.postMessage(et(t,v[s],n)):self.postMessage({suggestions:st(t,v[s],n),results:et(t,v[s],n)})};
//# sourceMappingURL=index.js.map
